{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Engineer? Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation A strong focus on the security and reliability and know how to balance both Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-engineer","text":"Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation A strong focus on the security and reliability and know how to balance both Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"So You Wana Be an AI Engineer?"},{"location":"cicd/","text":"CI/CD Continuous Integration and Continuous Delivery/Deployment is an important part of DevOps, where a standard mode of operations are automated in a pipeline frequently, and reliably. Differences between CI-CD-CD. Source This process is usually integrated with a version control platform like Github and Gitlab. Both popular platforms provide in-built CI/CD automation called Github Actions and Gitlab-CI . They are largely free, with some paid add-on tools. Other popular ones include CircleCI and TravisCI . Gitlab-CI Basics Gitlab CI is run by \"runners\", which are essentially google servers spun up when a job is activated. A series of jobs make up a pipeline , while certain jobs of a specific attribute are assigned to a stage . By default, all jobs in a stage needs to finish before the next stage's jobs starts. A typical pipeline workflow consists of the various stages. Test: security scans, unit-tests, integration tests Build: building a docker image Deploy: deployment of image to dev/staging/production A pipeline with various stages, and individual jobs, shown in Gitlab's interface Yaml File Gitlab-CI will auto run if a file called .gitlab-ci.yml is stored in the root of the repository. Inside the ymal file, containing mostly shell commands that tells runner what to execute. It can also refer to scripts stored in the repo so as to keep the ymal file short and concise. Github Actions Yaml File Github Actions will auto run if one or more ymal file is located in the folder .github/workflows/ .","title":"CI/CD"},{"location":"cicd/#cicd","text":"Continuous Integration and Continuous Delivery/Deployment is an important part of DevOps, where a standard mode of operations are automated in a pipeline frequently, and reliably. Differences between CI-CD-CD. Source This process is usually integrated with a version control platform like Github and Gitlab. Both popular platforms provide in-built CI/CD automation called Github Actions and Gitlab-CI . They are largely free, with some paid add-on tools. Other popular ones include CircleCI and TravisCI .","title":"CI/CD"},{"location":"cicd/#gitlab-ci","text":"","title":"Gitlab-CI"},{"location":"cicd/#basics","text":"Gitlab CI is run by \"runners\", which are essentially google servers spun up when a job is activated. A series of jobs make up a pipeline , while certain jobs of a specific attribute are assigned to a stage . By default, all jobs in a stage needs to finish before the next stage's jobs starts. A typical pipeline workflow consists of the various stages. Test: security scans, unit-tests, integration tests Build: building a docker image Deploy: deployment of image to dev/staging/production A pipeline with various stages, and individual jobs, shown in Gitlab's interface","title":"Basics"},{"location":"cicd/#yaml-file","text":"Gitlab-CI will auto run if a file called .gitlab-ci.yml is stored in the root of the repository. Inside the ymal file, containing mostly shell commands that tells runner what to execute. It can also refer to scripts stored in the repo so as to keep the ymal file short and concise.","title":"Yaml File"},{"location":"cicd/#github-actions","text":"","title":"Github Actions"},{"location":"cicd/#yaml-file_1","text":"Github Actions will auto run if one or more ymal file is located in the folder .github/workflows/ .","title":"Yaml File"},{"location":"code-standards/","text":"Code Standards requirements.txt requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101 pipreqs also allow an option --mode=compat , which enables patch version updates only. This is important as it allows bug fixes or security patches installed within the micro versions with little chance that the code will break since it is a micro update. A side note on semantic versioning. As defined by the convention , it usually follows the version of Major.Minor.Patch; e.g. Flask==1.0.2, where the patch version is backward compatible. Python also have its own description in PEP 440 , naming it as Major.Minor.Micro. DocStrings DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco ISort Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py Black Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py Flake8 A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Code Standards"},{"location":"code-standards/#code-standards","text":"","title":"Code Standards"},{"location":"code-standards/#requirementstxt","text":"requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101 pipreqs also allow an option --mode=compat , which enables patch version updates only. This is important as it allows bug fixes or security patches installed within the micro versions with little chance that the code will break since it is a micro update. A side note on semantic versioning. As defined by the convention , it usually follows the version of Major.Minor.Patch; e.g. Flask==1.0.2, where the patch version is backward compatible. Python also have its own description in PEP 440 , naming it as Major.Minor.Micro.","title":"requirements.txt"},{"location":"code-standards/#docstrings","text":"DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco","title":"DocStrings"},{"location":"code-standards/#isort","text":"Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py","title":"ISort"},{"location":"code-standards/#black","text":"Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py","title":"Black"},{"location":"code-standards/#flake8","text":"A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Flake8"},{"location":"demo/","text":"Demo Site For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo # streamlit settings st . set_page_config ( page_title = 'Demo Site' ) json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" , type = \"password\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"demo/#demo-site","text":"For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo # streamlit settings st . set_page_config ( page_title = 'Demo Site' ) json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" , type = \"password\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"docker/","text":"Docker Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian alpine installations will need apk , while debian uses apt Docker is not the only container service available, but the most widely used Basics There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network Dockerfile The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7-slim RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install --no-cache-dir -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] Intel-GPU If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename> ARM-GPU The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename> Common Commands Build The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu Run For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Expose Flask port system-port:container-port --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id> Check Status This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats Clean To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker volume prune sudo docker network prune sudo docker system prune Debug We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash Storage By design, docker containers do not store persistent data. Thus, any data written in containers will not be available once the container is removed. There are three options to persist data, bind mount, volume, or tmpfs mount. More from 4sysops , and docker Bind mount provides a direct connection of a local folder's file system to the container's system. We can easily swap a file within the local folder and it will be immediately reflected within the container. This is helpful when we need to change a new model after training. docker run \\ -p 5000 :5000 \\ --mount type = bind,source = /Users/jake/Desktop/data,target = /data,readonly \\ --name <containername> <imagename> Volume mount is the preferred mechanism for updating a file from a container into the file system. The volume folder is stored in the local filesystem managed by docker in /var/lib/docker/volumes . docker run \\ -p 5000 :5000 \\ --mount type = volume,source = <volumename>,target = /data \\ --name <containername> <imagename> Cmd Desc docker volume inspect volume_name inspect the volume; view mounted directory in docker docker volume ls view all volumes in docker docker rm volume delete volume Network For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. For sending REST-APIs between docker containers in the same network, the IP address will be http://host.docker.internal for Mac & Windows, and http://172.17.0.1 for Linux. Optimize Image Size There are various ways to reduce the image size being built. Slim Build For python base image, we have many options to choose the python various and build type. As a rule-of-thumb, we can use the slim build as defined below. It has a lot less libraries, and can reduce the image by more than 500Mb. Most of the time the container can run well, though some libraries like opencv can only work with the full image. Note that the alpine build is the smallest, but more often then not, you will find that a lot of python library dependencies does not work well here. FROM python:3.8-slim Disable Cache By default, all the python libraries installed are cached. This refers to installation files(.whl, etc) or source files (.tar.gz, etc) to avoid re-download when not expired. However, this is usually not required when building your image. COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt Below shows the image size being reduced. The bottom most is the full python image, with cache. The middle is the python slim image, with cache. The top most is the python slim image with no cache. Docker-Compose When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . We need to install it first using sudo apt install docker-compose . The official Docker blog post gives a good introduction on this. version: \"3.2\" services: facedetection: build: context: ./project dockerfile: Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name: facedetection ports: - 5001:5000 logging: options: max-size: \"10m\" max-file: \"5\" deploy: resources: limits: cpus: '0.001' memory: 50M volumes: - type: bind source: /Users/jake/Desktop/source target: /model restart: unless-stopped maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 logging: options: max-size: \"10m\" max-file: \"5\" environment: - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' restart: unless-stopped The commands follows docker commands closely, with some of the more important ones as follows. Cmd Desc docker-compose build build images docker-compose pull pull image from a registry; must input key image docker-compose up run containers docker-compose up servicename run specific containers docker-compose up -d run containers in detached mode docker-compose ps view containers' statuses docker-compose stop stop containers docker-compose start start containers Docker Dashboard Docker in Windows & Mac comes by default a docker dashboard, which gives you a easy GUI to see and manage your images and containers, rather than within the commandline. However, this is lacking in Linux. A great free alternative (with more features) is Portainer . We just need to launch it using docker with the following commands, and the web-based GUI will be accessible via localhost:9000 . After creating a user account, the rest of it is pretty intuitive. sudo docker volume create portainer_data sudo docker run -d -p 8000 :8000 -p 9000 :9000 --name = portainer --restart = always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce The home page, showing the local docker connection, and some summary statistics. On clicking that, an overview of the local docker is shown. Entering the container panel, we have various options to control our containers. We can even go into the container, by clicking the console link.","title":"Docker"},{"location":"docker/#docker","text":"Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian alpine installations will need apk , while debian uses apt Docker is not the only container service available, but the most widely used","title":"Docker"},{"location":"docker/#basics","text":"There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network","title":"Basics"},{"location":"docker/#dockerfile","text":"The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7-slim RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install --no-cache-dir -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ]","title":"Dockerfile"},{"location":"docker/#intel-gpu","text":"If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename>","title":"Intel-GPU"},{"location":"docker/#arm-gpu","text":"The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename>","title":"ARM-GPU"},{"location":"docker/#common-commands","text":"","title":"Common Commands"},{"location":"docker/#build","text":"The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu","title":"Build"},{"location":"docker/#run","text":"For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Expose Flask port system-port:container-port --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id>","title":"Run"},{"location":"docker/#check-status","text":"This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats","title":"Check Status"},{"location":"docker/#clean","text":"To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker volume prune sudo docker network prune sudo docker system prune","title":"Clean"},{"location":"docker/#debug","text":"We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash","title":"Debug"},{"location":"docker/#storage","text":"By design, docker containers do not store persistent data. Thus, any data written in containers will not be available once the container is removed. There are three options to persist data, bind mount, volume, or tmpfs mount. More from 4sysops , and docker Bind mount provides a direct connection of a local folder's file system to the container's system. We can easily swap a file within the local folder and it will be immediately reflected within the container. This is helpful when we need to change a new model after training. docker run \\ -p 5000 :5000 \\ --mount type = bind,source = /Users/jake/Desktop/data,target = /data,readonly \\ --name <containername> <imagename> Volume mount is the preferred mechanism for updating a file from a container into the file system. The volume folder is stored in the local filesystem managed by docker in /var/lib/docker/volumes . docker run \\ -p 5000 :5000 \\ --mount type = volume,source = <volumename>,target = /data \\ --name <containername> <imagename> Cmd Desc docker volume inspect volume_name inspect the volume; view mounted directory in docker docker volume ls view all volumes in docker docker rm volume delete volume","title":"Storage"},{"location":"docker/#network","text":"For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. For sending REST-APIs between docker containers in the same network, the IP address will be http://host.docker.internal for Mac & Windows, and http://172.17.0.1 for Linux.","title":"Network"},{"location":"docker/#optimize-image-size","text":"There are various ways to reduce the image size being built.","title":"Optimize Image Size"},{"location":"docker/#slim-build","text":"For python base image, we have many options to choose the python various and build type. As a rule-of-thumb, we can use the slim build as defined below. It has a lot less libraries, and can reduce the image by more than 500Mb. Most of the time the container can run well, though some libraries like opencv can only work with the full image. Note that the alpine build is the smallest, but more often then not, you will find that a lot of python library dependencies does not work well here. FROM python:3.8-slim","title":"Slim Build"},{"location":"docker/#disable-cache","text":"By default, all the python libraries installed are cached. This refers to installation files(.whl, etc) or source files (.tar.gz, etc) to avoid re-download when not expired. However, this is usually not required when building your image. COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt Below shows the image size being reduced. The bottom most is the full python image, with cache. The middle is the python slim image, with cache. The top most is the python slim image with no cache.","title":"Disable Cache"},{"location":"docker/#docker-compose","text":"When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . We need to install it first using sudo apt install docker-compose . The official Docker blog post gives a good introduction on this. version: \"3.2\" services: facedetection: build: context: ./project dockerfile: Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name: facedetection ports: - 5001:5000 logging: options: max-size: \"10m\" max-file: \"5\" deploy: resources: limits: cpus: '0.001' memory: 50M volumes: - type: bind source: /Users/jake/Desktop/source target: /model restart: unless-stopped maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 logging: options: max-size: \"10m\" max-file: \"5\" environment: - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' restart: unless-stopped The commands follows docker commands closely, with some of the more important ones as follows. Cmd Desc docker-compose build build images docker-compose pull pull image from a registry; must input key image docker-compose up run containers docker-compose up servicename run specific containers docker-compose up -d run containers in detached mode docker-compose ps view containers' statuses docker-compose stop stop containers docker-compose start start containers","title":"Docker-Compose"},{"location":"docker/#docker-dashboard","text":"Docker in Windows & Mac comes by default a docker dashboard, which gives you a easy GUI to see and manage your images and containers, rather than within the commandline. However, this is lacking in Linux. A great free alternative (with more features) is Portainer . We just need to launch it using docker with the following commands, and the web-based GUI will be accessible via localhost:9000 . After creating a user account, the rest of it is pretty intuitive. sudo docker volume create portainer_data sudo docker run -d -p 8000 :8000 -p 9000 :9000 --name = portainer --restart = always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce The home page, showing the local docker connection, and some summary statistics. On clicking that, an overview of the local docker is shown. Entering the container panel, we have various options to control our containers. We can even go into the container, by clicking the console link.","title":"Docker Dashboard"},{"location":"dvc/","text":"Data Version Control DVC is a library for ML versioning of blobs, which can include data and/or models. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ] DVC INIT dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded to the repository, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs Add S3 Keys To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\" Add Remote Link We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin Push Data to S3 We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link. Update Git We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version. Retrieve Data from S3 We can pull from specific dvc folders by using dvc pull -r <remote name> <foldername>.dvc Full Code Example # add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Data Version Control"},{"location":"dvc/#data-version-control","text":"DVC is a library for ML versioning of blobs, which can include data and/or models. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ]","title":"Data Version Control"},{"location":"dvc/#dvc-init","text":"dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded to the repository, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs","title":"DVC INIT"},{"location":"dvc/#add-s3-keys","text":"To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\"","title":"Add S3 Keys"},{"location":"dvc/#add-remote-link","text":"We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin","title":"Add Remote Link"},{"location":"dvc/#push-data-to-s3","text":"We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link.","title":"Push Data to S3"},{"location":"dvc/#update-git","text":"We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version.","title":"Update Git"},{"location":"dvc/#retrieve-data-from-s3","text":"We can pull from specific dvc folders by using dvc pull -r <remote name> <foldername>.dvc","title":"Retrieve Data from S3"},{"location":"dvc/#full-code-example","text":"# add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Full Code Example"},{"location":"fastapi/","text":"FastAPI FastAPI is one of the next generation python web framework that uses ASGI (asynchronous server gateway interface) instead of the traditional WSGI. It also includes a number of useful functions to make API creations easier. Uvicorn FastAPI uses Uvicorn as its ASGI. We can configure its settings as described here . We can also specify it in the fastapi python app script, or at the terminal when we launch uvicorn. For the former, with the below specification, we can just execute python app.py to start the application. # app.py from fastapi import FastAPI import uvicorn app = FastAPI () if __name__ == \"__main__\" : uvicorn . run ( 'app:app' , host = '0.0.0.0' , port = 5000 ) If we run from the terminal, with the app residing in example.py. uvicorn example:app --host = '0.0.0.0' --port = 5000 The documentation recommends that we use gunicorn which have richer features to better control over the workers processes. gunicorn app:app --bind 0 .0.0.0:5000 -w 1 --log-level debug -k uvicorn.workers.UvicornWorker Request-Response Schema FastAPI uses the pydantic library to define the schema of the request & response APIs. This allows the auto generation in the OpenAPI documentations, and for the former, for validating the schema when a request is received. For example, given the json: { \"boundingPoly\" : { \"normalizedVertices\" : [ { \"x\" : 0.406767 , \"y\" : 0.874573 , \"width\" : 0.357321 , \"height\" : 0.452179 , \"score\" : 0.972167 }, { \"x\" : 0.56781 , \"y\" : 0.874173 , \"width\" : 0.457373 , \"height\" : 0.452121 , \"score\" : 0.982109 } ] }, \"name\" : \"Cat\" } We can define in pydantic as below, using multiple basemodels for each level in the JSON. If there are no values input like y: float , it will listed as a required field If we add a value like y: float = 0.8369 , it will be an optional field, with the value also listed as a default and example value If we add a value like x: float = Field(..., example=0.82379) , it will be a required field, and also listed as an example value More attributes can be added in Field() , that will be populated in OpenAPI docs. class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 0.82379 , description = \"X-coordinates\" )) y : float = 0.8369 width : float height : float score : float class _boundingPoly ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] class ResponseSchema ( BaseModel ): boundingPoly : _boundingPoly name : str = \"Human\" We do the same for the request schema and place them in the routing function. from fastapi import FastAPI from pydantic import BaseModel , Field from typing import List import json import base64 import numpy as np @app . post ( '/api' , response_model = ResponseSchema ) async def human_detection ( request : RequestSchema ): JScontent = json . loads ( request . json ()) encodedImage = JScontent [ 'requests' ][ 0 ][ 'image' ][ 'content' ] npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) pred_output = model ( imgArr ) return pred_output Open-API OpenAPI documentations of Swagger UI or Redoc are automatically generated. You can access it at the endpoints of /docs and /redoc . First, the title, description and versions can be specified from the initialisation of fastapi. The request-response pydantic schema and examples will be added after its inclusion in a POST/GET request routing function. from fastapi import FastAPI app = FastAPI ( title = \"Human Detection API\" , description = \"Submit Image to Return Detected Humans in Bounding Boxes\" , version = \"1.0.0\" ) @app . post ( '/api' , response_model = RESPONSE_SCHEMA ) def human_detection ( request : REQUEST_SCHEMA ): do something return another_thing","title":"FastAPI"},{"location":"fastapi/#fastapi","text":"FastAPI is one of the next generation python web framework that uses ASGI (asynchronous server gateway interface) instead of the traditional WSGI. It also includes a number of useful functions to make API creations easier.","title":"FastAPI"},{"location":"fastapi/#uvicorn","text":"FastAPI uses Uvicorn as its ASGI. We can configure its settings as described here . We can also specify it in the fastapi python app script, or at the terminal when we launch uvicorn. For the former, with the below specification, we can just execute python app.py to start the application. # app.py from fastapi import FastAPI import uvicorn app = FastAPI () if __name__ == \"__main__\" : uvicorn . run ( 'app:app' , host = '0.0.0.0' , port = 5000 ) If we run from the terminal, with the app residing in example.py. uvicorn example:app --host = '0.0.0.0' --port = 5000 The documentation recommends that we use gunicorn which have richer features to better control over the workers processes. gunicorn app:app --bind 0 .0.0.0:5000 -w 1 --log-level debug -k uvicorn.workers.UvicornWorker","title":"Uvicorn"},{"location":"fastapi/#request-response-schema","text":"FastAPI uses the pydantic library to define the schema of the request & response APIs. This allows the auto generation in the OpenAPI documentations, and for the former, for validating the schema when a request is received. For example, given the json: { \"boundingPoly\" : { \"normalizedVertices\" : [ { \"x\" : 0.406767 , \"y\" : 0.874573 , \"width\" : 0.357321 , \"height\" : 0.452179 , \"score\" : 0.972167 }, { \"x\" : 0.56781 , \"y\" : 0.874173 , \"width\" : 0.457373 , \"height\" : 0.452121 , \"score\" : 0.982109 } ] }, \"name\" : \"Cat\" } We can define in pydantic as below, using multiple basemodels for each level in the JSON. If there are no values input like y: float , it will listed as a required field If we add a value like y: float = 0.8369 , it will be an optional field, with the value also listed as a default and example value If we add a value like x: float = Field(..., example=0.82379) , it will be a required field, and also listed as an example value More attributes can be added in Field() , that will be populated in OpenAPI docs. class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 0.82379 , description = \"X-coordinates\" )) y : float = 0.8369 width : float height : float score : float class _boundingPoly ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] class ResponseSchema ( BaseModel ): boundingPoly : _boundingPoly name : str = \"Human\" We do the same for the request schema and place them in the routing function. from fastapi import FastAPI from pydantic import BaseModel , Field from typing import List import json import base64 import numpy as np @app . post ( '/api' , response_model = ResponseSchema ) async def human_detection ( request : RequestSchema ): JScontent = json . loads ( request . json ()) encodedImage = JScontent [ 'requests' ][ 0 ][ 'image' ][ 'content' ] npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) pred_output = model ( imgArr ) return pred_output","title":"Request-Response Schema"},{"location":"fastapi/#open-api","text":"OpenAPI documentations of Swagger UI or Redoc are automatically generated. You can access it at the endpoints of /docs and /redoc . First, the title, description and versions can be specified from the initialisation of fastapi. The request-response pydantic schema and examples will be added after its inclusion in a POST/GET request routing function. from fastapi import FastAPI app = FastAPI ( title = \"Human Detection API\" , description = \"Submit Image to Return Detected Humans in Bounding Boxes\" , version = \"1.0.0\" ) @app . post ( '/api' , response_model = RESPONSE_SCHEMA ) def human_detection ( request : REQUEST_SCHEMA ): do something return another_thing","title":"Open-API"},{"location":"flask/","text":"Flask Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task. Simple Flask App Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" import traceback from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) # using .get will return None if request does not include key-value # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : tb = traceback . format_exc () app . logger . error ( tb ) return { \"errorMessages\" : tb . replace ( \" \\n \" , \"\" )} if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" ) Async From Flask>=2.0 onwards, it supports async syntax with the installation of pip install Flask[async] . It should be noted to use async only in I/O bound tasks which takes less than a few seconds to process. An excellent description from here . It works similarly & with the same response time as multi-threading. Synchronous Take for example this synchronous REST app that is requesting data from multiple urls. The response time is 1.024s. import json import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender_sync ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) @app . post ( \"/recommendation\" ) def fusion_api (): # synchronous, 1.024s --------- concat_list = [] for rre in api_urls . keys (): predictions = call_recommender_sync ( rre , api_urls ) concat_list . append ( predictions ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True ) Asynchronous To write in asynchronous code to send multiple requests, we need to use aiohttp in place of requests , and aiohttp to gather the results in a list. aiohttp is a non-blocking program, which allow other threads to continue running while it's waiting. The appropriate async and await syntax needs to be added too. The response time is around 0.356s, x2.87. import asyncio import json import requests from aiohttp from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } async def call_recommender_async ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } async with aiohttp . ClientSession () as session : async with session . post ( url , json = data ) as resp : prediction = await resp . json () return ( prediction , rre ) @app . post ( \"/recommendation\" ) async def fusion_api (): # asynchronous, 0.356s, x2.87 --------- concat_list = [] for rre in api_urls . keys (): predictions = asyncio . create_task ( call_recommender_async ( rre , api_urls )) concat_list . append ( predictions ) concat_list = await asyncio . gather ( * concat_list ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 ) Multi-Threading We can use concurrent.futures to send requests using multi-threading. The response time is similar, 0.359s, x2.85 faster. This is because of Python's Global Interpretor Lock (GIL), whereby only one thread can run one time. Python uses thread-switching to change to another thread to start another task, rendering multi-threading as an async, not parallel process. import json from concurrent.futures import ThreadPoolExecutor , as_completed import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) def multithread ( api_urls ): futures = [] with ThreadPoolExecutor ( max_workers = 4 ) as executor : for rre in api_urls . keys (): futures . append ( executor . submit ( call_recommender , rre , api_urls )) return [ future . result () for future in as_completed ( futures )] @app . post ( \"/recommendation\" ) def fusion_api (): # multi-threading, 0.359s, x2.85 --------- concat_list = multithread ( api_urls ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True ) Gunicorn Flask as a server is meant for development, as it tries to remind you everytime you launch it, giving the message WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. One of the best is Gunicorn ; a mature, fully featured server and process manager. It allows automated worker production and management through simple configurations. Command # gunicorn --bind <flask-ip>:<flask-port> <flask-script>:<flask-app> gunicorn --bind 0 .0.0.0:5000 serve_http:app Config File Rather than entering all the configs when launching gunicorn, we can hard code some of them in a config file gunicorn.conf.py . With this, we can adjust the workers based on the machine's cores. Gunicorn's documentation recommend the number of workers to be set as (total-cpu * 2) + 1 . One of the most important config besides workers is the preload , this allows the preloading of your model in memory and shared among all the workers. If not, each of your worker will load the model separately and consume a lot, if not all the RAM in the machine. # gunicorn.conf.py # to see all flask stdout, we can change the log level to debug import multiprocessing workers = ( multiprocessing . cpu_count () * 2 ) + 1 preload_app = True log_level = info timeout = 10 We can also use the gevent or gthread to implement concurrency if there are significant I/O blocking bottlenecks. For the latter, this is done by \u201cmonkey patching\u201d the code, mainly replacing blocking parts with compatible cooperative counterparts from gevent package. See this article for more information . Testing Python Requests The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response\"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j ) Postman Postman is a popular GUI to easily send requests and see the responses. CURL We can also use CURL in the terminal for commandline sending of requests. Here\u2019s a simple test to see the API works, without sending the data. curl --request POST localhost:5000/api Here\u2019s one complete request with data curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api To run multiple requests in parallel for stress testing curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & wait OpenAPI OpenAPI is a standard API documentation specification in ymal or json format, originated from Swagger. It usually comes with a user interface, the most popular being SwaggerUI . It provides all the information required about the API, with also an ability to test the API itself. There are three ways to go about this. generated separately as a single ymal file, and hosted using connexion generate as docstrings or individual ymal file for each endpoint using flasgger auto-generated using defined schemas, and adding additional info within the Flask app using Flask-Pydantic-Spec Personally, I believe the first is the most realistic, as the API specs are usually defined before the Flask app is created, and that doc can be sent to others for verification without creating a Flask app. Below is an example script for point 1. import connexion from flask import request from predict import prediction app = connexion . App ( __name__ , specification_dir = '.' ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : app . add_api ( 'openapi.yml' ) app . run ( host = \"0.0.0.0\" ) And point 3. \"\"\"flask server with pydantic validation & openapi integration\"\"\" from typing import List from flask import Flask , request from flask_pydantic_spec import FlaskPydanticSpec , Request , Response from pydantic import BaseModel , Field , confloat from predict import prediction app = Flask ( __name__ ) api = FlaskPydanticSpec ( \"flask\" , title = \"Objection Detection\" , version = \"v1.0.0\" ) class RequestSchema ( BaseModel ): maxResults : int = Field ( None , example = 20 , description = \"Maximum detection result to return\" ) min_height : float = Field ( None , example = 0.3 , description = \"Score\" ) min_width : float = Field ( None , example = 0.3 , description = \"Score\" ) score_th : float = Field ( None , example = 0.3 , description = \"Score\" ) nms_iou : float = Field ( ... , example = 0.4 , description = \"Non-max suppression, intersection over union\" ) type : str = Field ( ... , example = \"safetycone\" , description = \"name of object to detect\" ) image : str = Field ( ... , description = \"base64-encoded-image\" ) class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 5.12 , description = \"X-coordinate\" ) y : float = Field ( ... , example = 20.56 , description = \"Y-coordinate\" ) width : int = Field ( ... , example = 500 , description = \"width in pixel\" ) height : int = Field ( ... , example = 600 , description = \"height in pixel\" ) score : confloat ( gt = 0.0 , lt = 1.0 ) = Field ( ... , example = 0.79 , description = \"confidence score\" ) class ResponseSchema ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] @app . route ( \"/predict\" , methods = [ \"POST\" ]) @api . validate ( body = Request ( RequestSchema ), resp = Response ( HTTP_200 = ResponseSchema ), tags = [ \"API Name\" ] ) def get_predictions (): \"\"\"Short description of endpoint Long description of endpoint\"\"\" JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : api . register ( app ) app . run ( host = \"0.0.0.0\" ) You can refer to my repo for the full example.","title":"Flask"},{"location":"flask/#flask","text":"Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task.","title":"Flask"},{"location":"flask/#simple-flask-app","text":"Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" import traceback from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) # using .get will return None if request does not include key-value # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : tb = traceback . format_exc () app . logger . error ( tb ) return { \"errorMessages\" : tb . replace ( \" \\n \" , \"\" )} if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" )","title":"Simple Flask App"},{"location":"flask/#async","text":"From Flask>=2.0 onwards, it supports async syntax with the installation of pip install Flask[async] . It should be noted to use async only in I/O bound tasks which takes less than a few seconds to process. An excellent description from here . It works similarly & with the same response time as multi-threading.","title":"Async"},{"location":"flask/#synchronous","text":"Take for example this synchronous REST app that is requesting data from multiple urls. The response time is 1.024s. import json import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender_sync ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) @app . post ( \"/recommendation\" ) def fusion_api (): # synchronous, 1.024s --------- concat_list = [] for rre in api_urls . keys (): predictions = call_recommender_sync ( rre , api_urls ) concat_list . append ( predictions ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True )","title":"Synchronous"},{"location":"flask/#asynchronous","text":"To write in asynchronous code to send multiple requests, we need to use aiohttp in place of requests , and aiohttp to gather the results in a list. aiohttp is a non-blocking program, which allow other threads to continue running while it's waiting. The appropriate async and await syntax needs to be added too. The response time is around 0.356s, x2.87. import asyncio import json import requests from aiohttp from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } async def call_recommender_async ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } async with aiohttp . ClientSession () as session : async with session . post ( url , json = data ) as resp : prediction = await resp . json () return ( prediction , rre ) @app . post ( \"/recommendation\" ) async def fusion_api (): # asynchronous, 0.356s, x2.87 --------- concat_list = [] for rre in api_urls . keys (): predictions = asyncio . create_task ( call_recommender_async ( rre , api_urls )) concat_list . append ( predictions ) concat_list = await asyncio . gather ( * concat_list ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 )","title":"Asynchronous"},{"location":"flask/#multi-threading","text":"We can use concurrent.futures to send requests using multi-threading. The response time is similar, 0.359s, x2.85 faster. This is because of Python's Global Interpretor Lock (GIL), whereby only one thread can run one time. Python uses thread-switching to change to another thread to start another task, rendering multi-threading as an async, not parallel process. import json from concurrent.futures import ThreadPoolExecutor , as_completed import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) def multithread ( api_urls ): futures = [] with ThreadPoolExecutor ( max_workers = 4 ) as executor : for rre in api_urls . keys (): futures . append ( executor . submit ( call_recommender , rre , api_urls )) return [ future . result () for future in as_completed ( futures )] @app . post ( \"/recommendation\" ) def fusion_api (): # multi-threading, 0.359s, x2.85 --------- concat_list = multithread ( api_urls ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True )","title":"Multi-Threading"},{"location":"flask/#gunicorn","text":"Flask as a server is meant for development, as it tries to remind you everytime you launch it, giving the message WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. One of the best is Gunicorn ; a mature, fully featured server and process manager. It allows automated worker production and management through simple configurations.","title":"Gunicorn"},{"location":"flask/#command","text":"# gunicorn --bind <flask-ip>:<flask-port> <flask-script>:<flask-app> gunicorn --bind 0 .0.0.0:5000 serve_http:app","title":"Command"},{"location":"flask/#config-file","text":"Rather than entering all the configs when launching gunicorn, we can hard code some of them in a config file gunicorn.conf.py . With this, we can adjust the workers based on the machine's cores. Gunicorn's documentation recommend the number of workers to be set as (total-cpu * 2) + 1 . One of the most important config besides workers is the preload , this allows the preloading of your model in memory and shared among all the workers. If not, each of your worker will load the model separately and consume a lot, if not all the RAM in the machine. # gunicorn.conf.py # to see all flask stdout, we can change the log level to debug import multiprocessing workers = ( multiprocessing . cpu_count () * 2 ) + 1 preload_app = True log_level = info timeout = 10 We can also use the gevent or gthread to implement concurrency if there are significant I/O blocking bottlenecks. For the latter, this is done by \u201cmonkey patching\u201d the code, mainly replacing blocking parts with compatible cooperative counterparts from gevent package. See this article for more information .","title":"Config File"},{"location":"flask/#testing","text":"","title":"Testing"},{"location":"flask/#python-requests","text":"The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response\"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Python Requests"},{"location":"flask/#postman","text":"Postman is a popular GUI to easily send requests and see the responses.","title":"Postman"},{"location":"flask/#curl","text":"We can also use CURL in the terminal for commandline sending of requests. Here\u2019s a simple test to see the API works, without sending the data. curl --request POST localhost:5000/api Here\u2019s one complete request with data curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api To run multiple requests in parallel for stress testing curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & wait","title":"CURL"},{"location":"flask/#openapi","text":"OpenAPI is a standard API documentation specification in ymal or json format, originated from Swagger. It usually comes with a user interface, the most popular being SwaggerUI . It provides all the information required about the API, with also an ability to test the API itself. There are three ways to go about this. generated separately as a single ymal file, and hosted using connexion generate as docstrings or individual ymal file for each endpoint using flasgger auto-generated using defined schemas, and adding additional info within the Flask app using Flask-Pydantic-Spec Personally, I believe the first is the most realistic, as the API specs are usually defined before the Flask app is created, and that doc can be sent to others for verification without creating a Flask app. Below is an example script for point 1. import connexion from flask import request from predict import prediction app = connexion . App ( __name__ , specification_dir = '.' ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : app . add_api ( 'openapi.yml' ) app . run ( host = \"0.0.0.0\" ) And point 3. \"\"\"flask server with pydantic validation & openapi integration\"\"\" from typing import List from flask import Flask , request from flask_pydantic_spec import FlaskPydanticSpec , Request , Response from pydantic import BaseModel , Field , confloat from predict import prediction app = Flask ( __name__ ) api = FlaskPydanticSpec ( \"flask\" , title = \"Objection Detection\" , version = \"v1.0.0\" ) class RequestSchema ( BaseModel ): maxResults : int = Field ( None , example = 20 , description = \"Maximum detection result to return\" ) min_height : float = Field ( None , example = 0.3 , description = \"Score\" ) min_width : float = Field ( None , example = 0.3 , description = \"Score\" ) score_th : float = Field ( None , example = 0.3 , description = \"Score\" ) nms_iou : float = Field ( ... , example = 0.4 , description = \"Non-max suppression, intersection over union\" ) type : str = Field ( ... , example = \"safetycone\" , description = \"name of object to detect\" ) image : str = Field ( ... , description = \"base64-encoded-image\" ) class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 5.12 , description = \"X-coordinate\" ) y : float = Field ( ... , example = 20.56 , description = \"Y-coordinate\" ) width : int = Field ( ... , example = 500 , description = \"width in pixel\" ) height : int = Field ( ... , example = 600 , description = \"height in pixel\" ) score : confloat ( gt = 0.0 , lt = 1.0 ) = Field ( ... , example = 0.79 , description = \"confidence score\" ) class ResponseSchema ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] @app . route ( \"/predict\" , methods = [ \"POST\" ]) @api . validate ( body = Request ( RequestSchema ), resp = Response ( HTTP_200 = ResponseSchema ), tags = [ \"API Name\" ] ) def get_predictions (): \"\"\"Short description of endpoint Long description of endpoint\"\"\" JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : api . register ( app ) app . run ( host = \"0.0.0.0\" ) You can refer to my repo for the full example.","title":"OpenAPI"},{"location":"git/","text":"Git Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code Remote URL When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git Branches Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. Commands # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname> Workflow Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch Merge Request Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request. Version Control Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> Commands To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request. Resets Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset HEAD <file.py> Revert If we want to revert commits in the remote, we can use the following. # git push -f origin <commit-hash>:<branch-name> git push -f origin 4a30214b819:master Workflow The various git commands and how they interact at different stages are as illustrated below. Release Tags We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags Delete from Git History BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. We cannot delete specific files and folders based on path, the reasoning given by the developer of BFG in stackoverflow . CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git java -jar bfg-<version>.jar --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions java -jar bfg-<version>.jar --delete-folders <foldername> repo-name.git delete all folders with the folder model cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote After uploading the new git history to the repository, it is important to not push any of you or your team's legacy git history back to the repo.","title":"Code Version Control"},{"location":"git/#git","text":"Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code","title":"Git"},{"location":"git/#remote-url","text":"When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git","title":"Remote URL"},{"location":"git/#branches","text":"Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option.","title":"Branches"},{"location":"git/#commands","text":"# check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname>","title":"Commands"},{"location":"git/#workflow","text":"Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch","title":"Workflow"},{"location":"git/#merge-request","text":"Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Merge Request"},{"location":"git/#version-control","text":"Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename>","title":"Version Control"},{"location":"git/#commands_1","text":"To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request.","title":"Commands"},{"location":"git/#resets","text":"Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset HEAD <file.py>","title":"Resets"},{"location":"git/#revert","text":"If we want to revert commits in the remote, we can use the following. # git push -f origin <commit-hash>:<branch-name> git push -f origin 4a30214b819:master","title":"Revert"},{"location":"git/#workflow_1","text":"The various git commands and how they interact at different stages are as illustrated below.","title":"Workflow"},{"location":"git/#release-tags","text":"We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags","title":"Release Tags"},{"location":"git/#delete-from-git-history","text":"BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. We cannot delete specific files and folders based on path, the reasoning given by the developer of BFG in stackoverflow . CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git java -jar bfg-<version>.jar --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions java -jar bfg-<version>.jar --delete-folders <foldername> repo-name.git delete all folders with the folder model cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote After uploading the new git history to the repository, it is important to not push any of you or your team's legacy git history back to the repo.","title":"Delete from Git History"},{"location":"gpu/","text":"Using GPU Neural Networks are processed using GPU as they can better compute multiple parallel processes. Nvidia graphic cards are currently the defacto GPU used, with it supporting all neural network libraries. Unfortunately, it is sometimes a pain to install CUDA and get it working with our libraries, so hopefully this page serves as a guide to overcome the difficulties. Compatibility For tensorflow and pytorch, they have compatibility setups for CUDA versions, and also for the former; cudnn, python and the compiler. You can view the requirements from the docs of tensorflow and pytorch . GPU setups compatible with tensorflow versions Remove/Uninstall CUDA Sometimes, we need to remove legacy versions of CUDA so that we can install a new one to work with our libraries. To do that, we can use the following commands. sudo rm /etc/apt/sources.list.d/cuda* sudo apt remove --autoremove nvidia-cuda-toolkit sudo apt remove --autoremove nvidia-* Installing CUDA 10.1 in Ubuntu 18 & 20 A great help from this medium article, CUDA 10.1 installation on Ubuntu 20.04 . First we setup the correct CUDA PPA on your system. sudo apt update sudo add-apt-repository ppa:graphics-driverssudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list' Then, install CUDA 10.1 packages sudo apt update sudo apt install cuda-10-1 sudo apt install libcudnn7 As the last step one need to specify PATH to CUDA in \u2018.profile\u2019 file. Open the file by running sudo nano ~/.profile And add the following lines at the end of the file. # set PATH for cuda 10.1 installation if [ -d \"/usr/local/cuda-10.1/bin/\" ] ; then export PATH = /usr/local/cuda-10.1/bin ${ PATH :+: ${ PATH }} export LD_LIBRARY_PATH = /usr/local/cuda-10.1/lib64 ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} fi Reboot the system. Installing CUDA For a more generic installation, we can do the following, though I have personally not succeeded in getting this work. Haha. After verifying the version, go to Nvidia's CUDA website , and select the version required. You will be led to a selection screen to specify your OS. As part of the CUDA installation, cudnn , the relevant driver, and nvidia-smi are installed. System Verification To check that CUDA is installed properly at the system level, we can use the following commands in the terminal to check. Noun Desc cat /proc/driver/nvidia/version Verify driver & gcc version nvcc -V Verify CUDA toolkit version nvidia-smi CUDA & driver versions, check memory usage Library Verification To check that tensorflow and pytorch can detect the GPU and use it, we can use the following commands. # tensorflow import tensorflow as tf # return empty list if not available tf . config . list_physical_devices ( 'GPU' ) # pytorch import torch torch . cuda . is_available () # True torch . cuda . current_device () # 0 torch . cuda . device ( 0 ) # <torch.cuda.device at 0x7efce0b03be0> torch . cuda . device_count () # 1 torch . cuda . get_device_name ( 0 ) # 'GeForce GTX 950M' Kill GPU processes Sometimes our GPU might be out of memory. We can use nvidia-smi to check what processes and application used are taking up the memory so that we can close the app. If we are unable to locate the application, we can kill the process directly as follows. sudo kill - 9 < process id > Why so Hard? As to why it is so hard to get install CUDA. Watch this famous YouTube video .","title":"Using GPU"},{"location":"gpu/#using-gpu","text":"Neural Networks are processed using GPU as they can better compute multiple parallel processes. Nvidia graphic cards are currently the defacto GPU used, with it supporting all neural network libraries. Unfortunately, it is sometimes a pain to install CUDA and get it working with our libraries, so hopefully this page serves as a guide to overcome the difficulties.","title":"Using GPU"},{"location":"gpu/#compatibility","text":"For tensorflow and pytorch, they have compatibility setups for CUDA versions, and also for the former; cudnn, python and the compiler. You can view the requirements from the docs of tensorflow and pytorch . GPU setups compatible with tensorflow versions","title":"Compatibility"},{"location":"gpu/#removeuninstall-cuda","text":"Sometimes, we need to remove legacy versions of CUDA so that we can install a new one to work with our libraries. To do that, we can use the following commands. sudo rm /etc/apt/sources.list.d/cuda* sudo apt remove --autoremove nvidia-cuda-toolkit sudo apt remove --autoremove nvidia-*","title":"Remove/Uninstall CUDA"},{"location":"gpu/#installing-cuda-101-in-ubuntu-18-20","text":"A great help from this medium article, CUDA 10.1 installation on Ubuntu 20.04 . First we setup the correct CUDA PPA on your system. sudo apt update sudo add-apt-repository ppa:graphics-driverssudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list' Then, install CUDA 10.1 packages sudo apt update sudo apt install cuda-10-1 sudo apt install libcudnn7 As the last step one need to specify PATH to CUDA in \u2018.profile\u2019 file. Open the file by running sudo nano ~/.profile And add the following lines at the end of the file. # set PATH for cuda 10.1 installation if [ -d \"/usr/local/cuda-10.1/bin/\" ] ; then export PATH = /usr/local/cuda-10.1/bin ${ PATH :+: ${ PATH }} export LD_LIBRARY_PATH = /usr/local/cuda-10.1/lib64 ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} fi Reboot the system.","title":"Installing CUDA 10.1 in Ubuntu 18 &amp; 20"},{"location":"gpu/#installing-cuda","text":"For a more generic installation, we can do the following, though I have personally not succeeded in getting this work. Haha. After verifying the version, go to Nvidia's CUDA website , and select the version required. You will be led to a selection screen to specify your OS. As part of the CUDA installation, cudnn , the relevant driver, and nvidia-smi are installed.","title":"Installing CUDA"},{"location":"gpu/#system-verification","text":"To check that CUDA is installed properly at the system level, we can use the following commands in the terminal to check. Noun Desc cat /proc/driver/nvidia/version Verify driver & gcc version nvcc -V Verify CUDA toolkit version nvidia-smi CUDA & driver versions, check memory usage","title":"System Verification"},{"location":"gpu/#library-verification","text":"To check that tensorflow and pytorch can detect the GPU and use it, we can use the following commands. # tensorflow import tensorflow as tf # return empty list if not available tf . config . list_physical_devices ( 'GPU' ) # pytorch import torch torch . cuda . is_available () # True torch . cuda . current_device () # 0 torch . cuda . device ( 0 ) # <torch.cuda.device at 0x7efce0b03be0> torch . cuda . device_count () # 1 torch . cuda . get_device_name ( 0 ) # 'GeForce GTX 950M'","title":"Library Verification"},{"location":"gpu/#kill-gpu-processes","text":"Sometimes our GPU might be out of memory. We can use nvidia-smi to check what processes and application used are taking up the memory so that we can close the app. If we are unable to locate the application, we can kill the process directly as follows. sudo kill - 9 < process id >","title":"Kill GPU processes"},{"location":"gpu/#why-so-hard","text":"As to why it is so hard to get install CUDA. Watch this famous YouTube video .","title":"Why so Hard?"},{"location":"ml-process/","text":"Machine Learning Life Cycle Andrew Ng used another similar flow to describe the process, and explained it quite succinctly using an example speech recognition project. Machine Learning Life Cycle Scoping * Decide to work on a speech recognition for voice search * Key metrics: accuracy, latency, throughput * Estimate resources & timeline Data * Is data labelled consistently? * How much silence before and after each clip? * How to perform volume normalization","title":"Machine Learning Life Cycle"},{"location":"ml-process/#machine-learning-life-cycle","text":"Andrew Ng used another similar flow to describe the process, and explained it quite succinctly using an example speech recognition project. Machine Learning Life Cycle Scoping * Decide to work on a speech recognition for voice search * Key metrics: accuracy, latency, throughput * Estimate resources & timeline Data * Is data labelled consistently? * How much silence before and after each clip? * How to perform volume normalization","title":"Machine Learning Life Cycle"},{"location":"mlflow/","text":"Model Version The process of model development usually involves various iterations. This can be due to changes in the data, model type, and model hyperparameters, all of which can influence the final evaluation metrics selected. Therefore, we need a system to track each of the model version we trained, as well as log all the different information we used for each training cycle. Together with model deployment & monitoring tools, this comprises of the entire ML lifecycle, also known as MLOps. MLFlow There are a number of different model version platforms, like AWS Sagemaker , and DVC Pipelines + CML . However, one of most popular open-source platform is MLFlow , which we will demostrate here. To start, we install MLFlow with pip install mlflow , and to launch the user interface, mlflow ui . For a more verbose specification, to indicate the database, artifacts folder and the server ip (and port) we can do as below. cd <project-directory> mlflow server \\ --backend-store-uri sqlite:///mlflow.db \\ --default-artifact-root ./artifacts \\ --host 0 .0.0.0 -p 5000 Python Code The mlflow tracking and model APIs are very easy to use. It involves the following. Starting CMD Desc mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") point to mlflow server mlflow.set_experiment(\"iris\") create experiment if not exist mlflow.start_run(run_name=\"test run\") give a name for each run (optional) Logging Params, Metrics, Artifacts CMD Desc log_param(\"n_estimators\", n_estimators) log some parameter log_metric(\"f1\", f1) log evaluation metric log_artifacts(\"logs\") log any files Note that we can also (log metrics based on epochs)[https://www.mlflow.org/docs/latest/tracking.html#performance-tracking-with-metrics] of a training so that we can plot the loss over each epoch. This is done by including the step argument in mlflow.log_metrics(key=\"loss\", value=loss, step=epoch) . Logging Model We also also auto-save the model using the model \"flavor\" API. The model signature, which is the input and output names and format, can also be recorded. All these information will also be stored under artifacts. signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) Code import os from statistics import mean import matplotlib.pyplot as plt import mlflow import mlflow.sklearn import pandas as pd from mlflow import log_artifacts , log_metric , log_param from mlflow.models.signature import infer_signature from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import ( f1_score , plot_confusion_matrix , precision_score , recall_score ) from sklearn.model_selection import train_test_split iris = load_iris () X = pd . DataFrame ( iris [ \"data\" ], columns = iris [ \"feature_names\" ]) y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) def train ( n_estimators ): model = RandomForestClassifier ( n_estimators = n_estimators ) model = model . fit ( X_train , y_train ) return model def evaluation ( model , y_predict ): f1 = mean ( f1_score ( y_test , y_predict , average = None )) precision = mean ( precision_score ( y_test , y_predict , average = None )) recall = mean ( recall_score ( y_test , y_predict , average = None )) confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) return acc , f1 , precision , recall , confusion def mlflow_logs ( n_estimators , acc , f1 , precision , recall ): log_param ( \"n_estimators\" , n_estimators ) log_metric ( \"f1\" , f1 ) log_metric ( \"precision\" , precision ) log_metric ( \"recall\" , recall ) log_artifacts ( \"logs\" ) if __name__ == \"__main__\" : # start mlflow mlflow . set_tracking_uri ( \"http://127.0.0.1:5000\" ) mlflow . set_experiment ( \"iris\" ) mlflow . start_run ( run_name = \"test run\" ) n_estimators = 99 # train, predict, evaluate model = train ( n_estimators ) y_predict = model . predict ( X_test ) acc , f1 , precision , recall , confusion = evaluation ( model , y_predict ) # mlflow logging mlflow_logs ( n_estimators , acc , f1 , precision , recall ) signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) MLFlow UI This is the interface seen when MLFlow launches. Main UI Clicking on a specific run leads to its details. Run Details And any artifacts stored. This can include any file, including images. Artifacts Or the model. Model artifact & signature If we registered the model, it will be recorded in the model registry. Model registry","title":"Model Version Control"},{"location":"mlflow/#model-version","text":"The process of model development usually involves various iterations. This can be due to changes in the data, model type, and model hyperparameters, all of which can influence the final evaluation metrics selected. Therefore, we need a system to track each of the model version we trained, as well as log all the different information we used for each training cycle. Together with model deployment & monitoring tools, this comprises of the entire ML lifecycle, also known as MLOps.","title":"Model Version"},{"location":"mlflow/#mlflow","text":"There are a number of different model version platforms, like AWS Sagemaker , and DVC Pipelines + CML . However, one of most popular open-source platform is MLFlow , which we will demostrate here. To start, we install MLFlow with pip install mlflow , and to launch the user interface, mlflow ui . For a more verbose specification, to indicate the database, artifacts folder and the server ip (and port) we can do as below. cd <project-directory> mlflow server \\ --backend-store-uri sqlite:///mlflow.db \\ --default-artifact-root ./artifacts \\ --host 0 .0.0.0 -p 5000","title":"MLFlow"},{"location":"mlflow/#python-code","text":"The mlflow tracking and model APIs are very easy to use. It involves the following. Starting CMD Desc mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") point to mlflow server mlflow.set_experiment(\"iris\") create experiment if not exist mlflow.start_run(run_name=\"test run\") give a name for each run (optional) Logging Params, Metrics, Artifacts CMD Desc log_param(\"n_estimators\", n_estimators) log some parameter log_metric(\"f1\", f1) log evaluation metric log_artifacts(\"logs\") log any files Note that we can also (log metrics based on epochs)[https://www.mlflow.org/docs/latest/tracking.html#performance-tracking-with-metrics] of a training so that we can plot the loss over each epoch. This is done by including the step argument in mlflow.log_metrics(key=\"loss\", value=loss, step=epoch) . Logging Model We also also auto-save the model using the model \"flavor\" API. The model signature, which is the input and output names and format, can also be recorded. All these information will also be stored under artifacts. signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) Code import os from statistics import mean import matplotlib.pyplot as plt import mlflow import mlflow.sklearn import pandas as pd from mlflow import log_artifacts , log_metric , log_param from mlflow.models.signature import infer_signature from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import ( f1_score , plot_confusion_matrix , precision_score , recall_score ) from sklearn.model_selection import train_test_split iris = load_iris () X = pd . DataFrame ( iris [ \"data\" ], columns = iris [ \"feature_names\" ]) y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) def train ( n_estimators ): model = RandomForestClassifier ( n_estimators = n_estimators ) model = model . fit ( X_train , y_train ) return model def evaluation ( model , y_predict ): f1 = mean ( f1_score ( y_test , y_predict , average = None )) precision = mean ( precision_score ( y_test , y_predict , average = None )) recall = mean ( recall_score ( y_test , y_predict , average = None )) confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) return acc , f1 , precision , recall , confusion def mlflow_logs ( n_estimators , acc , f1 , precision , recall ): log_param ( \"n_estimators\" , n_estimators ) log_metric ( \"f1\" , f1 ) log_metric ( \"precision\" , precision ) log_metric ( \"recall\" , recall ) log_artifacts ( \"logs\" ) if __name__ == \"__main__\" : # start mlflow mlflow . set_tracking_uri ( \"http://127.0.0.1:5000\" ) mlflow . set_experiment ( \"iris\" ) mlflow . start_run ( run_name = \"test run\" ) n_estimators = 99 # train, predict, evaluate model = train ( n_estimators ) y_predict = model . predict ( X_test ) acc , f1 , precision , recall , confusion = evaluation ( model , y_predict ) # mlflow logging mlflow_logs ( n_estimators , acc , f1 , precision , recall ) signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature )","title":"Python Code"},{"location":"mlflow/#mlflow-ui","text":"This is the interface seen when MLFlow launches. Main UI Clicking on a specific run leads to its details. Run Details And any artifacts stored. This can include any file, including images. Artifacts Or the model. Model artifact & signature If we registered the model, it will be recorded in the model registry. Model registry","title":"MLFlow UI"},{"location":"mlops/","text":"MLOps A machine learning lifecycle doesn't follow the typical software development lifecycle, though the essential parts of automation , traceability , and monitoring are still key to its success. This is where a modified form of CI/CD for machine learning comes in, i.e. MLOps. Maturity Model A maturity model is a tool that helps people assess the current effectiveness of a person or group and supports figuring out what capabilities they need to acquire next in order to improve their performance. We can use it as a metric for establishing the progressive requirements needed to measure the maturity of a machine learning production environment and its associated processes. Both Google and Azure provide a good detail description of an MLOps maturity model. I have taken part of Azure's and summarised it below. - 1: NO MLOps 2: DevOps, No MLOps 3: Auto Training 4: Auto Deployment 5: Auto Retraining Highlights Entire lifecycle is manual Automated app builds and unit-tests Training is centrally managed & traceable Full traceability from deployment back to original data Full system automation and monitoring","title":"MLOps"},{"location":"mlops/#mlops","text":"A machine learning lifecycle doesn't follow the typical software development lifecycle, though the essential parts of automation , traceability , and monitoring are still key to its success. This is where a modified form of CI/CD for machine learning comes in, i.e. MLOps.","title":"MLOps"},{"location":"mlops/#maturity-model","text":"A maturity model is a tool that helps people assess the current effectiveness of a person or group and supports figuring out what capabilities they need to acquire next in order to improve their performance. We can use it as a metric for establishing the progressive requirements needed to measure the maturity of a machine learning production environment and its associated processes. Both Google and Azure provide a good detail description of an MLOps maturity model. I have taken part of Azure's and summarised it below. - 1: NO MLOps 2: DevOps, No MLOps 3: Auto Training 4: Auto Deployment 5: Auto Retraining Highlights Entire lifecycle is manual Automated app builds and unit-tests Training is centrally managed & traceable Full traceability from deployment back to original data Full system automation and monitoring","title":"Maturity Model"},{"location":"monitoring/","text":"Monitoring After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source Monitoring Systems Prometheus Prometheus is the go-to open source metrics monitoring platform for ML systems. Grafana Grafana can be integrated with Prometheus to display the metrics in interactive visualizations. Kibana Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. This helps to detect concept or data drifts. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use. Evidently Evidently helps evaluate and monitor machine learning models in production. It generates interactive reports or JSON profiles from pandas DataFramesor csv files. You can use visual reports for ad hoc analysis, debugging and team sharing, and JSON profiles to integrate Evidently in prediction pipelines or with other visualization tools.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source","title":"Monitoring"},{"location":"monitoring/#monitoring-systems","text":"","title":"Monitoring Systems"},{"location":"monitoring/#prometheus","text":"Prometheus is the go-to open source metrics monitoring platform for ML systems.","title":"Prometheus"},{"location":"monitoring/#grafana","text":"Grafana can be integrated with Prometheus to display the metrics in interactive visualizations.","title":"Grafana"},{"location":"monitoring/#kibana","text":"Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. This helps to detect concept or data drifts. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use.","title":"Kibana"},{"location":"monitoring/#evidently","text":"Evidently helps evaluate and monitor machine learning models in production. It generates interactive reports or JSON profiles from pandas DataFramesor csv files. You can use visual reports for ad hoc analysis, debugging and team sharing, and JSON profiles to integrate Evidently in prediction pipelines or with other visualization tools.","title":"Evidently"},{"location":"opt-data/","text":"Data Structures Pandas Pandas documentation lists ways to enhance the performances in terms of both query and storage. Storage & Memory We can reduce the memory required to store the dataframes if we reduce the datatype to its minimal or appropriate ones. This can also make faster queries if categorical dtypes are assigned properly. def reduce_df ( df , cat_col = [], cat_percent = 0.5 , verbose = True ): \"\"\"\"maximium data memory optimization Args: cat_col (list): list of columm names to convert to category dtype cat_percent (float): only convert if percentage duplicates more than stated verbose (bool): prints reduction percentage \"\"\" mem_start = df . memory_usage () . sum () size = len ( df ) for col in df . columns : dtype_ = df [ col ] . dtype if dtype_ == float : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'float' ) elif dtype_ == int : min_ = df [ col ] . min () if min_ > 0 : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'integer' ) else : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'unsigned' ) elif dtype_ == object : if col in cat_col : dup_percent = 1 - ( len ( df [ col ] . unique ()) / size ) if dup_percent >= cat_percent : df [ col ] = df [ col ] . astype ( 'category' ) if verbose : mem_end = df . memory_usage () . sum () print ( \" {:.2f}% o f original size is reduced\" . format (( mem_start - mem_end ) / mem_start * 100 )) return df Looping As a rule of thumb, never use crude for loops to iterate and change values in the dataframe. The apply method is already internally optimized, like using iterators in Cython. We can also used vectorization on pandas series or numpy arrays as suggested by this author . import numpy as np import pandas as pd # Define a basic Haversine distance formula def haversine ( lat1 , lon1 , lat2 , lon2 ): MILES = 3959 lat1 , lon1 , lat2 , lon2 = map ( np . deg2rad , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a )) total_miles = MILES * c return total_miles # optimized on apply-lambda df [ 'distance' ] = df . apply ( lambda row : haversine ( 40.671 , - 73.985 , row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) # optimized pandas series df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ], df [ 'longitude' ]) # optimized numpy array df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ] . values , df [ 'longitude' ] . values ) Querying In pandas, there are various ways of querying, and the speed various with each type. In the example below, the fastest way is to convert the query column into a category data-type as it contains many duplicates, hence conversion into integers underthehood make it query more efficient. However, this might not always be the case, and using isin or in1d might suffice. More information here . # Unoptimized, 0.01288s result = df [ df [ \"antecedents\" ] == item ] # Optimized Queries # ------------------- # with pandas .query, 0.00722s, x1.8 result = df . query ( \"antecedents == @item\" ) # with isin, 0.00440s, x2.9 result = df [ df [ \"antecedents\" ] . isin ([ item ])] # with indexing, 0.004342s x3.0 df = df . set_index ( \"antecedents\" ) result = df [ df . index . isin ([ item ])] # with numpy, 0.004175s, x3.1 result = df [ np . in1d ( df [ \"antecedents\" ] . values , [ item ])] # using category datatype, 0.000608s, x211.8 df [ \"antecedents\" ] = df [ \"antecedents\" ] . astype ( 'category' ) result = df [ df [ \"antecedents\" ] == item ] Graph A graph data structure enables much faster query if there is a network of interconnectivity between each other, known as nodes . Information stored between each connection of two nodes are called links . A simple graph structure in python can be created using the networkx library. However, this does not scale when the data become huge. For that, a graph database like neo4j is more appropriate. import networkx as nx import pandas as pd df = pd . read_csv ( \"association_rules.csv\" ) # convert dataframe to networkx graph graph = nx . convert_matrix . from_pandas_edgelist ( df , \"antecedents\" , \"consequents\" , [ \"antecedent support\" , \"consequent support\" , \"support\" , \"confidence\" , \"lift\" , \"leverage\" , \"conviction\" , ], create_using = nx . DiGraph , ) # query result = list ( graph [ item ])","title":"Data"},{"location":"opt-data/#data-structures","text":"","title":"Data Structures"},{"location":"opt-data/#pandas","text":"Pandas documentation lists ways to enhance the performances in terms of both query and storage.","title":"Pandas"},{"location":"opt-data/#storage-memory","text":"We can reduce the memory required to store the dataframes if we reduce the datatype to its minimal or appropriate ones. This can also make faster queries if categorical dtypes are assigned properly. def reduce_df ( df , cat_col = [], cat_percent = 0.5 , verbose = True ): \"\"\"\"maximium data memory optimization Args: cat_col (list): list of columm names to convert to category dtype cat_percent (float): only convert if percentage duplicates more than stated verbose (bool): prints reduction percentage \"\"\" mem_start = df . memory_usage () . sum () size = len ( df ) for col in df . columns : dtype_ = df [ col ] . dtype if dtype_ == float : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'float' ) elif dtype_ == int : min_ = df [ col ] . min () if min_ > 0 : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'integer' ) else : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'unsigned' ) elif dtype_ == object : if col in cat_col : dup_percent = 1 - ( len ( df [ col ] . unique ()) / size ) if dup_percent >= cat_percent : df [ col ] = df [ col ] . astype ( 'category' ) if verbose : mem_end = df . memory_usage () . sum () print ( \" {:.2f}% o f original size is reduced\" . format (( mem_start - mem_end ) / mem_start * 100 )) return df","title":"Storage &amp; Memory"},{"location":"opt-data/#looping","text":"As a rule of thumb, never use crude for loops to iterate and change values in the dataframe. The apply method is already internally optimized, like using iterators in Cython. We can also used vectorization on pandas series or numpy arrays as suggested by this author . import numpy as np import pandas as pd # Define a basic Haversine distance formula def haversine ( lat1 , lon1 , lat2 , lon2 ): MILES = 3959 lat1 , lon1 , lat2 , lon2 = map ( np . deg2rad , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a )) total_miles = MILES * c return total_miles # optimized on apply-lambda df [ 'distance' ] = df . apply ( lambda row : haversine ( 40.671 , - 73.985 , row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) # optimized pandas series df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ], df [ 'longitude' ]) # optimized numpy array df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ] . values , df [ 'longitude' ] . values )","title":"Looping"},{"location":"opt-data/#querying","text":"In pandas, there are various ways of querying, and the speed various with each type. In the example below, the fastest way is to convert the query column into a category data-type as it contains many duplicates, hence conversion into integers underthehood make it query more efficient. However, this might not always be the case, and using isin or in1d might suffice. More information here . # Unoptimized, 0.01288s result = df [ df [ \"antecedents\" ] == item ] # Optimized Queries # ------------------- # with pandas .query, 0.00722s, x1.8 result = df . query ( \"antecedents == @item\" ) # with isin, 0.00440s, x2.9 result = df [ df [ \"antecedents\" ] . isin ([ item ])] # with indexing, 0.004342s x3.0 df = df . set_index ( \"antecedents\" ) result = df [ df . index . isin ([ item ])] # with numpy, 0.004175s, x3.1 result = df [ np . in1d ( df [ \"antecedents\" ] . values , [ item ])] # using category datatype, 0.000608s, x211.8 df [ \"antecedents\" ] = df [ \"antecedents\" ] . astype ( 'category' ) result = df [ df [ \"antecedents\" ] == item ]","title":"Querying"},{"location":"opt-data/#graph","text":"A graph data structure enables much faster query if there is a network of interconnectivity between each other, known as nodes . Information stored between each connection of two nodes are called links . A simple graph structure in python can be created using the networkx library. However, this does not scale when the data become huge. For that, a graph database like neo4j is more appropriate. import networkx as nx import pandas as pd df = pd . read_csv ( \"association_rules.csv\" ) # convert dataframe to networkx graph graph = nx . convert_matrix . from_pandas_edgelist ( df , \"antecedents\" , \"consequents\" , [ \"antecedent support\" , \"consequent support\" , \"support\" , \"confidence\" , \"lift\" , \"leverage\" , \"conviction\" , ], create_using = nx . DiGraph , ) # query result = list ( graph [ item ])","title":"Graph"},{"location":"opt-model/","text":"Model Optimization There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision. We will concentrate on a few optimization techniques namely: Quantization CPU-GPU Flow TensorRT On the 2 popular libraries, pytorch & tensorflow. Keras is part of tensorflow's API already, so whatever tensorflow can do, keras should be able to too. Quantization Pytorch docs CPU Flow Control Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . import tensorflow as tf import numpy as np image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) For pytorch we can use the torch.autograd.profiler . import torch import torchvision.models as models import torch.autograd.profiler as profiler model = models . resnet18 () . cuda () inputs = torch . randn ( 5 , 3 , 224 , 224 ) . cuda () with profiler . profile ( record_shapes = True ) as prof : with profiler . record_function ( \"model_inference\" ): model ( inputs ) prof . export_chrome_trace ( \"trace.json\" ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0' References Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace Pytorch profiler Tensorflow profiler Tensorflow Model Optimization Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization . TensorRT TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below. Layer Fusion During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT Quantization Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. Save Model An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory> Benchmark Functions To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model {input_saved_model_dir} ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds TRT Conversion Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory> Others There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO) References Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"Model"},{"location":"opt-model/#model-optimization","text":"There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision. We will concentrate on a few optimization techniques namely: Quantization CPU-GPU Flow TensorRT On the 2 popular libraries, pytorch & tensorflow. Keras is part of tensorflow's API already, so whatever tensorflow can do, keras should be able to too.","title":"Model Optimization"},{"location":"opt-model/#quantization","text":"Pytorch docs","title":"Quantization"},{"location":"opt-model/#cpu-flow-control","text":"Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . import tensorflow as tf import numpy as np image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) For pytorch we can use the torch.autograd.profiler . import torch import torchvision.models as models import torch.autograd.profiler as profiler model = models . resnet18 () . cuda () inputs = torch . randn ( 5 , 3 , 224 , 224 ) . cuda () with profiler . profile ( record_shapes = True ) as prof : with profiler . record_function ( \"model_inference\" ): model ( inputs ) prof . export_chrome_trace ( \"trace.json\" ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0'","title":"CPU Flow Control"},{"location":"opt-model/#references","text":"Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace Pytorch profiler Tensorflow profiler","title":"References"},{"location":"opt-model/#tensorflow-model-optimization","text":"Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization .","title":"Tensorflow Model Optimization"},{"location":"opt-model/#tensorrt","text":"TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below.","title":"TensorRT"},{"location":"opt-model/#layer-fusion","text":"During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT","title":"Layer Fusion"},{"location":"opt-model/#quantization_1","text":"Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation.","title":"Quantization"},{"location":"opt-model/#save-model","text":"An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory>","title":"Save Model"},{"location":"opt-model/#benchmark-functions","text":"To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model {input_saved_model_dir} ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds","title":"Benchmark Functions"},{"location":"opt-model/#trt-conversion","text":"Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory>","title":"TRT Conversion"},{"location":"opt-model/#others","text":"There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO)","title":"Others"},{"location":"opt-model/#references_1","text":"Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"References"},{"location":"opt-para-concurr/","text":"Concurrency & Parallelism Parallel programming means executing operations while using multiple CPU's processes (cores) or using multiple threads in a process. This speeds up the operation multitudes faster based on the number of processes or threads launched. Python's GIL problem Python has a Global Interpreter Lock (GIL), which prevent two threads from executing simultaneously in the same program. However, libraries like numpy bypass this limitation by running external code in C. Because of this GIL limitation, threads provide no benefit for CPU intensive tasks , and is only used for Input/Ouput (IO) tasks . CPU-limiting processes refer to one where most part of the life is spent in cpu, while IO-limiting processes refer to one where most part of the life is spent in i/o state, i.e. instructing another program to run something. Why does IO task work when it can also only use a single thread in a process at a time? Let's take an example, one thread fires off a request to a URL and while it is waiting for a response, that thread can be swapped out for another thread that fires another request to another URL. Since a thread doesn\u2019t have to do anything until it receives a response, it doesn\u2019t really matter that only one thread is executing at a given time. This makes multi-threading in Python as a concurrent or asychronous process. Given all these, we know when to use multi-processes and threads to run tasks with improved latency. Multiprocess Multithread more overhead than threads as opening and closing processes takes more time fast as they share memory space and efficiently read and write to the same variables only for CPU-intensive tasks because of overheads only for IO-intensive tasks because fo GIL E.g. complex calculations E.g. networking, file read-write, database query True Parallel Process Concurrent Process Great detailed explanations are provided by Brendan Fortuner , Thilina Rajapakse , stackoverflow , and testdrive.io 1 & 2 . Examples Below are two examples of embarassingly parallel tasks, one a CPU-bound job... def compute_x ( x , y = 2 ): from time import sleep sleep ( 0.8 ) return ( x , y ) list_to_iterate = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] sum_all_x = [ compute_x ( i ) for i in list_to_iterate ] and an IO-bound job where it is requesting data from a website. import requests wiki_page_urls = [ \"https://en.wikipedia.org/wiki/Ocean\" , \"https://en.wikipedia.org/wiki/Island\" , \"https://en.wikipedia.org/wiki/this_page_does_not_exist\" , \"https://en.wikipedia.org/wiki/Shark\" , ] def get_wiki_page_existence ( wiki_page_url , timeout = 10 ): response = requests . get ( url = wiki_page_url , timeout = timeout ) page_status = \"unknown\" if response . status_code == 200 : page_status = \"exists\" elif response . status_code == 404 : page_status = \"does not exist\" return wiki_page_url + \" - \" + page_status Joblib Joblib makes embarrassingly parallel for loops written in a list comprehension very simple, with the following syntax. from joblib import Parallel , delayed variable_name = Parallel ( n_jobs =< no_of_jobs > )( delayed ( < func_name > )( < arg1 > , < arg2 > , < arg3 > ) for i in < list_to_iterate > ) Parallel We can run this in parallel with multiprocessing in joblib as this. By default it uses the backend loky , which is more robust than multiprocessing , taken from the older multiprocessing.Pool library. However, it might not always be faster. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 )( delayed ( compute_x )( i ) for i in list_to_iterate ) Concurrent However, if this is an I/O intensive task, using threading to run it asynchronously is preferred. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 , backend = \"threading\" )( delayed ( compute_x )( i ) for i in list_to_iterate ) Concurrent Futures concurrent.futures is an in-built library that can easily run multi-processing or threading tasks. Parallel from concurrent.futures import ProcessPoolExecutor , as_completed def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] for i in list_to_iterate : executor . submit ( compute_x , x = i ) result = [ future . result () for future in as_completed ( futures )] return result # alternatively, we can use the map function for a more concise result # however, it does not allow multiple args to the worker function def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = executor . map ( compute_x , list_to_iterate ) result = [ future for future in futures ] return result # to do that, we need to use another library to wrap the worker function # and additional args together from functools import partial def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : wrap = partial ( compute_x , y = 3 ) futures = executor . map ( wrap , list_to_iterate ) result = [ future for future in futures ] return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate ) Concurrent from concurrent.futures import ThreadPoolExecutor , as_completed with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [] for url in wiki_page_urls : futures . append ( executor . submit ( get_wiki_page_existence , wiki_page_url = url )) result = [ future . result () for future in as_completed ( futures )] Multiprocessing The multiprocessing module is another in-built library that supports both multi-processing multiprocessing.Pool or threading multiprocessing.pool.ThreadPool tasks. Parallel import multiprocessing as mp def multiproc ( list_to_iterate , workers = 2 ): pool = mp . Pool ( workers ) result = pool . map ( compute_x , list_to_iterate ) return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate ) Concurrent import multiprocessing as mp def multithread ( list_to_iterate , workers = 2 ): pool = mp . pool . ThreadPool ( workers ) result = pool . map ( get_wiki_page_existence , wiki_page_urls ) return result if __name__ == \"__main__\" : result = multithread ( list_to_iterate ) Numba Numba is one of the unique cases where has true parallel multi-threading, being able to overcome the GIL.","title":"Concurrency & Parallelism"},{"location":"opt-para-concurr/#concurrency-parallelism","text":"Parallel programming means executing operations while using multiple CPU's processes (cores) or using multiple threads in a process. This speeds up the operation multitudes faster based on the number of processes or threads launched.","title":"Concurrency &amp; Parallelism"},{"location":"opt-para-concurr/#pythons-gil-problem","text":"Python has a Global Interpreter Lock (GIL), which prevent two threads from executing simultaneously in the same program. However, libraries like numpy bypass this limitation by running external code in C. Because of this GIL limitation, threads provide no benefit for CPU intensive tasks , and is only used for Input/Ouput (IO) tasks . CPU-limiting processes refer to one where most part of the life is spent in cpu, while IO-limiting processes refer to one where most part of the life is spent in i/o state, i.e. instructing another program to run something. Why does IO task work when it can also only use a single thread in a process at a time? Let's take an example, one thread fires off a request to a URL and while it is waiting for a response, that thread can be swapped out for another thread that fires another request to another URL. Since a thread doesn\u2019t have to do anything until it receives a response, it doesn\u2019t really matter that only one thread is executing at a given time. This makes multi-threading in Python as a concurrent or asychronous process. Given all these, we know when to use multi-processes and threads to run tasks with improved latency. Multiprocess Multithread more overhead than threads as opening and closing processes takes more time fast as they share memory space and efficiently read and write to the same variables only for CPU-intensive tasks because of overheads only for IO-intensive tasks because fo GIL E.g. complex calculations E.g. networking, file read-write, database query True Parallel Process Concurrent Process Great detailed explanations are provided by Brendan Fortuner , Thilina Rajapakse , stackoverflow , and testdrive.io 1 & 2 .","title":"Python's GIL problem"},{"location":"opt-para-concurr/#examples","text":"Below are two examples of embarassingly parallel tasks, one a CPU-bound job... def compute_x ( x , y = 2 ): from time import sleep sleep ( 0.8 ) return ( x , y ) list_to_iterate = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] sum_all_x = [ compute_x ( i ) for i in list_to_iterate ] and an IO-bound job where it is requesting data from a website. import requests wiki_page_urls = [ \"https://en.wikipedia.org/wiki/Ocean\" , \"https://en.wikipedia.org/wiki/Island\" , \"https://en.wikipedia.org/wiki/this_page_does_not_exist\" , \"https://en.wikipedia.org/wiki/Shark\" , ] def get_wiki_page_existence ( wiki_page_url , timeout = 10 ): response = requests . get ( url = wiki_page_url , timeout = timeout ) page_status = \"unknown\" if response . status_code == 200 : page_status = \"exists\" elif response . status_code == 404 : page_status = \"does not exist\" return wiki_page_url + \" - \" + page_status","title":"Examples"},{"location":"opt-para-concurr/#joblib","text":"Joblib makes embarrassingly parallel for loops written in a list comprehension very simple, with the following syntax. from joblib import Parallel , delayed variable_name = Parallel ( n_jobs =< no_of_jobs > )( delayed ( < func_name > )( < arg1 > , < arg2 > , < arg3 > ) for i in < list_to_iterate > )","title":"Joblib"},{"location":"opt-para-concurr/#parallel","text":"We can run this in parallel with multiprocessing in joblib as this. By default it uses the backend loky , which is more robust than multiprocessing , taken from the older multiprocessing.Pool library. However, it might not always be faster. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 )( delayed ( compute_x )( i ) for i in list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent","text":"However, if this is an I/O intensive task, using threading to run it asynchronously is preferred. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 , backend = \"threading\" )( delayed ( compute_x )( i ) for i in list_to_iterate )","title":"Concurrent"},{"location":"opt-para-concurr/#concurrent-futures","text":"concurrent.futures is an in-built library that can easily run multi-processing or threading tasks.","title":"Concurrent Futures"},{"location":"opt-para-concurr/#parallel_1","text":"from concurrent.futures import ProcessPoolExecutor , as_completed def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] for i in list_to_iterate : executor . submit ( compute_x , x = i ) result = [ future . result () for future in as_completed ( futures )] return result # alternatively, we can use the map function for a more concise result # however, it does not allow multiple args to the worker function def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = executor . map ( compute_x , list_to_iterate ) result = [ future for future in futures ] return result # to do that, we need to use another library to wrap the worker function # and additional args together from functools import partial def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : wrap = partial ( compute_x , y = 3 ) futures = executor . map ( wrap , list_to_iterate ) result = [ future for future in futures ] return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent_1","text":"from concurrent.futures import ThreadPoolExecutor , as_completed with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [] for url in wiki_page_urls : futures . append ( executor . submit ( get_wiki_page_existence , wiki_page_url = url )) result = [ future . result () for future in as_completed ( futures )]","title":"Concurrent"},{"location":"opt-para-concurr/#multiprocessing","text":"The multiprocessing module is another in-built library that supports both multi-processing multiprocessing.Pool or threading multiprocessing.pool.ThreadPool tasks.","title":"Multiprocessing"},{"location":"opt-para-concurr/#parallel_2","text":"import multiprocessing as mp def multiproc ( list_to_iterate , workers = 2 ): pool = mp . Pool ( workers ) result = pool . map ( compute_x , list_to_iterate ) return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent_2","text":"import multiprocessing as mp def multithread ( list_to_iterate , workers = 2 ): pool = mp . pool . ThreadPool ( workers ) result = pool . map ( get_wiki_page_existence , wiki_page_urls ) return result if __name__ == \"__main__\" : result = multithread ( list_to_iterate )","title":"Concurrent"},{"location":"opt-para-concurr/#numba","text":"Numba is one of the unique cases where has true parallel multi-threading, being able to overcome the GIL.","title":"Numba"},{"location":"opt-spark/","text":"Spark Launching Spark The SparkConf object sets the configuration for the Spark Application. The SparkContext is the entry point of Spark functionality. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The SparkSession is the entry point into the Structured API. from pyspark import SparkContext , SparkConf from pyspark.sql import * from pyspark.sql.functions import * from pyspark.sql.types import * # set master can increase the cores cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) Read Files # from database cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) cnfg . set ( \"spark.jars\" , \"D: \\\\ mysql-connector-java-5.1.49.jar\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) df = ( spark . read . format ( \"jdbc\" ) . options ( url = \"jdbc:mysql://localhost/videoshop\" , driver = \"com.mysql.jdbc.Driver\" , dbtable = \"SomeTableName\" , user = \"venkat\" , password = \"P@ssw0rd1\" ) . load ()) # read csv df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( filepath )) # read json df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . json ( filepath )) We can also define a schema for a particular data. custschema = StructType ([ StructField ( \"Customerid\" , IntegerType (), True ), StructField ( \"CustName\" , StringType (), True ), StructField ( \"MemCat\" , StringType (), True ), StructField ( \"Age\" , IntegerType (), True ), StructField ( \"Gender\" , StringType (), True ), StructField ( \"AmtSpent\" , DoubleType (), True ), StructField ( \"Address\" , StringType (), True ), StructField ( \"City\" , StringType (), True ), StructField ( \"CountryID\" , StringType (), True ), StructField ( \"Title\" , StringType (), True ), StructField ( \"PhoneNo\" , StringType (), True ) ]) df . printSchema () df = ( spark . read . schema ( schema = custschema ) . csv ( inputFilePath )) df . show () Write Files Note that we write files to a folder. The file name is system generated, e.g. \"part-00000-2ee2d8b6-169a-4f48-a503-8b6a2fdedab0-c000.json\". df . write . json ( \"D: \\\\ CountryOUT\" ) Basic Summary # bool for col truncation df . show ( 20 , False ) # schema df . printSchema () # statistics df . describe ( \"MemberCategory\" , \"Gender\" , \"AmountSpent\" , \"Age\" ) . show () Spark SQL The Spark SQL API works in structure similar to SQL. This is used when the data format is well structured, and presentable in a tabular format. Spark SQL is a high level API compared to RDD, therefore is easy to use. # simple query df2 = df . orderBy ( \"Age\" ) . where ( \"Age>20\" ) . select ( df [ \"CustomerID\" ], df [ \"CustomerName\" ], df [ \"Age\" ]) df2 . show ( 200 , False ) # aggregate (single value) tot = df . agg ( sum ( \"AmountSpent\" )) . first ()[ 0 ] tot = df . agg ( avg ( \"Age\" )) . first ()[ 0 ] std = df . agg ( stddev_pop ( \"AmountSpent\" )) . first ()[ 0 ] skw = df . agg ( skewness ( \"AmountSpent\" )) . first ()[ 0 ] # group by df . groupBy ( \"MemberCategory\" ) . sum ( \"AmountSpent\" ) . show ( 200 , False ) We can also do joins from multiple dataframes. customerFilePath = r \"D:\\workspace\\Customer.csv\" countryFilePath = r \"D:\\workspace\\Country.csv\" dfCustomer = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( customerFilePath ) ) dfCountry = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( countryFilePath ) ) joinDF = dfCustomer . join ( dfCountry , \"CountryCode\" ) ( joinDF . select ( \"CustomerID\" , \"CustomerName\" , \"CountryCode\" , \"CountryName\" , \"Currency\" , \"TimeZone\" ) . show ( 300 , False ) ) ) RDD Resilient Distributed Dataset","title":"Spark"},{"location":"opt-spark/#spark","text":"","title":"Spark"},{"location":"opt-spark/#launching-spark","text":"The SparkConf object sets the configuration for the Spark Application. The SparkContext is the entry point of Spark functionality. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The SparkSession is the entry point into the Structured API. from pyspark import SparkContext , SparkConf from pyspark.sql import * from pyspark.sql.functions import * from pyspark.sql.types import * # set master can increase the cores cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc )","title":"Launching Spark"},{"location":"opt-spark/#read-files","text":"# from database cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) cnfg . set ( \"spark.jars\" , \"D: \\\\ mysql-connector-java-5.1.49.jar\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) df = ( spark . read . format ( \"jdbc\" ) . options ( url = \"jdbc:mysql://localhost/videoshop\" , driver = \"com.mysql.jdbc.Driver\" , dbtable = \"SomeTableName\" , user = \"venkat\" , password = \"P@ssw0rd1\" ) . load ()) # read csv df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( filepath )) # read json df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . json ( filepath )) We can also define a schema for a particular data. custschema = StructType ([ StructField ( \"Customerid\" , IntegerType (), True ), StructField ( \"CustName\" , StringType (), True ), StructField ( \"MemCat\" , StringType (), True ), StructField ( \"Age\" , IntegerType (), True ), StructField ( \"Gender\" , StringType (), True ), StructField ( \"AmtSpent\" , DoubleType (), True ), StructField ( \"Address\" , StringType (), True ), StructField ( \"City\" , StringType (), True ), StructField ( \"CountryID\" , StringType (), True ), StructField ( \"Title\" , StringType (), True ), StructField ( \"PhoneNo\" , StringType (), True ) ]) df . printSchema () df = ( spark . read . schema ( schema = custschema ) . csv ( inputFilePath )) df . show ()","title":"Read Files"},{"location":"opt-spark/#write-files","text":"Note that we write files to a folder. The file name is system generated, e.g. \"part-00000-2ee2d8b6-169a-4f48-a503-8b6a2fdedab0-c000.json\". df . write . json ( \"D: \\\\ CountryOUT\" )","title":"Write Files"},{"location":"opt-spark/#basic-summary","text":"# bool for col truncation df . show ( 20 , False ) # schema df . printSchema () # statistics df . describe ( \"MemberCategory\" , \"Gender\" , \"AmountSpent\" , \"Age\" ) . show ()","title":"Basic Summary"},{"location":"opt-spark/#spark-sql","text":"The Spark SQL API works in structure similar to SQL. This is used when the data format is well structured, and presentable in a tabular format. Spark SQL is a high level API compared to RDD, therefore is easy to use. # simple query df2 = df . orderBy ( \"Age\" ) . where ( \"Age>20\" ) . select ( df [ \"CustomerID\" ], df [ \"CustomerName\" ], df [ \"Age\" ]) df2 . show ( 200 , False ) # aggregate (single value) tot = df . agg ( sum ( \"AmountSpent\" )) . first ()[ 0 ] tot = df . agg ( avg ( \"Age\" )) . first ()[ 0 ] std = df . agg ( stddev_pop ( \"AmountSpent\" )) . first ()[ 0 ] skw = df . agg ( skewness ( \"AmountSpent\" )) . first ()[ 0 ] # group by df . groupBy ( \"MemberCategory\" ) . sum ( \"AmountSpent\" ) . show ( 200 , False ) We can also do joins from multiple dataframes. customerFilePath = r \"D:\\workspace\\Customer.csv\" countryFilePath = r \"D:\\workspace\\Country.csv\" dfCustomer = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( customerFilePath ) ) dfCountry = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( countryFilePath ) ) joinDF = dfCustomer . join ( dfCountry , \"CountryCode\" ) ( joinDF . select ( \"CustomerID\" , \"CustomerName\" , \"CountryCode\" , \"CountryName\" , \"Currency\" , \"TimeZone\" ) . show ( 300 , False ) ) )","title":"Spark SQL"},{"location":"opt-spark/#rdd","text":"Resilient Distributed Dataset","title":"RDD"},{"location":"security/","text":"Security Scans If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline. Source Code Scan Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' ) Dependency Scan This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels, nor does it scan for libraries which have their own open-source dependencies. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json Secrets Scan detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/* Pre-Commit Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version: python: python3 repos: - repo: https://github.com/PyCQA/bandit rev: 1.7 hooks: - id: bandit args: [-lll] - repo: https://github.com/Yelp/detect-secrets rev: v0.13.0 hooks: - id: detect-secrets args: [--no-base64-string-scan] Another alternative which I prefer is using tox to compile all the testing and security scans together in a single file. See my section on testing for more. Synk Advisor Synk Advisor is a great site to check the security of open-source libraries. Besides checking using known security vulnerabilities as a metric, it also establishes an overall health score using the library's popularity, maintenance, and community.","title":"Security"},{"location":"security/#security-scans","text":"If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline.","title":"Security Scans"},{"location":"security/#source-code-scan","text":"Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' )","title":"Source Code Scan"},{"location":"security/#dependency-scan","text":"This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels, nor does it scan for libraries which have their own open-source dependencies. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json","title":"Dependency Scan"},{"location":"security/#secrets-scan","text":"detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/*","title":"Secrets Scan"},{"location":"security/#pre-commit","text":"Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version: python: python3 repos: - repo: https://github.com/PyCQA/bandit rev: 1.7 hooks: - id: bandit args: [-lll] - repo: https://github.com/Yelp/detect-secrets rev: v0.13.0 hooks: - id: detect-secrets args: [--no-base64-string-scan] Another alternative which I prefer is using tox to compile all the testing and security scans together in a single file. See my section on testing for more.","title":"Pre-Commit"},{"location":"security/#synk-advisor","text":"Synk Advisor is a great site to check the security of open-source libraries. Besides checking using known security vulnerabilities as a metric, it also establishes an overall health score using the library's popularity, maintenance, and community.","title":"Synk Advisor"},{"location":"serverless/","text":"Serverless Architecture Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice. Lambda & API Gateway In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins . Zappa Zappa is a popular python library used to automatically launch python lambda functions & api-gateways in AWS. VENV Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works. Setup AWS Policy & Keys for Zappa Deployment The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] } How to Use After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user role is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only. Commands Cmd Desc zappa init create a zappa_settings.json file zappa deploy <stage> deploy stage as specified in json zappa update <stage> update stage zappa undeploy <stage> delete lambda & API Gateway zappa package <stage> zip all files together zappa tail <stage> print tail logs from CloudWatch zappa status check status Lambda Execution Role The default execution role created by zappa is too liberal. From the author: It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] } Serverless Framework Serverless Framework is another powerful app to deploy serverless architecture. It's main benefits are being able to deploy to all major cloud providers, i.e. AWS, Azure and GCP, and also supporting multiple languages like nodeJS, Python, Go and Swift. The instructions for deployment are contained in a ymal file called serverless.yml to be stored at the root directory. You can check out this python blog post and a range of other examples in the serverless website for details. Setting Up Install node, and then serverless. # mac brew install node npm install -g serverless","title":"Serverless"},{"location":"serverless/#serverless-architecture","text":"Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice.","title":"Serverless Architecture"},{"location":"serverless/#lambda-api-gateway","text":"In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins .","title":"Lambda &amp; API Gateway"},{"location":"serverless/#zappa","text":"Zappa is a popular python library used to automatically launch python lambda functions & api-gateways in AWS.","title":"Zappa"},{"location":"serverless/#venv","text":"Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works.","title":"VENV"},{"location":"serverless/#setup-aws-policy-keys-for-zappa-deployment","text":"The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] }","title":"Setup AWS Policy &amp; Keys for Zappa Deployment"},{"location":"serverless/#how-to-use","text":"After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user role is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only.","title":"How to Use"},{"location":"serverless/#commands","text":"Cmd Desc zappa init create a zappa_settings.json file zappa deploy <stage> deploy stage as specified in json zappa update <stage> update stage zappa undeploy <stage> delete lambda & API Gateway zappa package <stage> zip all files together zappa tail <stage> print tail logs from CloudWatch zappa status check status","title":"Commands"},{"location":"serverless/#lambda-execution-role","text":"The default execution role created by zappa is too liberal. From the author: It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] }","title":"Lambda Execution Role"},{"location":"serverless/#serverless-framework","text":"Serverless Framework is another powerful app to deploy serverless architecture. It's main benefits are being able to deploy to all major cloud providers, i.e. AWS, Azure and GCP, and also supporting multiple languages like nodeJS, Python, Go and Swift. The instructions for deployment are contained in a ymal file called serverless.yml to be stored at the root directory. You can check out this python blog post and a range of other examples in the serverless website for details.","title":"Serverless Framework"},{"location":"serverless/#setting-up","text":"Install node, and then serverless. # mac brew install node npm install -g serverless","title":"Setting Up"},{"location":"testing-api/","text":"API Tests Pytest To conduct unit or integration tests in pytest, we can use Flask's test_client() method and call it as a fixture. import sys import pytest sys . path . append ( \"project\" ) import app @pytest . fixture ( scope = \"session\" ) def client (): app_ = app . app . test_client () yield app_ def test_health_check ( client ): \"\"\"status check\"\"\" response = client . get ( '/' ) assert response . status_code == 200 assert response . json == { \"status\" : \"ok\" } Pytest Docker API We can use pytest-docker plugin to do automated integrated tests for containerised APIs within pytest. What it does is to launch the container using docker-compose, and apply as a fixture so that all tests can have access to the container(s). It then shuts down the container(s) automatically after all tests are completed. The integration test code below generates a dummy docker-compose-test.yml , so do include this in .gitignore Test Code import os import json import pytest import requests import yaml from requests.exceptions import ConnectionError # ---------------- # Global Variables service_name = \"model-prediction\" container_port = 5000 endpoint = \"prediction\" docker_compose_name = \"docker-compose-api.yml\" dockerfile_name = \"Dockerfile-api\" model_source_path = os . environ [ \"SOURCE_MOUNT\" ] MODEL_FILE = \"model.pkl\" request = \\ { \"your_request\" : \"xxxx\" } # ----------------- # Utility Functions def gen_docker_compose_test (): \"\"\"change docker-compose.yml for testing\"\"\" with open ( docker_compose_name ) as f : list_doc = yaml . safe_load ( f ) # delete image url del list_doc [ \"services\" ][ service_name ][ \"image\" ] # add build folder & dockerfile name list_doc [ \"services\" ][ service_name ][ \"build\" ] = \\ { \"context\" : \"project/\" , \"dockerfile\" : dockerfile_name } # edit model source path folder list_doc [ \"services\" ][ service_name ][ \"volumes\" ][ 0 ][ \"source\" ] = \\ model_source_path # change model name in env list_doc [ \"services\" ][ service_name ][ \"environment\" ] = \\ [ 'WORKERS=max' , f 'MODEL_NAME= {MODEL_FILE} ' ] with open ( \"docker-compose-test.yml\" , \"w\" ) as f : yaml . dump ( list_doc , f ) def is_responsive ( url ): try : response = requests . get ( url ) if response . status_code == 200 : return True except ConnectionError : return False # ------------------------------------- # Setup Fixtures to Launch Container(s) @pytest . fixture ( scope = \"session\" ) def docker_compose_file ( pytestconfig ): \"\"\"set docker-compose*.yml file path\"\"\" gen_docker_compose_test () compose_filepath = os . path . join ( str ( pytestconfig . rootdir ), \"docker-compose-test.yml\" ) return compose_filepath @pytest . fixture ( scope = \"session\" ) def http_service ( docker_ip , docker_services ): \"\"\"ensure HTTP service is up and responsive\"\"\" host_port = docker_services . port_for ( service_name , container_port ) url = f \"http:// {docker_ip} : {host_port} \" docker_services . wait_until_responsive ( timeout = 60.0 , pause = 0.1 , check = lambda : is_responsive ( url ) ) return url @pytest . fixture ( scope = \"session\" ) def docker_cleanup (): \"\"\"remove images & containers, default only 'down -v'\"\"\" return \"down -v --rmi all\" # ------------------------------- # Integration Test Cases def test_sample_request ( http_service ): \"\"\"test sample request and returns more than 1 sku\"\"\" # http_service refers to previous fixture, returning the url url = f \" {http_service} / {endpoint} \" response = requests . post ( url , json = request ) content = json . loads ( response . content ) sku_count = len ( content [ \"sku\" ]) assert sku_count > 0 CI Pipeline Below is an example Gitlab-CI job to run this integration test. There are various points to note: base image to use the stated which have all the essential libraries bundled. If we are mounting a file/model to the API, we need to use the SOURCE_MOUNT path indicated so that docker-in-docker service can access it If we are using tox, in tox.ini, we need to enable access to env variables from host os to access the SOURCE_MOUNT variable using this command passenv = * integration-tests : stage : test timeout : 10m image : sassy19a/dockercompose-py3-awscli2 services : - docker:dind variables : SOURCE_MOUNT : \"/builds/shared/$CI_PROJECT_PATH\" before_script : - pip3 install tox # download model file for mounting - aws s3 cp \"${S3_MODEL_DIRECTORY}/association/rules.pkl\" $SOURCE_MOUNT/rules.pkl script : - tox -e integration artifacts : when : always reports : junit : report.xml rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' changes : - project/*.py - project/Dockerfile-api Postman Postman allows us to write unit-test cases easily. This is done by creating a folder called collections from the left panel, and inside it, we can create individual requests, with a proper unit-test name for them. For each request, under the test tab, we need to define the condition to pass each test. There are some helper scripts on the right panel to assist you. For the collection, we can define variables which we can call in our tests with a double curly brackets {{variable_name}} . To run all the tests in the collection, click on your collection folder, and click Run at the top right corner. We can export the entire unit-tests script as a file json, and pass it to someone. Newman With the collections in json format, we can run that script in the commandline in our CI/CD pipeline, requiring installing npm & then newman, npm install newman . See more from Postman's website . Memory Leak Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker . Profiling After detecting potential issues, we will need to profile the code to see how to can resolve or improve on the issue. Jake VanderPlas provides an excellent description on how to do it using magic commands in jupyter notebooks. Latency Profiler If the latency is not up to par, we can profile our code to identify bottlenecks in the latency line by line. This is done using the library line-profiler . To use the library, we first add the decorator @profile to the functions or methods you want to analyse. @profile def zvalue ( self , df , item , length ): vec = [ 0 ] * ( length + 1 ) start = time () df_filtered = df [ np . in1d ( df [ \"SKU\" ] . values , [ item ])] if len ( df_filtered ) == 1 : z_value = - np . inf else : loc = [ length + i for i in df_filtered . lag . tolist ()] value = df_filtered . quantity . tolist () for i , j in zip ( loc , range ( len ( value ))): vec [ i ] = value [ j ] z_value = self . rolling_zscore ( vec [: - 1 ], vec [ - 1 :], decay = self . decay ) return z_value Then we create the report by using the kernprof -l command; a .lprof report is generated. Following which, we view the report with the python -m line_profiler command. kernprof -l <script_name>.py python -m line_profiler <script_name>.py.lprof Profile Result Memory Profiler Another libray that works similarly but for memory is memory_profiler . Same as before, we just need to add @profile at the function or method, followed by the command python -m memory_profiler <script>.py This will generate a report in the terminal as shown. For variables that consume a lot of memory, but are not required downstream, we can delete it via del <variable-name> . Line # Mem usage Increment Occurences Line Contents ============================================================ 12 76 .180 MiB 76 .180 MiB 1 @profile 13 def load_model ( config ) : 14 76 .180 MiB 0 .000 MiB 1 api_model_name = os.path.join ( config.modeldir, config.model_name ) 15 16 230 .598 MiB 154 .418 MiB 1 model = pickle.load ( open ( api_model_name, \"rb\" )) 17 230 .598 MiB 0 .000 MiB 1 if not isinstance ( model, dict ) : 18 raise ValueError ( \"WARNING: this pickle file at {} is not a dict\" .format ( api_model_name )) 19 20 319 .242 MiB 88 .645 MiB 1 predictor = Predict ( config.PARAMS [ 0 :5 ] , config.l0, api_model_name ) 21 319 .242 MiB 0 .000 MiB 1 return predictor Load Test Load testing is the process of determination of behavior of system when multiple users access it at the same time. Locust is a popular open source load testing library developed in python. We install it using pip install locust . And write the following script, saving it as locustfile.py . from locust import HttpUser , task , between import json with open ( 'test_data/sample_request.json' ) as f : request = json . loads ( f . read ()) class APIUser ( HttpUser ): host = 'http://localhost:5000' wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): apikey = \"yourapikey\" self . client . post ( '/predict' , headers = { \"x-api-key\" : apikey } json = request ) We then launch it in the terminal using the command locust , and then the GUI can be access from localhost:8089 . We can test the total number of users as 10, with a spawn rate of 1. This means that the API will be launched with only 1 user requesting, and after every 3-5 seconds (the wait_time), another new user will be added, until the total of 10 users are reached. Start screen Going to the statistics, a summary of the total users, failures and request per seconds Summary We can also have a visual representation in line charts. Charts For an overall report, with a more detailed statistics and response percentile levels with charts all within a single page, go to Download Data > Download Report to download a html version of the report. Report Other commands are as follows: locust -f locust_files/my_locust_file.py --headless -u 1000 -r 100 -run-time 1h30m CMD Desc -f specify file name & location if they are diff from default --headless only print in console, with no GUI -u no. users to spawn -r spawn rate; no users to add per second -run-time time limit for test","title":"API Testing"},{"location":"testing-api/#api-tests","text":"","title":"API Tests"},{"location":"testing-api/#pytest","text":"To conduct unit or integration tests in pytest, we can use Flask's test_client() method and call it as a fixture. import sys import pytest sys . path . append ( \"project\" ) import app @pytest . fixture ( scope = \"session\" ) def client (): app_ = app . app . test_client () yield app_ def test_health_check ( client ): \"\"\"status check\"\"\" response = client . get ( '/' ) assert response . status_code == 200 assert response . json == { \"status\" : \"ok\" }","title":"Pytest"},{"location":"testing-api/#pytest-docker-api","text":"We can use pytest-docker plugin to do automated integrated tests for containerised APIs within pytest. What it does is to launch the container using docker-compose, and apply as a fixture so that all tests can have access to the container(s). It then shuts down the container(s) automatically after all tests are completed. The integration test code below generates a dummy docker-compose-test.yml , so do include this in .gitignore","title":"Pytest Docker API"},{"location":"testing-api/#test-code","text":"import os import json import pytest import requests import yaml from requests.exceptions import ConnectionError # ---------------- # Global Variables service_name = \"model-prediction\" container_port = 5000 endpoint = \"prediction\" docker_compose_name = \"docker-compose-api.yml\" dockerfile_name = \"Dockerfile-api\" model_source_path = os . environ [ \"SOURCE_MOUNT\" ] MODEL_FILE = \"model.pkl\" request = \\ { \"your_request\" : \"xxxx\" } # ----------------- # Utility Functions def gen_docker_compose_test (): \"\"\"change docker-compose.yml for testing\"\"\" with open ( docker_compose_name ) as f : list_doc = yaml . safe_load ( f ) # delete image url del list_doc [ \"services\" ][ service_name ][ \"image\" ] # add build folder & dockerfile name list_doc [ \"services\" ][ service_name ][ \"build\" ] = \\ { \"context\" : \"project/\" , \"dockerfile\" : dockerfile_name } # edit model source path folder list_doc [ \"services\" ][ service_name ][ \"volumes\" ][ 0 ][ \"source\" ] = \\ model_source_path # change model name in env list_doc [ \"services\" ][ service_name ][ \"environment\" ] = \\ [ 'WORKERS=max' , f 'MODEL_NAME= {MODEL_FILE} ' ] with open ( \"docker-compose-test.yml\" , \"w\" ) as f : yaml . dump ( list_doc , f ) def is_responsive ( url ): try : response = requests . get ( url ) if response . status_code == 200 : return True except ConnectionError : return False # ------------------------------------- # Setup Fixtures to Launch Container(s) @pytest . fixture ( scope = \"session\" ) def docker_compose_file ( pytestconfig ): \"\"\"set docker-compose*.yml file path\"\"\" gen_docker_compose_test () compose_filepath = os . path . join ( str ( pytestconfig . rootdir ), \"docker-compose-test.yml\" ) return compose_filepath @pytest . fixture ( scope = \"session\" ) def http_service ( docker_ip , docker_services ): \"\"\"ensure HTTP service is up and responsive\"\"\" host_port = docker_services . port_for ( service_name , container_port ) url = f \"http:// {docker_ip} : {host_port} \" docker_services . wait_until_responsive ( timeout = 60.0 , pause = 0.1 , check = lambda : is_responsive ( url ) ) return url @pytest . fixture ( scope = \"session\" ) def docker_cleanup (): \"\"\"remove images & containers, default only 'down -v'\"\"\" return \"down -v --rmi all\" # ------------------------------- # Integration Test Cases def test_sample_request ( http_service ): \"\"\"test sample request and returns more than 1 sku\"\"\" # http_service refers to previous fixture, returning the url url = f \" {http_service} / {endpoint} \" response = requests . post ( url , json = request ) content = json . loads ( response . content ) sku_count = len ( content [ \"sku\" ]) assert sku_count > 0","title":"Test Code"},{"location":"testing-api/#ci-pipeline","text":"Below is an example Gitlab-CI job to run this integration test. There are various points to note: base image to use the stated which have all the essential libraries bundled. If we are mounting a file/model to the API, we need to use the SOURCE_MOUNT path indicated so that docker-in-docker service can access it If we are using tox, in tox.ini, we need to enable access to env variables from host os to access the SOURCE_MOUNT variable using this command passenv = * integration-tests : stage : test timeout : 10m image : sassy19a/dockercompose-py3-awscli2 services : - docker:dind variables : SOURCE_MOUNT : \"/builds/shared/$CI_PROJECT_PATH\" before_script : - pip3 install tox # download model file for mounting - aws s3 cp \"${S3_MODEL_DIRECTORY}/association/rules.pkl\" $SOURCE_MOUNT/rules.pkl script : - tox -e integration artifacts : when : always reports : junit : report.xml rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' changes : - project/*.py - project/Dockerfile-api","title":"CI Pipeline"},{"location":"testing-api/#postman","text":"Postman allows us to write unit-test cases easily. This is done by creating a folder called collections from the left panel, and inside it, we can create individual requests, with a proper unit-test name for them. For each request, under the test tab, we need to define the condition to pass each test. There are some helper scripts on the right panel to assist you. For the collection, we can define variables which we can call in our tests with a double curly brackets {{variable_name}} . To run all the tests in the collection, click on your collection folder, and click Run at the top right corner. We can export the entire unit-tests script as a file json, and pass it to someone.","title":"Postman"},{"location":"testing-api/#newman","text":"With the collections in json format, we can run that script in the commandline in our CI/CD pipeline, requiring installing npm & then newman, npm install newman . See more from Postman's website .","title":"Newman"},{"location":"testing-api/#memory-leak","text":"Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker .","title":"Memory Leak"},{"location":"testing-api/#profiling","text":"After detecting potential issues, we will need to profile the code to see how to can resolve or improve on the issue. Jake VanderPlas provides an excellent description on how to do it using magic commands in jupyter notebooks.","title":"Profiling"},{"location":"testing-api/#latency-profiler","text":"If the latency is not up to par, we can profile our code to identify bottlenecks in the latency line by line. This is done using the library line-profiler . To use the library, we first add the decorator @profile to the functions or methods you want to analyse. @profile def zvalue ( self , df , item , length ): vec = [ 0 ] * ( length + 1 ) start = time () df_filtered = df [ np . in1d ( df [ \"SKU\" ] . values , [ item ])] if len ( df_filtered ) == 1 : z_value = - np . inf else : loc = [ length + i for i in df_filtered . lag . tolist ()] value = df_filtered . quantity . tolist () for i , j in zip ( loc , range ( len ( value ))): vec [ i ] = value [ j ] z_value = self . rolling_zscore ( vec [: - 1 ], vec [ - 1 :], decay = self . decay ) return z_value Then we create the report by using the kernprof -l command; a .lprof report is generated. Following which, we view the report with the python -m line_profiler command. kernprof -l <script_name>.py python -m line_profiler <script_name>.py.lprof Profile Result","title":"Latency Profiler"},{"location":"testing-api/#memory-profiler","text":"Another libray that works similarly but for memory is memory_profiler . Same as before, we just need to add @profile at the function or method, followed by the command python -m memory_profiler <script>.py This will generate a report in the terminal as shown. For variables that consume a lot of memory, but are not required downstream, we can delete it via del <variable-name> . Line # Mem usage Increment Occurences Line Contents ============================================================ 12 76 .180 MiB 76 .180 MiB 1 @profile 13 def load_model ( config ) : 14 76 .180 MiB 0 .000 MiB 1 api_model_name = os.path.join ( config.modeldir, config.model_name ) 15 16 230 .598 MiB 154 .418 MiB 1 model = pickle.load ( open ( api_model_name, \"rb\" )) 17 230 .598 MiB 0 .000 MiB 1 if not isinstance ( model, dict ) : 18 raise ValueError ( \"WARNING: this pickle file at {} is not a dict\" .format ( api_model_name )) 19 20 319 .242 MiB 88 .645 MiB 1 predictor = Predict ( config.PARAMS [ 0 :5 ] , config.l0, api_model_name ) 21 319 .242 MiB 0 .000 MiB 1 return predictor","title":"Memory Profiler"},{"location":"testing-api/#load-test","text":"Load testing is the process of determination of behavior of system when multiple users access it at the same time. Locust is a popular open source load testing library developed in python. We install it using pip install locust . And write the following script, saving it as locustfile.py . from locust import HttpUser , task , between import json with open ( 'test_data/sample_request.json' ) as f : request = json . loads ( f . read ()) class APIUser ( HttpUser ): host = 'http://localhost:5000' wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): apikey = \"yourapikey\" self . client . post ( '/predict' , headers = { \"x-api-key\" : apikey } json = request ) We then launch it in the terminal using the command locust , and then the GUI can be access from localhost:8089 . We can test the total number of users as 10, with a spawn rate of 1. This means that the API will be launched with only 1 user requesting, and after every 3-5 seconds (the wait_time), another new user will be added, until the total of 10 users are reached. Start screen Going to the statistics, a summary of the total users, failures and request per seconds Summary We can also have a visual representation in line charts. Charts For an overall report, with a more detailed statistics and response percentile levels with charts all within a single page, go to Download Data > Download Report to download a html version of the report. Report Other commands are as follows: locust -f locust_files/my_locust_file.py --headless -u 1000 -r 100 -run-time 1h30m CMD Desc -f specify file name & location if they are diff from default --headless only print in console, with no GUI -u no. users to spawn -r spawn rate; no users to add per second -run-time time limit for test","title":"Load Test"},{"location":"testing-pytest/","text":"Testing Testing is an important aspect for any software system. The real value of testing occurs with system changes. Done correctly, each tests reduces uncertainty when analysing a change to the system A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source Much of my knowledge in machine learning testing is through this udemy course called Testing & Monitoring Machine Learning Model Deployments . I would highly recommend you to sign up for it. Pytest Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here Basics A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ss show more print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest . INI File We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Class We can place our classes in classes to better organise them. Note that the class name must start with a Test_* for pytest to register, and each method name must have test_ . Below is an example class Test_personalised : def test_ucf_present_no_customer_id ( self ): try : out = RequestSchema ( ** data1 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 1\" def test_adl_present_no_customer_id ( self ): try : out = RequestSchema ( ** data2 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 2\" Fixtures When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called. Parameterize Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( \"inputs\" , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int ) If we have multiple variables for each iteration, we can do the following. @pytest . mark . parametrize ( \"image, expect\" , [ ( \"cone_dsl_00009.jpg\" , 2 ), ( \"cone_dsl_00016.jpg\" , 3 ), ( \"cone_dsl_00017.jpg\" , 2 ), ( \"cone_dsl_00018.jpg\" , 3 )]) def test_predict_validation ( image , expect ): \"\"\"test model with a few key test images\"\"\" img_path = os . path . join ( \"tests/data/images\" , image ) img_arr = cv2 . imread ( img_path ) prediction = detectObj ( img_arr ) detection_cnt = len ( prediction ) assert detection_cnt == expect Function Annotation While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>} Mocking Mocking is often used in unit tests where we can mock or \"fake\" a library, function, return value etc., as we only want to test the unit/function itself, not other related stuff that are integrated with it. In the first example, we want to test the first function log_metrics , however we do not want to use the library bedrock_client as this logs the metrics to a server. Mock Function without Return # train_pipeline.py def log_metrics ( precision , recall , f1 , results = \"logs/results.json\" ): \"\"\"Log metrics to bedrock & results.json\"\"\" from bedrock_client.bedrock.api import BedrockApi print ( f \" Precision = {precision:.5f} \" ) print ( f \" Recall = {recall:.5f} \" ) print ( f \" F1 = {f1:.5f} \" ) with open ( results ) as json_file : json_data = json . load ( json_file ) json_data [ \"score\" ] = { \"Precision\" : precision , \"Recall\" : recall , \"F1\" : f1 } results = open ( results , \"w\" ) json_data = json . dumps ( json_data , indent = 2 ) results . write ( json_data ) print ( \"added scores to results.json\" ) bedrock = BedrockApi ( logging . getLogger ( __name__ )) bedrock . log_metric ( \"Precision\" , precision ) bedrock . log_metric ( \"Recall\" , recall ) bedrock . log_metric ( \"F1\" , f1 ) Hence, we use the Mock() method to create a mock bedrock_client , so that when the import is called, all the metrics will be stored within the mocked library. Note that we have to delete this mock instance at the end ( del sys.modules['bedrock_client.bedrock.api'] ) so that it does not bring over to other test cases. from train_pipeline import evaluate , log_metrics from unittest.mock import Mock def test_log_metrics ( tmp_path ): \"\"\"use mock to replace bedrock api\"\"\" sys . modules [ 'bedrock_client.bedrock.api' ] = Mock () precision = recall = f1 = 0.9 with open ( tmp_path / \"results.json\" , 'w' ) as f : json . dump ({}, f ) log_metrics ( precision , recall , f1 , results = tmp_path / \"results.json\" ) assert os . path . isfile ( tmp_path / \"results.json\" ) del sys . modules [ 'bedrock_client.bedrock.api' ] Mock Function with Mock Return In the second example, we have an evaluate function that we want to test. However, we do not want to use the predict script from predict import detectObj , as that will be tested individually. def evaluate ( val_imgfolder , val_txtfolder , class_ , w_th = 0.03 , h_th = 0.03 , score_th = 0.4 , nms_iou = 0.4 ): import cv2 import utils_bbox as utils from predict import detectObj img_list = get_file_list ( val_imgfolder , ( \".jpg\" , \"jpeg\" , \".png\" , \"bmp\" )) txt_list = get_file_list ( val_txtfolder , ( \".txt\" )) TP = FP = FN = 0 num_correct = num_wrong = 0 for imgname , txtname in zip ( img_list , txt_list ): img_path = os . path . join ( val_imgfolder , imgname ) txt_path = os . path . join ( val_txtfolder , txtname ) image = cv2 . imread ( img_path )[:, :, :: - 1 ] H , W , _ = image . shape pred_arr = detectObj ( image , min_height = h_th , min_width = w_th , score_th = score_th , nms_iou = nms_iou ) pred_boxes = utils . xcycwhnorm_into_xyxy ( pred_arr , H , W ) gt_arr = utils . read_txt_into_array_w_filter ( txt_path , H , W , h_th , w_th ) gt_boxes = utils . convert_array_to_list ( gt_arr ) answer , _ , _ = utils . evaluate_bboxes ( gt_boxes , pred_boxes , iou_th = 0.50 , iop_th = 0.90 ) TP += answer [ 0 ][ \"TP\" ] FP += answer [ 0 ][ \"FP\" ] FN += answer [ 0 ][ \"FN\" ] num_correct += answer [ \"num_correct\" ] num_wrong += answer [ \"num_wrong\" ] precision = TP / ( TP + FP ) recall = TP / ( TP + FN ) f1score = 2 * TP / ( 2 * TP + FP + FN ) return precision , recall , f1score Hence, again, we use the Mock function to create a mocked predict import, and from there, call a mocked detectObj function. We want this method to return a specific value in the evaluate() function, hence the line detectObj.return_value = dummy_pred . With this, we have added in a test case where we sent a fixed predicted return, so that we can have an expected result to compare. from train_pipeline import evaluate , log_metrics from unittest.mock import Mock def test_evaluate_w_known_prediction ( tmp_path ): \"\"\"test evaluate function to ensure the precision, recall & f1 return accurately\"\"\" # mocked prediction return sys . modules [ 'predict' ] = Mock () from predict import detectObj dummy_pred = np . array ( [[ 0.24826389 , 0.62890625 , 0.11805556 , 0.171875 , 0.92410028 , 0. ], [ 0.15104167 , 0.55989583 , 0.10069444 , 0.140625 , 0.85716665 , 0. ]]) detectObj . return_value = dummy_pred # copy related image & label to tmp dir val_imgfolder = tmp_path / \"images\" val_txtfolder = tmp_path / \"labels\" os . mkdir ( val_imgfolder ) os . mkdir ( val_txtfolder ) copyfile ( \"tests/data/images/cone_dsl_00007.jpg\" , val_imgfolder / \"cone_dsl_00007.jpg\" ) copyfile ( \"tests/data/labels/cone_dsl_00007.txt\" , val_txtfolder / \"cone_dsl_00007.txt\" ) class_ = \"safetycone\" precision , recall , f1 = evaluate ( val_imgfolder , val_txtfolder , class_ ) assert precision == recall == f1 == 1 del sys . modules [ 'predict' ] Without Mocking An alternative, to mocking within the testing script, is to add in an if-else condition with if \"pytest\" in sys.modules: within the operational script. This will check if pytest is imported in the environment, and if so, you can input a dummy value. This is especially useful for scripts that lie outside any class or functions. if \"pytest\" in sys . modules : model = \"return value\" else : modelpath = os . path . join ( modeldir , convert_model_name ) model = load_model_item2vec_sku ( modelpath ) Tox Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To reinstall the libraries, run tox -r . To run a specific environment, run tox -e integration . Note that skipsdist = True has to be added if you don't have a setup.py file present. [tox] skipsdist = True envlist = unit, integration [testenv:unit] deps = pytest==5.3.5 pytest-cov==2.10.1 -r{toxinidir}/tests/unit_tests/requirements.txt commands = pytest tests/unit_tests/ -v --cov=project --junitxml=report.xml [testenv:integration] deps = pytest==5.3.5 Cython==0.29.17 numpy==1.19.2 commands = pip install -r {toxinidir}/project/requirements-serve.txt pytest tests/integration_tests/ -v --junitxml=report.xml # side tests ----------------------------- [testenv:lint] deps = flake8 commands = flake8 --select E,F,W association [testenv:bandit] deps = bandit commands = bandit -r -ll association [testenv:safety] deps = safety -r{toxinidir}/association/requirements.txt commands = safety check [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Test Coverage A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Overview & Pytest"},{"location":"testing-pytest/#testing","text":"Testing is an important aspect for any software system. The real value of testing occurs with system changes. Done correctly, each tests reduces uncertainty when analysing a change to the system A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source Much of my knowledge in machine learning testing is through this udemy course called Testing & Monitoring Machine Learning Model Deployments . I would highly recommend you to sign up for it.","title":"Testing"},{"location":"testing-pytest/#pytest","text":"Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here","title":"Pytest"},{"location":"testing-pytest/#basics","text":"A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ss show more print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest .","title":"Basics"},{"location":"testing-pytest/#ini-file","text":"We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"INI File"},{"location":"testing-pytest/#class","text":"We can place our classes in classes to better organise them. Note that the class name must start with a Test_* for pytest to register, and each method name must have test_ . Below is an example class Test_personalised : def test_ucf_present_no_customer_id ( self ): try : out = RequestSchema ( ** data1 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 1\" def test_adl_present_no_customer_id ( self ): try : out = RequestSchema ( ** data2 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 2\"","title":"Class"},{"location":"testing-pytest/#fixtures","text":"When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called.","title":"Fixtures"},{"location":"testing-pytest/#parameterize","text":"Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( \"inputs\" , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int ) If we have multiple variables for each iteration, we can do the following. @pytest . mark . parametrize ( \"image, expect\" , [ ( \"cone_dsl_00009.jpg\" , 2 ), ( \"cone_dsl_00016.jpg\" , 3 ), ( \"cone_dsl_00017.jpg\" , 2 ), ( \"cone_dsl_00018.jpg\" , 3 )]) def test_predict_validation ( image , expect ): \"\"\"test model with a few key test images\"\"\" img_path = os . path . join ( \"tests/data/images\" , image ) img_arr = cv2 . imread ( img_path ) prediction = detectObj ( img_arr ) detection_cnt = len ( prediction ) assert detection_cnt == expect","title":"Parameterize"},{"location":"testing-pytest/#function-annotation","text":"While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>}","title":"Function Annotation"},{"location":"testing-pytest/#mocking","text":"Mocking is often used in unit tests where we can mock or \"fake\" a library, function, return value etc., as we only want to test the unit/function itself, not other related stuff that are integrated with it. In the first example, we want to test the first function log_metrics , however we do not want to use the library bedrock_client as this logs the metrics to a server.","title":"Mocking"},{"location":"testing-pytest/#mock-function-without-return","text":"# train_pipeline.py def log_metrics ( precision , recall , f1 , results = \"logs/results.json\" ): \"\"\"Log metrics to bedrock & results.json\"\"\" from bedrock_client.bedrock.api import BedrockApi print ( f \" Precision = {precision:.5f} \" ) print ( f \" Recall = {recall:.5f} \" ) print ( f \" F1 = {f1:.5f} \" ) with open ( results ) as json_file : json_data = json . load ( json_file ) json_data [ \"score\" ] = { \"Precision\" : precision , \"Recall\" : recall , \"F1\" : f1 } results = open ( results , \"w\" ) json_data = json . dumps ( json_data , indent = 2 ) results . write ( json_data ) print ( \"added scores to results.json\" ) bedrock = BedrockApi ( logging . getLogger ( __name__ )) bedrock . log_metric ( \"Precision\" , precision ) bedrock . log_metric ( \"Recall\" , recall ) bedrock . log_metric ( \"F1\" , f1 ) Hence, we use the Mock() method to create a mock bedrock_client , so that when the import is called, all the metrics will be stored within the mocked library. Note that we have to delete this mock instance at the end ( del sys.modules['bedrock_client.bedrock.api'] ) so that it does not bring over to other test cases. from train_pipeline import evaluate , log_metrics from unittest.mock import Mock def test_log_metrics ( tmp_path ): \"\"\"use mock to replace bedrock api\"\"\" sys . modules [ 'bedrock_client.bedrock.api' ] = Mock () precision = recall = f1 = 0.9 with open ( tmp_path / \"results.json\" , 'w' ) as f : json . dump ({}, f ) log_metrics ( precision , recall , f1 , results = tmp_path / \"results.json\" ) assert os . path . isfile ( tmp_path / \"results.json\" ) del sys . modules [ 'bedrock_client.bedrock.api' ]","title":"Mock Function without Return"},{"location":"testing-pytest/#mock-function-with-mock-return","text":"In the second example, we have an evaluate function that we want to test. However, we do not want to use the predict script from predict import detectObj , as that will be tested individually. def evaluate ( val_imgfolder , val_txtfolder , class_ , w_th = 0.03 , h_th = 0.03 , score_th = 0.4 , nms_iou = 0.4 ): import cv2 import utils_bbox as utils from predict import detectObj img_list = get_file_list ( val_imgfolder , ( \".jpg\" , \"jpeg\" , \".png\" , \"bmp\" )) txt_list = get_file_list ( val_txtfolder , ( \".txt\" )) TP = FP = FN = 0 num_correct = num_wrong = 0 for imgname , txtname in zip ( img_list , txt_list ): img_path = os . path . join ( val_imgfolder , imgname ) txt_path = os . path . join ( val_txtfolder , txtname ) image = cv2 . imread ( img_path )[:, :, :: - 1 ] H , W , _ = image . shape pred_arr = detectObj ( image , min_height = h_th , min_width = w_th , score_th = score_th , nms_iou = nms_iou ) pred_boxes = utils . xcycwhnorm_into_xyxy ( pred_arr , H , W ) gt_arr = utils . read_txt_into_array_w_filter ( txt_path , H , W , h_th , w_th ) gt_boxes = utils . convert_array_to_list ( gt_arr ) answer , _ , _ = utils . evaluate_bboxes ( gt_boxes , pred_boxes , iou_th = 0.50 , iop_th = 0.90 ) TP += answer [ 0 ][ \"TP\" ] FP += answer [ 0 ][ \"FP\" ] FN += answer [ 0 ][ \"FN\" ] num_correct += answer [ \"num_correct\" ] num_wrong += answer [ \"num_wrong\" ] precision = TP / ( TP + FP ) recall = TP / ( TP + FN ) f1score = 2 * TP / ( 2 * TP + FP + FN ) return precision , recall , f1score Hence, again, we use the Mock function to create a mocked predict import, and from there, call a mocked detectObj function. We want this method to return a specific value in the evaluate() function, hence the line detectObj.return_value = dummy_pred . With this, we have added in a test case where we sent a fixed predicted return, so that we can have an expected result to compare. from train_pipeline import evaluate , log_metrics from unittest.mock import Mock def test_evaluate_w_known_prediction ( tmp_path ): \"\"\"test evaluate function to ensure the precision, recall & f1 return accurately\"\"\" # mocked prediction return sys . modules [ 'predict' ] = Mock () from predict import detectObj dummy_pred = np . array ( [[ 0.24826389 , 0.62890625 , 0.11805556 , 0.171875 , 0.92410028 , 0. ], [ 0.15104167 , 0.55989583 , 0.10069444 , 0.140625 , 0.85716665 , 0. ]]) detectObj . return_value = dummy_pred # copy related image & label to tmp dir val_imgfolder = tmp_path / \"images\" val_txtfolder = tmp_path / \"labels\" os . mkdir ( val_imgfolder ) os . mkdir ( val_txtfolder ) copyfile ( \"tests/data/images/cone_dsl_00007.jpg\" , val_imgfolder / \"cone_dsl_00007.jpg\" ) copyfile ( \"tests/data/labels/cone_dsl_00007.txt\" , val_txtfolder / \"cone_dsl_00007.txt\" ) class_ = \"safetycone\" precision , recall , f1 = evaluate ( val_imgfolder , val_txtfolder , class_ ) assert precision == recall == f1 == 1 del sys . modules [ 'predict' ]","title":"Mock Function with Mock Return"},{"location":"testing-pytest/#without-mocking","text":"An alternative, to mocking within the testing script, is to add in an if-else condition with if \"pytest\" in sys.modules: within the operational script. This will check if pytest is imported in the environment, and if so, you can input a dummy value. This is especially useful for scripts that lie outside any class or functions. if \"pytest\" in sys . modules : model = \"return value\" else : modelpath = os . path . join ( modeldir , convert_model_name ) model = load_model_item2vec_sku ( modelpath )","title":"Without Mocking"},{"location":"testing-pytest/#tox","text":"Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To reinstall the libraries, run tox -r . To run a specific environment, run tox -e integration . Note that skipsdist = True has to be added if you don't have a setup.py file present. [tox] skipsdist = True envlist = unit, integration [testenv:unit] deps = pytest==5.3.5 pytest-cov==2.10.1 -r{toxinidir}/tests/unit_tests/requirements.txt commands = pytest tests/unit_tests/ -v --cov=project --junitxml=report.xml [testenv:integration] deps = pytest==5.3.5 Cython==0.29.17 numpy==1.19.2 commands = pip install -r {toxinidir}/project/requirements-serve.txt pytest tests/integration_tests/ -v --junitxml=report.xml # side tests ----------------------------- [testenv:lint] deps = flake8 commands = flake8 --select E,F,W association [testenv:bandit] deps = bandit commands = bandit -r -ll association [testenv:safety] deps = safety -r{toxinidir}/association/requirements.txt commands = safety check [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"Tox"},{"location":"testing-pytest/#test-coverage","text":"A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Test Coverage"},{"location":"testing-schema/","text":"Schema Validation There are quite a number of popular schema validation libraries available, but I will just stick to Pydantic , for the reasons that it is popular, touted to be the fastest , and used natively in FastAPI. Config The config file is one of the most frequently changed file in the repository, and for that reason, it is important to include in our unit-tests. The below shows an example using a config yaml file & pydantic, with a custom validator. # config.yml modeldir: model modelname: nameofname format: dataframe # graph or dataframe import pytest import yaml from pydantic import BaseModel , ValidationError , validator class configSchema ( BaseModel ): modeldir : str modelname : str format : str @validator ( 'format' ) def format_list ( cls , v ): if v not in [ \"dataframe\" , \"graph\" ]: raise ValueError ( \"must be either 'dataframe or 'graph'\" ) return v def test_config (): \"\"\"test for all key-values in config file\"\"\" cf = yaml . safe_load ( open ( \"foldername/config.yml\" )) try : out = configSchema ( ** cf ) print ( out ) except ValidationError as e : pytest . raises ( e ) API Request Pydantic can also be used in Flask to validate all incoming requests. To validate the request schema before passing to the code, we can write a decorator function. import logging from functools import wraps from typing import List , Optional from flask import Flask , abort , jsonify , make_response , request , logging as flog from pydantic import BaseModel , ValidationError , confloat , conint , validator app = Flask ( __name__ ) class _weightage ( BaseModel ): recommender : str weight : confloat ( ge = 0 , le = 1 ) @validator ( \"recommender\" ) def recommender_list ( cls , v ): if v not in recommender_list : raise ValueError ( \"Recommender is not part of {} \" . format ( str ( recommender_list ))) class RequestSchema ( BaseModel ): productSKU : List [ str ] storeId : str weightage : List [ _weightage ] customerId : Optional [ str ] preceding_time_window : Optional [ str ] resultSize : Optional [ conint ( ge = 1 , le = 50 )] = 20 @validator ( \"weightage\" ) def weight_sum ( cls , v ): sumw = 0 for i in range ( len ( v )): weight = v [ i ] . weight sumw = weight + sumw if sumw != 1 : raise ValueError ( \"sum of weightage is not equals to 1\" ) def validate_request ( requestschema ): \"\"\"decorator to validate request schema\"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : requestschema ( ** request . json ) except ValidationError as e : app . logger . error ( ' {} - {} ' . format ( e , [ 422 ])) err_json = json . loads ( e . json ()) abort ( make_response ( jsonify ( err_json ), 422 )) return func ( * args , ** kwargs ) return wrapper return decorator @app . route ( \"/recommendation\" , methods = [ \"POST\" ]) @validate_request ( RequestSchema ) def fusion_api (): req_content = request . json # do something return predicted_results Pandas Ok, I take back what I said on Pydantic. For validating pandas dataframes, it is slightly difficult to use that library, hence we will use a library heavily inspired by Pydantic, called pandera . from pandera import DataFrameSchema , Column , Check schema = DataFrameSchema ({ \"antecedents\" : Column ( \"category\" ), \"consequents\" : Column ( object ), \"antecedent support\" : Column ( \"float32\" ), \"consequent support\" : Column ( \"float32\" ), \"support\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , \\ error = \"range checker [0, 1]\" )), \"confidence\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , error = \"range checker [0, 1]\" )), \"lift\" : Column ( \"float32\" ), \"leverage\" : Column ( \"float32\" ), \"conviction\" : Column ( \"float32\" ), \"storeid\" : Column ( \"category\" ), }) try : validated_df = schema ( df , lazy = True ) except Exception as e : print ( e ) The validated_df is the same dataframe that can be used to continue coding. If we are validating in pytest, we can add a try, except to catch the error in a graceful way. The argument lazy=True should added to ensure that it captures all errors before giving a validation error report as shown below. A total of 1 schema errors were found . Error Counts ------------ - schema_component_check : 1 Schema Error Summary -------------------- failure_cases n_failure_cases schema_context column check Column consequents pandas_dtype ( 'float64' ) [ object ] 1 Usage Tip --------- Directly inspect all errors by catching the exception : try : schema . validate ( dataframe , lazy = True ) except SchemaErrors as err : err . failure_cases # dataframe of schema errors err . data # invalid dataframe","title":"Schema Testing"},{"location":"testing-schema/#schema-validation","text":"There are quite a number of popular schema validation libraries available, but I will just stick to Pydantic , for the reasons that it is popular, touted to be the fastest , and used natively in FastAPI.","title":"Schema Validation"},{"location":"testing-schema/#config","text":"The config file is one of the most frequently changed file in the repository, and for that reason, it is important to include in our unit-tests. The below shows an example using a config yaml file & pydantic, with a custom validator. # config.yml modeldir: model modelname: nameofname format: dataframe # graph or dataframe import pytest import yaml from pydantic import BaseModel , ValidationError , validator class configSchema ( BaseModel ): modeldir : str modelname : str format : str @validator ( 'format' ) def format_list ( cls , v ): if v not in [ \"dataframe\" , \"graph\" ]: raise ValueError ( \"must be either 'dataframe or 'graph'\" ) return v def test_config (): \"\"\"test for all key-values in config file\"\"\" cf = yaml . safe_load ( open ( \"foldername/config.yml\" )) try : out = configSchema ( ** cf ) print ( out ) except ValidationError as e : pytest . raises ( e )","title":"Config"},{"location":"testing-schema/#api-request","text":"Pydantic can also be used in Flask to validate all incoming requests. To validate the request schema before passing to the code, we can write a decorator function. import logging from functools import wraps from typing import List , Optional from flask import Flask , abort , jsonify , make_response , request , logging as flog from pydantic import BaseModel , ValidationError , confloat , conint , validator app = Flask ( __name__ ) class _weightage ( BaseModel ): recommender : str weight : confloat ( ge = 0 , le = 1 ) @validator ( \"recommender\" ) def recommender_list ( cls , v ): if v not in recommender_list : raise ValueError ( \"Recommender is not part of {} \" . format ( str ( recommender_list ))) class RequestSchema ( BaseModel ): productSKU : List [ str ] storeId : str weightage : List [ _weightage ] customerId : Optional [ str ] preceding_time_window : Optional [ str ] resultSize : Optional [ conint ( ge = 1 , le = 50 )] = 20 @validator ( \"weightage\" ) def weight_sum ( cls , v ): sumw = 0 for i in range ( len ( v )): weight = v [ i ] . weight sumw = weight + sumw if sumw != 1 : raise ValueError ( \"sum of weightage is not equals to 1\" ) def validate_request ( requestschema ): \"\"\"decorator to validate request schema\"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : requestschema ( ** request . json ) except ValidationError as e : app . logger . error ( ' {} - {} ' . format ( e , [ 422 ])) err_json = json . loads ( e . json ()) abort ( make_response ( jsonify ( err_json ), 422 )) return func ( * args , ** kwargs ) return wrapper return decorator @app . route ( \"/recommendation\" , methods = [ \"POST\" ]) @validate_request ( RequestSchema ) def fusion_api (): req_content = request . json # do something return predicted_results","title":"API Request"},{"location":"testing-schema/#pandas","text":"Ok, I take back what I said on Pydantic. For validating pandas dataframes, it is slightly difficult to use that library, hence we will use a library heavily inspired by Pydantic, called pandera . from pandera import DataFrameSchema , Column , Check schema = DataFrameSchema ({ \"antecedents\" : Column ( \"category\" ), \"consequents\" : Column ( object ), \"antecedent support\" : Column ( \"float32\" ), \"consequent support\" : Column ( \"float32\" ), \"support\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , \\ error = \"range checker [0, 1]\" )), \"confidence\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , error = \"range checker [0, 1]\" )), \"lift\" : Column ( \"float32\" ), \"leverage\" : Column ( \"float32\" ), \"conviction\" : Column ( \"float32\" ), \"storeid\" : Column ( \"category\" ), }) try : validated_df = schema ( df , lazy = True ) except Exception as e : print ( e ) The validated_df is the same dataframe that can be used to continue coding. If we are validating in pytest, we can add a try, except to catch the error in a graceful way. The argument lazy=True should added to ensure that it captures all errors before giving a validation error report as shown below. A total of 1 schema errors were found . Error Counts ------------ - schema_component_check : 1 Schema Error Summary -------------------- failure_cases n_failure_cases schema_context column check Column consequents pandas_dtype ( 'float64' ) [ object ] 1 Usage Tip --------- Directly inspect all errors by catching the exception : try : schema . validate ( dataframe , lazy = True ) except SchemaErrors as err : err . failure_cases # dataframe of schema errors err . data # invalid dataframe","title":"Pandas"},{"location":"testing2/","text":"Machine Learning Tests Fake Data Sometimes we are unable to store a subset of the data in our repository for testing, due to the confidentiality of data. In this case, we can instead generate fake data using the popular library called Faker .","title":"ML Testing"},{"location":"testing2/#machine-learning-tests","text":"","title":"Machine Learning Tests"},{"location":"testing2/#fake-data","text":"Sometimes we are unable to store a subset of the data in our repository for testing, due to the confidentiality of data. In this case, we can instead generate fake data using the popular library called Faker .","title":"Fake Data"},{"location":"tf-serving/","text":"Tensorflow Serving Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things. Save Model as Protobuf We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables TensorFlow Serving with Docker It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind,source = /path/to/model_folder/,target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container REST API As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction ) gRPC Client To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"TF Serving"},{"location":"tf-serving/#tensorflow-serving","text":"Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things.","title":"Tensorflow Serving"},{"location":"tf-serving/#save-model-as-protobuf","text":"We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables","title":"Save Model as Protobuf"},{"location":"tf-serving/#tensorflow-serving-with-docker","text":"It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind,source = /path/to/model_folder/,target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container","title":"TensorFlow Serving with Docker"},{"location":"tf-serving/#rest-api","text":"As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction )","title":"REST API"},{"location":"tf-serving/#grpc-client","text":"To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"gRPC Client"},{"location":"virtual_env/","text":"Virtual Environment Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment. Anaconda Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname> VENV venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. The downside of venv is that you cannot install a particular python version. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate Jupyter Notebook Sometimes we need to experiment with certain code or data, and not ready to convert into python scripts; hence the use of jupyter notebooks. To change the environment or kernel (as named in jupyter), we first setup our virtual env using venv or conda, and install the necessary library as follows. pip install ipykernel python -m ipykernel install --user --name = <env_name> In the jupyter notebook, we then select the kernel option from the top right and switch to the virtual env we launched earlier. Sourced from this article","title":"Virtual Environment"},{"location":"virtual_env/#virtual-environment","text":"Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment.","title":"Virtual Environment"},{"location":"virtual_env/#anaconda","text":"Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname>","title":"Anaconda"},{"location":"virtual_env/#venv","text":"venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. The downside of venv is that you cannot install a particular python version. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"VENV"},{"location":"virtual_env/#jupyter-notebook","text":"Sometimes we need to experiment with certain code or data, and not ready to convert into python scripts; hence the use of jupyter notebooks. To change the environment or kernel (as named in jupyter), we first setup our virtual env using venv or conda, and install the necessary library as follows. pip install ipykernel python -m ipykernel install --user --name = <env_name> In the jupyter notebook, we then select the kernel option from the top right and switch to the virtual env we launched earlier. Sourced from this article","title":"Jupyter Notebook"}]}