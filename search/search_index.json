{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Engineer? Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation A strong focus on the security and reliability and know how to balance both Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-engineer","text":"Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation A strong focus on the security and reliability and know how to balance both Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"So You Wana Be an AI Engineer?"},{"location":"cicd/","text":"CI/CD Continuous Integration and Continuous Delivery/Deployment is an important part of DevOps, where a standard mode of operations are automated in a pipeline frequently, and reliably. Differences between CI-CD-CD. Source This process is usually integrated with a version control platform like Github and Gitlab. Both popular platforms provide in-built CI/CD automation called Github Actions and Gitlab-CI . They are largely free, with some paid add-on tools. Other popular ones include CircleCI and TravisCI . Gitlab-CI Basics Gitlab CI is run by \"runners\", which are essentially google servers spun up when a job is activated. A series of jobs make up a pipeline , while certain jobs of a specific attribute are assigned to a stage . By default, all jobs in a stage needs to finish before the next stage's jobs starts. A typical pipeline workflow consists of the various stages. Test: security scans, unit-tests, integration tests Build: building a docker image Deploy: deployment of image to dev/staging/production A pipeline with various stages, and individual jobs, shown in Gitlab's interface Yaml File Gitlab-CI will auto run if a file called .gitlab-ci.yml is stored in the root of the repository. Inside the ymal file, containing mostly shell commands that tells runner what to execute. It can also refer to scripts stored in the repo so as to keep the ymal file short and concise. Github Actions Yaml File Github Actions will auto run if one or more ymal file is located in the folder .github/workflows/ .","title":"CI/CD"},{"location":"cicd/#cicd","text":"Continuous Integration and Continuous Delivery/Deployment is an important part of DevOps, where a standard mode of operations are automated in a pipeline frequently, and reliably. Differences between CI-CD-CD. Source This process is usually integrated with a version control platform like Github and Gitlab. Both popular platforms provide in-built CI/CD automation called Github Actions and Gitlab-CI . They are largely free, with some paid add-on tools. Other popular ones include CircleCI and TravisCI .","title":"CI/CD"},{"location":"cicd/#gitlab-ci","text":"","title":"Gitlab-CI"},{"location":"cicd/#basics","text":"Gitlab CI is run by \"runners\", which are essentially google servers spun up when a job is activated. A series of jobs make up a pipeline , while certain jobs of a specific attribute are assigned to a stage . By default, all jobs in a stage needs to finish before the next stage's jobs starts. A typical pipeline workflow consists of the various stages. Test: security scans, unit-tests, integration tests Build: building a docker image Deploy: deployment of image to dev/staging/production A pipeline with various stages, and individual jobs, shown in Gitlab's interface","title":"Basics"},{"location":"cicd/#yaml-file","text":"Gitlab-CI will auto run if a file called .gitlab-ci.yml is stored in the root of the repository. Inside the ymal file, containing mostly shell commands that tells runner what to execute. It can also refer to scripts stored in the repo so as to keep the ymal file short and concise.","title":"Yaml File"},{"location":"cicd/#github-actions","text":"","title":"Github Actions"},{"location":"cicd/#yaml-file_1","text":"Github Actions will auto run if one or more ymal file is located in the folder .github/workflows/ .","title":"Yaml File"},{"location":"code-standards/","text":"Code Standards requirements.txt requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1.0 scipy == 1.4.1 scikit_image == 0.16.2 numpy == 1.18.1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5.0 + cu101 torchvision == 0.6.0 + cu101 pipreqs also allow an option --mode=\"compat\" , which enables patch version updates only. This is important as it allows bug fixes or security patches installed within the micro versions with little chance that the code will break since it is a micro update. A side note on semantic versioning. As defined by the convention , it usually follows the version of Major.Minor.Patch; e.g. Flask==1.0.2, where the patch version is backward compatible. Python also have its own description in PEP 440 , naming it as Major.Minor.Micro. DocStrings DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco ISort Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py Black Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py Flake8 A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Code Standards"},{"location":"code-standards/#code-standards","text":"","title":"Code Standards"},{"location":"code-standards/#requirementstxt","text":"requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1.0 scipy == 1.4.1 scikit_image == 0.16.2 numpy == 1.18.1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5.0 + cu101 torchvision == 0.6.0 + cu101 pipreqs also allow an option --mode=\"compat\" , which enables patch version updates only. This is important as it allows bug fixes or security patches installed within the micro versions with little chance that the code will break since it is a micro update. A side note on semantic versioning. As defined by the convention , it usually follows the version of Major.Minor.Patch; e.g. Flask==1.0.2, where the patch version is backward compatible. Python also have its own description in PEP 440 , naming it as Major.Minor.Micro.","title":"requirements.txt"},{"location":"code-standards/#docstrings","text":"DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco","title":"DocStrings"},{"location":"code-standards/#isort","text":"Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py","title":"ISort"},{"location":"code-standards/#black","text":"Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py","title":"Black"},{"location":"code-standards/#flake8","text":"A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Flake8"},{"location":"demo/","text":"Demo Site For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo # streamlit settings st . set_page_config ( page_title = 'Demo Site' ) json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" , type = \"password\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"demo/#demo-site","text":"For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo # streamlit settings st . set_page_config ( page_title = 'Demo Site' ) json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" , type = \"password\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"deploy-ecs/","text":"Docker Compose & ECS # create ECS context docker context create ecs <name> docker context ls docker context use <name> # login to ECR aws ecr get-login-password | docker login --username AWS --password-stdin <account-id>.dkr.ecr.ap-southeast-1.amazonaws.com # create ECS docker compose up --project-name <name> # create cloudformation stack docker compose down # delete cluster docker context rm -f flask-test docker context use default # create cloudformation template # can use yml file to view visually in AWS designer docker compose convert > cloudformation.yml From the documentation, \"By default, the Docker Compose CLI creates an ECS cluster for your Compose application, a Security Group per network in your Compose file on your AWS account\u2019s default VPC, and a LoadBalancer to route traffic to your services.\"","title":"Docker Compose & ECS"},{"location":"deploy-ecs/#docker-compose-ecs","text":"# create ECS context docker context create ecs <name> docker context ls docker context use <name> # login to ECR aws ecr get-login-password | docker login --username AWS --password-stdin <account-id>.dkr.ecr.ap-southeast-1.amazonaws.com # create ECS docker compose up --project-name <name> # create cloudformation stack docker compose down # delete cluster docker context rm -f flask-test docker context use default # create cloudformation template # can use yml file to view visually in AWS designer docker compose convert > cloudformation.yml From the documentation, \"By default, the Docker Compose CLI creates an ECS cluster for your Compose application, a Security Group per network in your Compose file on your AWS account\u2019s default VPC, and a LoadBalancer to route traffic to your services.\"","title":"Docker Compose &amp; ECS"},{"location":"deploy-fastapi/","text":"FastAPI FastAPI is one of the next generation python web framework that uses ASGI (asynchronous server gateway interface) instead of the traditional WSGI. It also includes a number of useful functions to make API creations easier. Uvicorn FastAPI uses Uvicorn as its ASGI. We can configure its settings as described here . We can also specify it in the fastapi python app script, or at the terminal when we launch uvicorn. For the former, with the below specification, we can just execute python app.py to start the application. # app.py from fastapi import FastAPI import uvicorn app = FastAPI () if __name__ == \"__main__\" : uvicorn . run ( 'app:app' , host = '0.0.0.0' , port = 5000 ) If we run from the terminal, with the app residing in example.py. uvicorn example:app --host = '0.0.0.0' --port = 5000 The documentation recommends that we use gunicorn which have richer features to better control over the workers processes. gunicorn app:app --bind 0 .0.0.0:5000 -w 1 --log-level debug -k uvicorn.workers.UvicornWorker Request-Response Schema FastAPI uses the pydantic library to define the schema of the request & response APIs. This allows the auto generation in the OpenAPI documentations, and for the former, for validating the schema when a request is received. For example, given the json: { \"boundingPoly\" : { \"normalizedVertices\" : [ { \"x\" : 0.406767 , \"y\" : 0.874573 , \"width\" : 0.357321 , \"height\" : 0.452179 , \"score\" : 0.972167 }, { \"x\" : 0.56781 , \"y\" : 0.874173 , \"width\" : 0.457373 , \"height\" : 0.452121 , \"score\" : 0.982109 } ] }, \"name\" : \"Cat\" } We can define in pydantic as below, using multiple basemodels for each level in the JSON. If there are no values input like y: float , it will listed as a required field If we add a value like y: float = 0.8369 , it will be an optional field, with the value also listed as a default and example value If we add a value like x: float = Field(..., example=0.82379) , it will be a required field, and also listed as an example value More attributes can be added in Field() , that will be populated in OpenAPI docs. class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 0.82379 , description = \"X-coordinates\" )) y : float = 0.8369 width : float height : float score : float class _boundingPoly ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] class ResponseSchema ( BaseModel ): boundingPoly : _boundingPoly name : str = \"Human\" We do the same for the request schema and place them in the routing function. from fastapi import FastAPI from pydantic import BaseModel , Field from typing import List import json import base64 import numpy as np @app . post ( '/api' , response_model = ResponseSchema ) async def human_detection ( request : RequestSchema ): JScontent = json . loads ( request . json ()) encodedImage = JScontent [ 'requests' ][ 0 ][ 'image' ][ 'content' ] npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) pred_output = model ( imgArr ) return pred_output Open-API OpenAPI documentations of Swagger UI or Redoc are automatically generated. You can access it at the endpoints of /docs and /redoc . First, the title, description and versions can be specified from the initialisation of fastapi. The request-response pydantic schema and examples will be added after its inclusion in a POST/GET request routing function. from fastapi import FastAPI app = FastAPI ( title = \"Human Detection API\" , description = \"Submit Image to Return Detected Humans in Bounding Boxes\" , version = \"1.0.0\" ) @app . post ( '/api' , response_model = RESPONSE_SCHEMA ) def human_detection ( request : REQUEST_SCHEMA ): do something return another_thing","title":"FastAPI"},{"location":"deploy-fastapi/#fastapi","text":"FastAPI is one of the next generation python web framework that uses ASGI (asynchronous server gateway interface) instead of the traditional WSGI. It also includes a number of useful functions to make API creations easier.","title":"FastAPI"},{"location":"deploy-fastapi/#uvicorn","text":"FastAPI uses Uvicorn as its ASGI. We can configure its settings as described here . We can also specify it in the fastapi python app script, or at the terminal when we launch uvicorn. For the former, with the below specification, we can just execute python app.py to start the application. # app.py from fastapi import FastAPI import uvicorn app = FastAPI () if __name__ == \"__main__\" : uvicorn . run ( 'app:app' , host = '0.0.0.0' , port = 5000 ) If we run from the terminal, with the app residing in example.py. uvicorn example:app --host = '0.0.0.0' --port = 5000 The documentation recommends that we use gunicorn which have richer features to better control over the workers processes. gunicorn app:app --bind 0 .0.0.0:5000 -w 1 --log-level debug -k uvicorn.workers.UvicornWorker","title":"Uvicorn"},{"location":"deploy-fastapi/#request-response-schema","text":"FastAPI uses the pydantic library to define the schema of the request & response APIs. This allows the auto generation in the OpenAPI documentations, and for the former, for validating the schema when a request is received. For example, given the json: { \"boundingPoly\" : { \"normalizedVertices\" : [ { \"x\" : 0.406767 , \"y\" : 0.874573 , \"width\" : 0.357321 , \"height\" : 0.452179 , \"score\" : 0.972167 }, { \"x\" : 0.56781 , \"y\" : 0.874173 , \"width\" : 0.457373 , \"height\" : 0.452121 , \"score\" : 0.982109 } ] }, \"name\" : \"Cat\" } We can define in pydantic as below, using multiple basemodels for each level in the JSON. If there are no values input like y: float , it will listed as a required field If we add a value like y: float = 0.8369 , it will be an optional field, with the value also listed as a default and example value If we add a value like x: float = Field(..., example=0.82379) , it will be a required field, and also listed as an example value More attributes can be added in Field() , that will be populated in OpenAPI docs. class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 0.82379 , description = \"X-coordinates\" )) y : float = 0.8369 width : float height : float score : float class _boundingPoly ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] class ResponseSchema ( BaseModel ): boundingPoly : _boundingPoly name : str = \"Human\" We do the same for the request schema and place them in the routing function. from fastapi import FastAPI from pydantic import BaseModel , Field from typing import List import json import base64 import numpy as np @app . post ( '/api' , response_model = ResponseSchema ) async def human_detection ( request : RequestSchema ): JScontent = json . loads ( request . json ()) encodedImage = JScontent [ 'requests' ][ 0 ][ 'image' ][ 'content' ] npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) pred_output = model ( imgArr ) return pred_output","title":"Request-Response Schema"},{"location":"deploy-fastapi/#open-api","text":"OpenAPI documentations of Swagger UI or Redoc are automatically generated. You can access it at the endpoints of /docs and /redoc . First, the title, description and versions can be specified from the initialisation of fastapi. The request-response pydantic schema and examples will be added after its inclusion in a POST/GET request routing function. from fastapi import FastAPI app = FastAPI ( title = \"Human Detection API\" , description = \"Submit Image to Return Detected Humans in Bounding Boxes\" , version = \"1.0.0\" ) @app . post ( '/api' , response_model = RESPONSE_SCHEMA ) def human_detection ( request : REQUEST_SCHEMA ): do something return another_thing","title":"Open-API"},{"location":"deploy-flask/","text":"Flask Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task. Simple Flask App Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" import traceback from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) # using .get will return None if request does not include key-value # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : tb = traceback . format_exc () app . logger . error ( tb ) return { \"errorMessages\" : tb . replace ( \" \\n \" , \"\" )} if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" ) Async From Flask>=2.0 onwards, it supports async syntax with the installation of pip install Flask[async] . It should be noted to use async only in I/O bound tasks which takes less than a few seconds to process. An excellent description from here . It works similarly & with the same response time as multi-threading. Synchronous Take for example this synchronous REST app that is requesting data from multiple urls. The response time is 1.024s. import json import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender_sync ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) @app . post ( \"/recommendation\" ) def fusion_api (): # synchronous, 1.024s --------- concat_list = [] for rre in api_urls . keys (): predictions = call_recommender_sync ( rre , api_urls ) concat_list . append ( predictions ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True ) Asynchronous To write in asynchronous code to send multiple requests, we need to use aiohttp in place of requests , and aiohttp to gather the results in a list. aiohttp is a non-blocking program, which allow other threads to continue running while it's waiting. The appropriate async and await syntax needs to be added too. The response time is around 0.356s, x2.87. import asyncio import json import requests from aiohttp from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } async def call_recommender_async ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } async with aiohttp . ClientSession () as session : async with session . post ( url , json = data ) as resp : prediction = await resp . json () return ( prediction , rre ) @app . post ( \"/recommendation\" ) async def fusion_api (): # asynchronous, 0.356s, x2.87 --------- concat_list = [] for rre in api_urls . keys (): predictions = asyncio . create_task ( call_recommender_async ( rre , api_urls )) concat_list . append ( predictions ) concat_list = await asyncio . gather ( * concat_list ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 ) Multi-Threading We can use concurrent.futures to send requests using multi-threading. The response time is similar, 0.359s, x2.85 faster. This is because of Python's Global Interpretor Lock (GIL), whereby only one thread can run one time. Python uses thread-switching to change to another thread to start another task, rendering multi-threading as an async, not parallel process. import json from concurrent.futures import ThreadPoolExecutor , as_completed import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) def multithread ( api_urls ): futures = [] with ThreadPoolExecutor ( max_workers = 4 ) as executor : for rre in api_urls . keys (): futures . append ( executor . submit ( call_recommender , rre , api_urls )) return [ future . result () for future in as_completed ( futures )] @app . post ( \"/recommendation\" ) def fusion_api (): # multi-threading, 0.359s, x2.85 --------- concat_list = multithread ( api_urls ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True ) Gunicorn Flask as a server is meant for development, as it tries to remind you everytime you launch it, giving the message WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. One of the best is Gunicorn ; a mature, fully featured server and process manager. It allows automated worker production and management through simple configurations. Command # gunicorn --bind <flask-ip>:<flask-port> <flask-script>:<flask-app> gunicorn --bind 0 .0.0.0:5000 serve_http:app Config File Rather than entering all the configs when launching gunicorn, we can hard code some of them in a config file gunicorn.conf.py . With this, we can adjust the workers based on the machine's cores. Gunicorn's documentation recommend the number of workers to be set as (total-cpu * 2) + 1 . One of the most important config besides workers is the preload , this allows the preloading of your model in memory and shared among all the workers. If not, each of your worker will load the model separately and consume a lot, if not all the RAM in the machine. # gunicorn.conf.py # to see all flask stdout, we can change the log level to debug import multiprocessing workers = ( multiprocessing . cpu_count () * 2 ) + 1 preload_app = True log_level = info timeout = 10 We can also use the gevent or gthread to implement concurrency if there are significant I/O blocking bottlenecks. For the latter, this is done by \u201cmonkey patching\u201d the code, mainly replacing blocking parts with compatible cooperative counterparts from gevent package. See this article for more information . Logging Using the default logging library does not automatically appear in the gunicorn logs. To do that, you have to use print(\"<something>\", flush=True) . Testing Python Requests The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response\"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j ) Postman Postman is a popular GUI to easily send requests and see the responses. CURL We can also use CURL in the terminal for commandline sending of requests. Here\u2019s a simple test to see the API works, without sending the data. curl --request POST localhost:5000/api Here\u2019s one complete request with data curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api To run multiple requests in parallel for stress testing curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & wait OpenAPI OpenAPI is a standard API documentation specification in ymal or json format, originated from Swagger. It usually comes with a user interface, the most popular being SwaggerUI . It provides all the information required about the API, with also an ability to test the API itself. There are three ways to go about this. generated separately as a single ymal file, and hosted using connexion generate as docstrings or individual ymal file for each endpoint using flasgger auto-generated using defined schemas, and adding additional info within the Flask app using Flask-Pydantic-Spec Personally, I believe the first is the most realistic, as the API specs are usually defined before the Flask app is created, and that doc can be sent to others for verification without creating a Flask app. Below is an example script for point 1. import connexion from flask import request from predict import prediction app = connexion . App ( __name__ , specification_dir = '.' ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : app . add_api ( 'openapi.yml' ) app . run ( host = \"0.0.0.0\" ) And point 3. \"\"\"flask server with pydantic validation & openapi integration\"\"\" from typing import List from flask import Flask , request from flask_pydantic_spec import FlaskPydanticSpec , Request , Response from pydantic import BaseModel , Field , confloat from predict import prediction app = Flask ( __name__ ) api = FlaskPydanticSpec ( \"flask\" , title = \"Objection Detection\" , version = \"v1.0.0\" ) class RequestSchema ( BaseModel ): maxResults : int = Field ( None , example = 20 , description = \"Maximum detection result to return\" ) min_height : float = Field ( None , example = 0.3 , description = \"Score\" ) min_width : float = Field ( None , example = 0.3 , description = \"Score\" ) score_th : float = Field ( None , example = 0.3 , description = \"Score\" ) nms_iou : float = Field ( ... , example = 0.4 , description = \"Non-max suppression, intersection over union\" ) type : str = Field ( ... , example = \"safetycone\" , description = \"name of object to detect\" ) image : str = Field ( ... , description = \"base64-encoded-image\" ) class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 5.12 , description = \"X-coordinate\" ) y : float = Field ( ... , example = 20.56 , description = \"Y-coordinate\" ) width : int = Field ( ... , example = 500 , description = \"width in pixel\" ) height : int = Field ( ... , example = 600 , description = \"height in pixel\" ) score : confloat ( gt = 0.0 , lt = 1.0 ) = Field ( ... , example = 0.79 , description = \"confidence score\" ) class ResponseSchema ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] @app . route ( \"/predict\" , methods = [ \"POST\" ]) @api . validate ( body = Request ( RequestSchema ), resp = Response ( HTTP_200 = ResponseSchema ), tags = [ \"API Name\" ] ) def get_predictions (): \"\"\"Short description of endpoint Long description of endpoint\"\"\" JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : api . register ( app ) app . run ( host = \"0.0.0.0\" ) You can refer to my repo for the full example.","title":"Flask"},{"location":"deploy-flask/#flask","text":"Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task.","title":"Flask"},{"location":"deploy-flask/#simple-flask-app","text":"Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" import traceback from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) # using .get will return None if request does not include key-value # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : tb = traceback . format_exc () app . logger . error ( tb ) return { \"errorMessages\" : tb . replace ( \" \\n \" , \"\" )} if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" )","title":"Simple Flask App"},{"location":"deploy-flask/#async","text":"From Flask>=2.0 onwards, it supports async syntax with the installation of pip install Flask[async] . It should be noted to use async only in I/O bound tasks which takes less than a few seconds to process. An excellent description from here . It works similarly & with the same response time as multi-threading.","title":"Async"},{"location":"deploy-flask/#synchronous","text":"Take for example this synchronous REST app that is requesting data from multiple urls. The response time is 1.024s. import json import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender_sync ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) @app . post ( \"/recommendation\" ) def fusion_api (): # synchronous, 1.024s --------- concat_list = [] for rre in api_urls . keys (): predictions = call_recommender_sync ( rre , api_urls ) concat_list . append ( predictions ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True )","title":"Synchronous"},{"location":"deploy-flask/#asynchronous","text":"To write in asynchronous code to send multiple requests, we need to use aiohttp in place of requests , and aiohttp to gather the results in a list. aiohttp is a non-blocking program, which allow other threads to continue running while it's waiting. The appropriate async and await syntax needs to be added too. The response time is around 0.356s, x2.87. import asyncio import json import requests from aiohttp from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } async def call_recommender_async ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } async with aiohttp . ClientSession () as session : async with session . post ( url , json = data ) as resp : prediction = await resp . json () return ( prediction , rre ) @app . post ( \"/recommendation\" ) async def fusion_api (): # asynchronous, 0.356s, x2.87 --------- concat_list = [] for rre in api_urls . keys (): predictions = asyncio . create_task ( call_recommender_async ( rre , api_urls )) concat_list . append ( predictions ) concat_list = await asyncio . gather ( * concat_list ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 )","title":"Asynchronous"},{"location":"deploy-flask/#multi-threading","text":"We can use concurrent.futures to send requests using multi-threading. The response time is similar, 0.359s, x2.85 faster. This is because of Python's Global Interpretor Lock (GIL), whereby only one thread can run one time. Python uses thread-switching to change to another thread to start another task, rendering multi-threading as an async, not parallel process. import json from concurrent.futures import ThreadPoolExecutor , as_completed import requests from flask import Flask , request app = Flask ( __name__ ) api_urls = \\ { \"sml\" : \"http://localhost:5001/test\" , \"asc\" : \"http://localhost:5002/test\" , \"trd\" : \"http://localhost:5003/test\" , \"psn\" : \"http://localhost:5004/test\" } def call_recommender ( rre , api_urls ): \"\"\"call individual recommenders & get predictions\"\"\" url = api_urls [ rre ] data = { \"resultSize\" : 1 } prediction = requests . post ( url , json = data ) . content prediction = json . loads ( prediction ) return ( prediction , rre ) def multithread ( api_urls ): futures = [] with ThreadPoolExecutor ( max_workers = 4 ) as executor : for rre in api_urls . keys (): futures . append ( executor . submit ( call_recommender , rre , api_urls )) return [ future . result () for future in as_completed ( futures )] @app . post ( \"/recommendation\" ) def fusion_api (): # multi-threading, 0.359s, x2.85 --------- concat_list = multithread ( api_urls ) return { \"result\" : str ( concat_list )} if __name__ == \"__main__\" : app . run ( port = 5000 , debug = True )","title":"Multi-Threading"},{"location":"deploy-flask/#gunicorn","text":"Flask as a server is meant for development, as it tries to remind you everytime you launch it, giving the message WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. One of the best is Gunicorn ; a mature, fully featured server and process manager. It allows automated worker production and management through simple configurations.","title":"Gunicorn"},{"location":"deploy-flask/#command","text":"# gunicorn --bind <flask-ip>:<flask-port> <flask-script>:<flask-app> gunicorn --bind 0 .0.0.0:5000 serve_http:app","title":"Command"},{"location":"deploy-flask/#config-file","text":"Rather than entering all the configs when launching gunicorn, we can hard code some of them in a config file gunicorn.conf.py . With this, we can adjust the workers based on the machine's cores. Gunicorn's documentation recommend the number of workers to be set as (total-cpu * 2) + 1 . One of the most important config besides workers is the preload , this allows the preloading of your model in memory and shared among all the workers. If not, each of your worker will load the model separately and consume a lot, if not all the RAM in the machine. # gunicorn.conf.py # to see all flask stdout, we can change the log level to debug import multiprocessing workers = ( multiprocessing . cpu_count () * 2 ) + 1 preload_app = True log_level = info timeout = 10 We can also use the gevent or gthread to implement concurrency if there are significant I/O blocking bottlenecks. For the latter, this is done by \u201cmonkey patching\u201d the code, mainly replacing blocking parts with compatible cooperative counterparts from gevent package. See this article for more information .","title":"Config File"},{"location":"deploy-flask/#logging","text":"Using the default logging library does not automatically appear in the gunicorn logs. To do that, you have to use print(\"<something>\", flush=True) .","title":"Logging"},{"location":"deploy-flask/#testing","text":"","title":"Testing"},{"location":"deploy-flask/#python-requests","text":"The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response\"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Python Requests"},{"location":"deploy-flask/#postman","text":"Postman is a popular GUI to easily send requests and see the responses.","title":"Postman"},{"location":"deploy-flask/#curl","text":"We can also use CURL in the terminal for commandline sending of requests. Here\u2019s a simple test to see the API works, without sending the data. curl --request POST localhost:5000/api Here\u2019s one complete request with data curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api To run multiple requests in parallel for stress testing curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\ http://localhost:5000/api & wait","title":"CURL"},{"location":"deploy-flask/#openapi","text":"OpenAPI is a standard API documentation specification in ymal or json format, originated from Swagger. It usually comes with a user interface, the most popular being SwaggerUI . It provides all the information required about the API, with also an ability to test the API itself. There are three ways to go about this. generated separately as a single ymal file, and hosted using connexion generate as docstrings or individual ymal file for each endpoint using flasgger auto-generated using defined schemas, and adding additional info within the Flask app using Flask-Pydantic-Spec Personally, I believe the first is the most realistic, as the API specs are usually defined before the Flask app is created, and that doc can be sent to others for verification without creating a Flask app. Below is an example script for point 1. import connexion from flask import request from predict import prediction app = connexion . App ( __name__ , specification_dir = '.' ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : app . add_api ( 'openapi.yml' ) app . run ( host = \"0.0.0.0\" ) And point 3. \"\"\"flask server with pydantic validation & openapi integration\"\"\" from typing import List from flask import Flask , request from flask_pydantic_spec import FlaskPydanticSpec , Request , Response from pydantic import BaseModel , Field , confloat from predict import prediction app = Flask ( __name__ ) api = FlaskPydanticSpec ( \"flask\" , title = \"Objection Detection\" , version = \"v1.0.0\" ) class RequestSchema ( BaseModel ): maxResults : int = Field ( None , example = 20 , description = \"Maximum detection result to return\" ) min_height : float = Field ( None , example = 0.3 , description = \"Score\" ) min_width : float = Field ( None , example = 0.3 , description = \"Score\" ) score_th : float = Field ( None , example = 0.3 , description = \"Score\" ) nms_iou : float = Field ( ... , example = 0.4 , description = \"Non-max suppression, intersection over union\" ) type : str = Field ( ... , example = \"safetycone\" , description = \"name of object to detect\" ) image : str = Field ( ... , description = \"base64-encoded-image\" ) class _normalizedVertices ( BaseModel ): x : float = Field ( ... , example = 5.12 , description = \"X-coordinate\" ) y : float = Field ( ... , example = 20.56 , description = \"Y-coordinate\" ) width : int = Field ( ... , example = 500 , description = \"width in pixel\" ) height : int = Field ( ... , example = 600 , description = \"height in pixel\" ) score : confloat ( gt = 0.0 , lt = 1.0 ) = Field ( ... , example = 0.79 , description = \"confidence score\" ) class ResponseSchema ( BaseModel ): normalizedVertices : List [ _normalizedVertices ] @app . route ( \"/predict\" , methods = [ \"POST\" ]) @api . validate ( body = Request ( RequestSchema ), resp = Response ( HTTP_200 = ResponseSchema ), tags = [ \"API Name\" ] ) def get_predictions (): \"\"\"Short description of endpoint Long description of endpoint\"\"\" JScontent = request . json img = JScontent [ \"image\" ] response = prediction ( img ) return response if __name__ == \"__main__\" : api . register ( app ) app . run ( host = \"0.0.0.0\" ) You can refer to my repo for the full example.","title":"OpenAPI"},{"location":"deploy-serverless/","text":"Serverless Architecture Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice. Lambda & API Gateway In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins . Zappa Zappa is a popular python library used to automatically launch python lambda functions & api-gateways in AWS. VENV Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works. Setup AWS Policy & Keys for Zappa Deployment The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] } How to Use After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user role is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only. Commands Cmd Desc zappa init create a zappa_settings.json file zappa deploy <stage> deploy stage as specified in json zappa update <stage> update stage zappa undeploy <stage> delete lambda & API Gateway zappa package <stage> zip all files together zappa tail <stage> print tail logs from CloudWatch zappa status check status Lambda Execution Role The default execution role created by zappa is too liberal. From the author: It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] } Serverless Framework Serverless Framework is another powerful app to deploy serverless architecture. It's main benefits are being able to deploy to all major cloud providers, i.e. AWS, Azure and GCP, and also supporting multiple languages like nodeJS, Python, Go and Swift. The instructions for deployment are contained in a ymal file called serverless.yml to be stored at the root directory. You can check out this python blog post and a range of other examples in the serverless website for details. Setting Up Install node, and then serverless. # mac brew install node # ubuntu / wsl2 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash source ~/.nvm/nvm.sh nvm ls nvm install node # test node installation node --version npm --version # install serverless, or specific a version npm install -g serverless npm install -g serverless@~2.39.x # test serverless installation serverless -h serverless --version # alternatively sls -h sls --version We also need to store our AWS, or other cloud credentials. Note that this is similar to aws configure, and is also saved at .aws/credentials . sls config credentials --provider aws --key $ACCESS_KEY --secret $SECRET_KEY Start new node project # start a new project, create package.json npm init # install libraries for flask & python # will create a package-lock.json, & # node_modules dir which contain all the dependencies (to add in .gitignore) npm install --save-dev serverless-wsgi serverless-python-requirements Basics Before we start with anything, we need to create a python virtual environment. For that, refer to this . We can add a template serverless.yml by using the following command. This will create a template with detailed description on how to write your serverless configuration. sls create --template aws-python3 --name hello-service To deploy your lambda, we just type: sls deploy A .serverless folder is created, and within it, cloudformation templates are created, with the a zip file of the lambda function, and uploaded to an S3 bucket. If the bucket is not defined in the serverless.yml , and IAM permission allows, a new bucket will be created. The lambda function is then created via cloudformation. To delete the stack, we can just use sls remove . Serverless File for Flask # serverless.yml service : sls-flask plugins : - serverless-python-requirements - serverless-wsgi # configs for plugins custom : wsgi : app : app.app packRequirements : false pythonRequirements : dockerizePip : non-linux zip : true # if libs too large, in this case, sklearn provider : name : aws runtime : python3.8 stage : dev region : ap-southeast-1 deploymentBucket : python-serverless-deployment-s3 lambdaHashingVersion : 20201221 functions : app : handler : wsgi_handler.handler events : - http : method : ANY path : / - http : method : ANY path : /{proxy+} Warm Start Your lambda if not used for awhile, will need a delay to startup again. This is known as a cold start. To prevent this, we can create a schedule event to invoke the lambda (default 5mins) to keep it warm. Refer to this post on how to do it. Large Libraries Some libraries are huge and can exceed lambda's 50MB zip file upload. We can choose to zip the libraries using the serverless-python-requirements plugin, though there is an existing bug that is at this point of time not resolved. Another option is to upload the library(ies) as lambda layer(s), and attach it to your lambda function. See this guide for more details . API Gateway We can also add specific API Gateway configurations like API keys and usage plans. See this guide for more details .","title":"Serverless"},{"location":"deploy-serverless/#serverless-architecture","text":"Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice.","title":"Serverless Architecture"},{"location":"deploy-serverless/#lambda-api-gateway","text":"In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins .","title":"Lambda &amp; API Gateway"},{"location":"deploy-serverless/#zappa","text":"Zappa is a popular python library used to automatically launch python lambda functions & api-gateways in AWS.","title":"Zappa"},{"location":"deploy-serverless/#venv","text":"Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works.","title":"VENV"},{"location":"deploy-serverless/#setup-aws-policy-keys-for-zappa-deployment","text":"The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] }","title":"Setup AWS Policy &amp; Keys for Zappa Deployment"},{"location":"deploy-serverless/#how-to-use","text":"After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user role is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only.","title":"How to Use"},{"location":"deploy-serverless/#commands","text":"Cmd Desc zappa init create a zappa_settings.json file zappa deploy <stage> deploy stage as specified in json zappa update <stage> update stage zappa undeploy <stage> delete lambda & API Gateway zappa package <stage> zip all files together zappa tail <stage> print tail logs from CloudWatch zappa status check status","title":"Commands"},{"location":"deploy-serverless/#lambda-execution-role","text":"The default execution role created by zappa is too liberal. From the author: It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] }","title":"Lambda Execution Role"},{"location":"deploy-serverless/#serverless-framework","text":"Serverless Framework is another powerful app to deploy serverless architecture. It's main benefits are being able to deploy to all major cloud providers, i.e. AWS, Azure and GCP, and also supporting multiple languages like nodeJS, Python, Go and Swift. The instructions for deployment are contained in a ymal file called serverless.yml to be stored at the root directory. You can check out this python blog post and a range of other examples in the serverless website for details.","title":"Serverless Framework"},{"location":"deploy-serverless/#setting-up","text":"Install node, and then serverless. # mac brew install node # ubuntu / wsl2 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash source ~/.nvm/nvm.sh nvm ls nvm install node # test node installation node --version npm --version # install serverless, or specific a version npm install -g serverless npm install -g serverless@~2.39.x # test serverless installation serverless -h serverless --version # alternatively sls -h sls --version We also need to store our AWS, or other cloud credentials. Note that this is similar to aws configure, and is also saved at .aws/credentials . sls config credentials --provider aws --key $ACCESS_KEY --secret $SECRET_KEY","title":"Setting Up"},{"location":"deploy-serverless/#start-new-node-project","text":"# start a new project, create package.json npm init # install libraries for flask & python # will create a package-lock.json, & # node_modules dir which contain all the dependencies (to add in .gitignore) npm install --save-dev serverless-wsgi serverless-python-requirements","title":"Start new node project"},{"location":"deploy-serverless/#basics","text":"Before we start with anything, we need to create a python virtual environment. For that, refer to this . We can add a template serverless.yml by using the following command. This will create a template with detailed description on how to write your serverless configuration. sls create --template aws-python3 --name hello-service To deploy your lambda, we just type: sls deploy A .serverless folder is created, and within it, cloudformation templates are created, with the a zip file of the lambda function, and uploaded to an S3 bucket. If the bucket is not defined in the serverless.yml , and IAM permission allows, a new bucket will be created. The lambda function is then created via cloudformation. To delete the stack, we can just use sls remove .","title":"Basics"},{"location":"deploy-serverless/#serverless-file-for-flask","text":"# serverless.yml service : sls-flask plugins : - serverless-python-requirements - serverless-wsgi # configs for plugins custom : wsgi : app : app.app packRequirements : false pythonRequirements : dockerizePip : non-linux zip : true # if libs too large, in this case, sklearn provider : name : aws runtime : python3.8 stage : dev region : ap-southeast-1 deploymentBucket : python-serverless-deployment-s3 lambdaHashingVersion : 20201221 functions : app : handler : wsgi_handler.handler events : - http : method : ANY path : / - http : method : ANY path : /{proxy+}","title":"Serverless File for Flask"},{"location":"deploy-serverless/#warm-start","text":"Your lambda if not used for awhile, will need a delay to startup again. This is known as a cold start. To prevent this, we can create a schedule event to invoke the lambda (default 5mins) to keep it warm. Refer to this post on how to do it.","title":"Warm Start"},{"location":"deploy-serverless/#large-libraries","text":"Some libraries are huge and can exceed lambda's 50MB zip file upload. We can choose to zip the libraries using the serverless-python-requirements plugin, though there is an existing bug that is at this point of time not resolved. Another option is to upload the library(ies) as lambda layer(s), and attach it to your lambda function. See this guide for more details .","title":"Large Libraries"},{"location":"deploy-serverless/#api-gateway","text":"We can also add specific API Gateway configurations like API keys and usage plans. See this guide for more details .","title":"API Gateway"},{"location":"deploy-terraform/","text":"Terraform Terraform is a software product which launches cloud Infrastructure as Code (IaC). It supports most cloud providers, making it cloud agnostic and one of the most popular choices to do IaC.","title":"Terraform"},{"location":"deploy-terraform/#terraform","text":"Terraform is a software product which launches cloud Infrastructure as Code (IaC). It supports most cloud providers, making it cloud agnostic and one of the most popular choices to do IaC.","title":"Terraform"},{"location":"docker/","text":"Docker Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian alpine installations will need apk , while debian uses apt Docker is not the only container service available, but the most widely used Installation You may visit Docker's official website for installation in various OS and architectures. Below is for ubuntu. # Uninstall old versions sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release # Add Docker\u2019s official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg # set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null # Install latest Docker Engine sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Basics There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network Dockerfile The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7-slim RUN apt-get update && apt-get -y upgrade \\ && apt-get install ffmpeg libsm6 libxext6 -y \\ && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements-serve.txt . RUN pip3 install --no-cache-dir --upgrade pip~ = 22 .3.1 \\ && pip install --no-cache-dir -r requirements-serve.txt COPY . /app ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] Intel-GPU If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update && apt-get -y upgrade \\ && apt-get install ffmpeg libsm6 libxext6 -y \\ && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements-serve.txt . RUN pip3 install --no-cache-dir --upgrade pip~ = 22 .3.1 \\ && pip install --no-cache-dir -r requirements-serve.txt COPY . /app ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename> ARM-GPU The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename> Common Commands Build The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu Run For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 OS system-port:container-port --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id> Check Status This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats Clean To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker volume prune sudo docker network prune sudo docker system prune # force, without the need to reply to yes|no sudo docker system prune -f Debug We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs <container_name> -f sudo docker logs <container_name> -f --tail 20 At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash Storage By design, docker containers do not store persistent data. Thus, any data written in containers will not be available once the container is removed. There are three options to persist data, bind mount, volume, or tmpfs mount. More from 4sysops , and docker Bind mount provides a direct connection of a local folder's file system to the container's system. We can easily swap a file within the local folder and it will be immediately reflected within the container. This is helpful when we need to change a new model after training. docker run \\ -p 5000 :5000 \\ --mount type = bind,source = /Users/jake/Desktop/data,target = /data,readonly \\ --name <containername> <imagename> Volume mount is the preferred mechanism for updating a file from a container into the file system. The volume folder is stored in the local filesystem managed by docker in /var/lib/docker/volumes . docker run \\ -p 5000 :5000 \\ --mount type = volume,source = <volumename>,target = /data \\ --name <containername> <imagename> Cmd Desc docker volume inspect volume_name inspect the volume; view mounted directory in docker docker volume ls view all volumes in docker docker rm volume delete volume Ports Arguably one of the more confusing commands. Docker port commands involve two types Expose: opening a port in a container to communicate with other containers Bind: linking a port in a container to communicate with the host Each container is a separate environment on its own, so you can have multiple containers having the same port. The same cannot be said for the latter. Since you can only bind to one specific host port, else you will receive an error of duplicate ports. Network For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. For sending REST-APIs between docker containers in the same network, the IP address will be http://host.docker.internal for Mac & Windows, and http://172.17.0.1 for Linux. Security Patches We can improve our container security by running the os updates. More information is described here . FROM debian:buster RUN apt-get update && apt-get -y upgrade Optimize Image Size There are various ways to reduce the image size being built. Slim Build For python base image, we have many options to choose the python various and build type. As a rule-of-thumb, we can use the slim build as defined below. It has a lot less libraries, and can reduce the image by more than 500Mb. Most of the time the container can run well, though some libraries like opencv can only work with the full image. Note that the alpine build is the smallest, but more often then not, you will find that a lot of python library dependencies does not work well here. FROM python:3.8-slim Disable Cache By default, all the python libraries installed are cached. This refers to installation files(.whl, etc) or source files (.tar.gz, etc) to avoid re-download when not expired. However, this is usually not required when building your image. COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt Below shows the image size being reduced. The bottom most is the full python image, with cache. The middle is the python slim image, with cache. The top most is the python slim image with no cache. Multi-Stage We can split the Dockerfile building by defining various stages. This is very useful when we want to build to a certain stage; for e.g., for testing we only need the dependencies, and we can use the command docker build --target <stage-name> -t <image-name> . FROM python:3.9-slim AS build # os patches RUN apt-get update && apt-get -y upgrade \\ # for gh-actions path-filter && apt-get -y install git \\ && rm -rf /var/lib/apt/lists/* FROM build AS install # install python requirements COPY requirements-test.txt . RUN pip3 install --no-cache-dir --upgrade pip \\ && pip3 install --no-cache-dir -r requirements-test.txt A second benefit is to reduce the image size, and improving security by ditching unwanted programs or libraries from former stages. Linting Linters are very useful to ensure that your image build is well optimized. One of the more popular linters is called hadolint . Below is an example output after scanning using the command hadolint <Dockerfile-name> . Dockerfile:4 DL3009 info: Delete the apt-get lists after installing something Dockerfile:5 DL3008 warning: Pin versions in apt get install. Instead of ` apt-get install <package> ` use ` apt-get install <package> = <version> ` Dockerfile:5 DL3015 info: Avoid additional packages by specifying ` --no-install-recommends ` Dockerfile:5 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:6 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:6 DL3015 info: Avoid additional packages by specifying ` --no-install-recommends ` Dockerfile:6 DL3008 warning: Pin versions in apt get install. Instead of ` apt-get install <package> ` use ` apt-get install <package> = <version> ` Dockerfile:9 DL3045 warning: ` COPY ` to a relative destination without ` WORKDIR ` set. Dockerfile:10 DL3013 warning: Pin versions in pip. Instead of ` pip install <package> ` use ` pip install <package> == <version> ` or ` pip install --requirement <requirements file> ` Dockerfile:10 DL3042 warning: Avoid use of cache directory with pip. Use ` pip install --no-cache-dir <package> ` Dockerfile:11 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:18 DL3009 info: Delete the apt-get lists after installing something Buildx Docker introduced a new cli feature called buildx that makes it possible and easy to build and publish Docker images that work on multiple CPU architectures. This is very important due to the prominence of Apple M1 (ARM64), where you need to build to x64 (amd64); as well as build ARM images to leverage cheaper ARM cloud instances (can be up to 30% cheaper than x64). You can see the various supported architectures below. docker buildx ls # linux/amd64, linux/riscv64, linux/ppc64le, linux/s390x, # linux/386, linux/arm64/v8, linux/arm/v7, linux/arm/v6 To do that, we can use the buildx build --platform linux/<architecture> command. For ARM, we can omit the version by using linux/arm64 . docker buildx build --platform linux/amd64 -t sassy19a/dummy-flask . In docker-compose this can be done by specifying the platform key. version : '2.4' services : testbuild : build : .../testbuild image : testbuild platform : linux/arm64/v8 Docker-Compose When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . We need to install it first using sudo apt install docker-compose . The official Docker blog post gives a good introduction on this. version : \"3.5\" services : facedetection : build : context : ./project dockerfile : Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name : facedetection ports : - 5001:5000 logging : options : max-size : \"10m\" max-file : \"5\" deploy : resources : limits : cpus : '0.001' memory : 50M volumes : - type : bind source : /Users/jake/Desktop/source target : /model restart : unless-stopped maskdetection : build : ./maskdetection container_name : maskdetection ports : - 5001:5000 logging : options : max-size : \"10m\" max-file : \"5\" environment : - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' restart : unless-stopped We can simplify repeated code using anchors, but each anchor must have the prefix of x- . version : \"3.5\" x-common : &common logging : options : max-size : \"10m\" max-file : \"5\" restart : unless-stopped services : facedetection : build : context : ./project dockerfile : Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name : facedetection ports : - 5001:5000 deploy : resources : limits : cpus : '0.001' memory : 50M volumes : - type : bind source : /Users/jake/Desktop/source target : /model << : *common maskdetection : build : ./maskdetection container_name : maskdetection ports : - 5001:5000 environment : - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' << : *common The commands follows docker commands closely, with some of the more important ones as follows. Cmd Desc docker-compose build build images docker-compose pull pull image from a registry; must input key image docker-compose up run containers docker-compose up servicename run specific containers docker-compose up -d run containers in detached mode docker-compose ps view containers' statuses docker-compose stop stop containers docker-compose start start containers docker-compose down remove all containers docker-compose down --rmi all --volumes remove all containers, images, and volumes Docker Dashboard Docker in Windows & Mac comes by default a docker dashboard, which gives you a easy GUI to see and manage your images and containers, rather than within the commandline. However, this is lacking in Linux. A great free alternative (with more features) is Portainer . We just need to launch it using docker with the following commands, and the web-based GUI will be accessible via localhost:9000 . After creating a user account, the rest of it is pretty intuitive. sudo docker volume create portainer_data sudo docker run -d -p 8000 :8000 -p 9000 :9000 --name = portainer --restart = always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce The home page, showing the local docker connection, and some summary statistics. On clicking that, an overview of the local docker is shown. Entering the container panel, we have various options to control our containers. We can even go into the container, by clicking the console link. Blue-Green Deployment Normal docker or docker-compose deployments requires downtime during redeployment, using docker stop/start <container> or docker restart <container> commands. To achieve a zero-downtime deployment, we can use a neat trick with nginx reload functionality, together with docker-compose scale bumping of versions. The details of this blue-green deployment are well explained in these two articles [ 1 , 2 ]. This is the docker-compose file which contains the api service, as well as nginx. They are tied to the same network, so that they can communicate with each other. Note that the network is not really required to explictly defined, as by default if they are launched together in a compose file, they will be within the same network. An essential part of this file is that we must not define the container name of the api service. version : \"3.2\" services : api : build : context : . networks : - bluegreen nginx : image : nginx container_name : nginx volumes : - ./nginx-conf.d:/etc/nginx/conf.d ports : - 8080:80 networks : - bluegreen depends_on : - api networks : bluegreen : name : bluegreen This is the nginx config file which is stored at the root location of nginx-conf.d/bluegreen.conf . The most important thing of this file is that the proxy link to the api service is using the service name itself. server { listen 80; location / { proxy_pass http://api:5000; } } This is the bash script for redeployment. In essence, it will first build/pull the first image into docker. Then, it will run another new container of the api service, with the service name being bumped to a new version. Nginx will then reload to start routing to the new container. After the old container is destroyed, nginx will reload again to stop routing to the old container. # define container service to redeploy service_name = api service_port = 5000 # either build new image or pull image docker-compose build $service_name # docker-compose pull $service_name reload_nginx () { docker exec nginx /usr/sbin/nginx -s reload } zero_downtime_deploy () { old_container_id = $( docker ps -f name = $service_name -q | tail -n1 ) # bring a new container online, running new code # (nginx continues routing to the old container only) docker-compose up -d --no-deps --scale $service_name = 2 --no-recreate $service_name # wait for new container to be available new_container_id = $( docker ps -f name = $service_name -q | head -n1 ) # start routing requests to the new container (as well as the old) reload_nginx # take the old container offline docker stop $old_container_id docker rm $old_container_id docker-compose up -d --no-deps --scale $service_name = 1 --no-recreate $service_name # stop routing requests to the old container reload_nginx } zero_downtime_deploy Quite a ingenious way to achieve a zero-downtime deployment I must say.","title":"Docker"},{"location":"docker/#docker","text":"Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian alpine installations will need apk , while debian uses apt Docker is not the only container service available, but the most widely used","title":"Docker"},{"location":"docker/#installation","text":"You may visit Docker's official website for installation in various OS and architectures. Below is for ubuntu. # Uninstall old versions sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release # Add Docker\u2019s official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg # set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null # Install latest Docker Engine sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin","title":"Installation"},{"location":"docker/#basics","text":"There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network","title":"Basics"},{"location":"docker/#dockerfile","text":"The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7-slim RUN apt-get update && apt-get -y upgrade \\ && apt-get install ffmpeg libsm6 libxext6 -y \\ && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements-serve.txt . RUN pip3 install --no-cache-dir --upgrade pip~ = 22 .3.1 \\ && pip install --no-cache-dir -r requirements-serve.txt COPY . /app ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ]","title":"Dockerfile"},{"location":"docker/#intel-gpu","text":"If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update && apt-get -y upgrade \\ && apt-get install ffmpeg libsm6 libxext6 -y \\ && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements-serve.txt . RUN pip3 install --no-cache-dir --upgrade pip~ = 22 .3.1 \\ && pip install --no-cache-dir -r requirements-serve.txt COPY . /app ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename>","title":"Intel-GPU"},{"location":"docker/#arm-gpu","text":"The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename>","title":"ARM-GPU"},{"location":"docker/#common-commands","text":"","title":"Common Commands"},{"location":"docker/#build","text":"The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu","title":"Build"},{"location":"docker/#run","text":"For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 OS system-port:container-port --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id>","title":"Run"},{"location":"docker/#check-status","text":"This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats","title":"Check Status"},{"location":"docker/#clean","text":"To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker volume prune sudo docker network prune sudo docker system prune # force, without the need to reply to yes|no sudo docker system prune -f","title":"Clean"},{"location":"docker/#debug","text":"We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs <container_name> -f sudo docker logs <container_name> -f --tail 20 At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash","title":"Debug"},{"location":"docker/#storage","text":"By design, docker containers do not store persistent data. Thus, any data written in containers will not be available once the container is removed. There are three options to persist data, bind mount, volume, or tmpfs mount. More from 4sysops , and docker Bind mount provides a direct connection of a local folder's file system to the container's system. We can easily swap a file within the local folder and it will be immediately reflected within the container. This is helpful when we need to change a new model after training. docker run \\ -p 5000 :5000 \\ --mount type = bind,source = /Users/jake/Desktop/data,target = /data,readonly \\ --name <containername> <imagename> Volume mount is the preferred mechanism for updating a file from a container into the file system. The volume folder is stored in the local filesystem managed by docker in /var/lib/docker/volumes . docker run \\ -p 5000 :5000 \\ --mount type = volume,source = <volumename>,target = /data \\ --name <containername> <imagename> Cmd Desc docker volume inspect volume_name inspect the volume; view mounted directory in docker docker volume ls view all volumes in docker docker rm volume delete volume","title":"Storage"},{"location":"docker/#ports","text":"Arguably one of the more confusing commands. Docker port commands involve two types Expose: opening a port in a container to communicate with other containers Bind: linking a port in a container to communicate with the host Each container is a separate environment on its own, so you can have multiple containers having the same port. The same cannot be said for the latter. Since you can only bind to one specific host port, else you will receive an error of duplicate ports.","title":"Ports"},{"location":"docker/#network","text":"For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. For sending REST-APIs between docker containers in the same network, the IP address will be http://host.docker.internal for Mac & Windows, and http://172.17.0.1 for Linux.","title":"Network"},{"location":"docker/#security-patches","text":"We can improve our container security by running the os updates. More information is described here . FROM debian:buster RUN apt-get update && apt-get -y upgrade","title":"Security Patches"},{"location":"docker/#optimize-image-size","text":"There are various ways to reduce the image size being built.","title":"Optimize Image Size"},{"location":"docker/#slim-build","text":"For python base image, we have many options to choose the python various and build type. As a rule-of-thumb, we can use the slim build as defined below. It has a lot less libraries, and can reduce the image by more than 500Mb. Most of the time the container can run well, though some libraries like opencv can only work with the full image. Note that the alpine build is the smallest, but more often then not, you will find that a lot of python library dependencies does not work well here. FROM python:3.8-slim","title":"Slim Build"},{"location":"docker/#disable-cache","text":"By default, all the python libraries installed are cached. This refers to installation files(.whl, etc) or source files (.tar.gz, etc) to avoid re-download when not expired. However, this is usually not required when building your image. COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt Below shows the image size being reduced. The bottom most is the full python image, with cache. The middle is the python slim image, with cache. The top most is the python slim image with no cache.","title":"Disable Cache"},{"location":"docker/#multi-stage","text":"We can split the Dockerfile building by defining various stages. This is very useful when we want to build to a certain stage; for e.g., for testing we only need the dependencies, and we can use the command docker build --target <stage-name> -t <image-name> . FROM python:3.9-slim AS build # os patches RUN apt-get update && apt-get -y upgrade \\ # for gh-actions path-filter && apt-get -y install git \\ && rm -rf /var/lib/apt/lists/* FROM build AS install # install python requirements COPY requirements-test.txt . RUN pip3 install --no-cache-dir --upgrade pip \\ && pip3 install --no-cache-dir -r requirements-test.txt A second benefit is to reduce the image size, and improving security by ditching unwanted programs or libraries from former stages.","title":"Multi-Stage"},{"location":"docker/#linting","text":"Linters are very useful to ensure that your image build is well optimized. One of the more popular linters is called hadolint . Below is an example output after scanning using the command hadolint <Dockerfile-name> . Dockerfile:4 DL3009 info: Delete the apt-get lists after installing something Dockerfile:5 DL3008 warning: Pin versions in apt get install. Instead of ` apt-get install <package> ` use ` apt-get install <package> = <version> ` Dockerfile:5 DL3015 info: Avoid additional packages by specifying ` --no-install-recommends ` Dockerfile:5 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:6 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:6 DL3015 info: Avoid additional packages by specifying ` --no-install-recommends ` Dockerfile:6 DL3008 warning: Pin versions in apt get install. Instead of ` apt-get install <package> ` use ` apt-get install <package> = <version> ` Dockerfile:9 DL3045 warning: ` COPY ` to a relative destination without ` WORKDIR ` set. Dockerfile:10 DL3013 warning: Pin versions in pip. Instead of ` pip install <package> ` use ` pip install <package> == <version> ` or ` pip install --requirement <requirements file> ` Dockerfile:10 DL3042 warning: Avoid use of cache directory with pip. Use ` pip install --no-cache-dir <package> ` Dockerfile:11 DL3059 info: Multiple consecutive ` RUN ` instructions. Consider consolidation. Dockerfile:18 DL3009 info: Delete the apt-get lists after installing something","title":"Linting"},{"location":"docker/#buildx","text":"Docker introduced a new cli feature called buildx that makes it possible and easy to build and publish Docker images that work on multiple CPU architectures. This is very important due to the prominence of Apple M1 (ARM64), where you need to build to x64 (amd64); as well as build ARM images to leverage cheaper ARM cloud instances (can be up to 30% cheaper than x64). You can see the various supported architectures below. docker buildx ls # linux/amd64, linux/riscv64, linux/ppc64le, linux/s390x, # linux/386, linux/arm64/v8, linux/arm/v7, linux/arm/v6 To do that, we can use the buildx build --platform linux/<architecture> command. For ARM, we can omit the version by using linux/arm64 . docker buildx build --platform linux/amd64 -t sassy19a/dummy-flask . In docker-compose this can be done by specifying the platform key. version : '2.4' services : testbuild : build : .../testbuild image : testbuild platform : linux/arm64/v8","title":"Buildx"},{"location":"docker/#docker-compose","text":"When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . We need to install it first using sudo apt install docker-compose . The official Docker blog post gives a good introduction on this. version : \"3.5\" services : facedetection : build : context : ./project dockerfile : Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name : facedetection ports : - 5001:5000 logging : options : max-size : \"10m\" max-file : \"5\" deploy : resources : limits : cpus : '0.001' memory : 50M volumes : - type : bind source : /Users/jake/Desktop/source target : /model restart : unless-stopped maskdetection : build : ./maskdetection container_name : maskdetection ports : - 5001:5000 logging : options : max-size : \"10m\" max-file : \"5\" environment : - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' restart : unless-stopped We can simplify repeated code using anchors, but each anchor must have the prefix of x- . version : \"3.5\" x-common : &common logging : options : max-size : \"10m\" max-file : \"5\" restart : unless-stopped services : facedetection : build : context : ./project dockerfile : Dockerfile-api # if Dockerfile name is not changed, can just use below # build: ./project container_name : facedetection ports : - 5001:5000 deploy : resources : limits : cpus : '0.001' memory : 50M volumes : - type : bind source : /Users/jake/Desktop/source target : /model << : *common maskdetection : build : ./maskdetection container_name : maskdetection ports : - 5001:5000 environment : - 'api_url={\"asc\":\"http://172.17.0.1:5001/product-association\", \"sml\":\"http://172.17.0.1:5002/product-similarity\", \"trd\":\"http://172.17.0.1:5003/product-trending\", \"psn\":\"http://172.17.0.1:5004/product-personalised\"}' - 'resultSize=10' << : *common The commands follows docker commands closely, with some of the more important ones as follows. Cmd Desc docker-compose build build images docker-compose pull pull image from a registry; must input key image docker-compose up run containers docker-compose up servicename run specific containers docker-compose up -d run containers in detached mode docker-compose ps view containers' statuses docker-compose stop stop containers docker-compose start start containers docker-compose down remove all containers docker-compose down --rmi all --volumes remove all containers, images, and volumes","title":"Docker-Compose"},{"location":"docker/#docker-dashboard","text":"Docker in Windows & Mac comes by default a docker dashboard, which gives you a easy GUI to see and manage your images and containers, rather than within the commandline. However, this is lacking in Linux. A great free alternative (with more features) is Portainer . We just need to launch it using docker with the following commands, and the web-based GUI will be accessible via localhost:9000 . After creating a user account, the rest of it is pretty intuitive. sudo docker volume create portainer_data sudo docker run -d -p 8000 :8000 -p 9000 :9000 --name = portainer --restart = always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce The home page, showing the local docker connection, and some summary statistics. On clicking that, an overview of the local docker is shown. Entering the container panel, we have various options to control our containers. We can even go into the container, by clicking the console link.","title":"Docker Dashboard"},{"location":"docker/#blue-green-deployment","text":"Normal docker or docker-compose deployments requires downtime during redeployment, using docker stop/start <container> or docker restart <container> commands. To achieve a zero-downtime deployment, we can use a neat trick with nginx reload functionality, together with docker-compose scale bumping of versions. The details of this blue-green deployment are well explained in these two articles [ 1 , 2 ]. This is the docker-compose file which contains the api service, as well as nginx. They are tied to the same network, so that they can communicate with each other. Note that the network is not really required to explictly defined, as by default if they are launched together in a compose file, they will be within the same network. An essential part of this file is that we must not define the container name of the api service. version : \"3.2\" services : api : build : context : . networks : - bluegreen nginx : image : nginx container_name : nginx volumes : - ./nginx-conf.d:/etc/nginx/conf.d ports : - 8080:80 networks : - bluegreen depends_on : - api networks : bluegreen : name : bluegreen This is the nginx config file which is stored at the root location of nginx-conf.d/bluegreen.conf . The most important thing of this file is that the proxy link to the api service is using the service name itself. server { listen 80; location / { proxy_pass http://api:5000; } } This is the bash script for redeployment. In essence, it will first build/pull the first image into docker. Then, it will run another new container of the api service, with the service name being bumped to a new version. Nginx will then reload to start routing to the new container. After the old container is destroyed, nginx will reload again to stop routing to the old container. # define container service to redeploy service_name = api service_port = 5000 # either build new image or pull image docker-compose build $service_name # docker-compose pull $service_name reload_nginx () { docker exec nginx /usr/sbin/nginx -s reload } zero_downtime_deploy () { old_container_id = $( docker ps -f name = $service_name -q | tail -n1 ) # bring a new container online, running new code # (nginx continues routing to the old container only) docker-compose up -d --no-deps --scale $service_name = 2 --no-recreate $service_name # wait for new container to be available new_container_id = $( docker ps -f name = $service_name -q | head -n1 ) # start routing requests to the new container (as well as the old) reload_nginx # take the old container offline docker stop $old_container_id docker rm $old_container_id docker-compose up -d --no-deps --scale $service_name = 1 --no-recreate $service_name # stop routing requests to the old container reload_nginx } zero_downtime_deploy Quite a ingenious way to achieve a zero-downtime deployment I must say.","title":"Blue-Green Deployment"},{"location":"dvc/","text":"Data Version Control DVC is a popular data version control library that implements data versioning together with Git. The purpose of data versioning in MLOps is that we can trace for each training run, what is the data version used. TL;DR # setting up ------- dvc init dvc remote add <remote-name> <s3-bucket-url> git add .dvc/ git commit -m \"add dvc config\" git push # store a data version ------- dvc add -r <remote-name> <data-directory> dvc push git add <data-directory>.dvc git commit -m \"data-version-commit-message\" git push # pull a data version ------- git pull # OR git checkout <git-commit-hash> dvc pull -r <remote-name> <data-directory>.dvc Setting Up Install pip install dvc [ s3 ] Initialisation cd <repository-root> dvc init Initialisation creates the following: .dvcignore file: exclude files from DVC .dvc folder cache folder: stores data version and corresponding hash keys. They will not be uploaded to git as a .gitignorefile is added to this folder to exclude config file: stores all dvc remote urls (e.g. s3 bucket links) Add Remote Link We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket # dvc remote add <remote-name> <s3-bucket-link> dvc remote add tp1 s3://rre/data/tp1 This will update the .dvc/config file as shown below. [ 'remote \"tp1\"' ] url = s3://rre/data/tp1 To assign a default remote, add a -d option dvc remote add -d tp2 s3://rre/data/tp2 The updated config will be [ core ] remote = tp2 [ 'remote \"tp1\"' ] url = s3://rre/data/tp1 [ 'remote \"tp2\"' ] url = s3://rre/data/tp2 Commit the config file to your git repo git add .dvc/ git commit -m \"Add dvc config\" git push Store Data Version DVC manages data versioning using two ways Publishing a data version to a remote (i.e. S3) using DVC commands Committing the data version\u2019s hash key to a remote git repository using Git commands Committing a Data Version Place your first version of your data to a directory, e.g., /data Add this data version to staging dvc add <data-directory>/ A file <data-directory>.dvc is created, containing the hash key of this data version Push the data version to remote # if you have set a default remote # and only one <data-directory>.dvc file in directory dvc push # more specific command dvc push -r <remotelink-name> <data-directory>.dvc Committing a Data Version Hash We need to commit the .dvc file to git so that we can retrieve the hash key for the data version based on the git commit message, or any tags git add <data-directory>.dvc git commit -m \"data version 1\" git push Pull Data Version To extract a data version, we need to pull the appropriate <data-directory>.dvc which contains the data version hash, using git pull the data version from S3, using dvc # pull current data version hash of <data-directory>.dvc git pull # pull previous data version hash of <data-directory>.dvc git checkout <commit-hash> # pull the data version dvc pull # or more specific dvc pull -r <remotelink-name> <data-directory>.dvc Read Data Version The python API allows you to read single files (only) directly from the DVC remote. This enables you to easily integrate DVC to your data/training pipeline script. See this link for full list of arguments. import pickle import dvc.api text_file = dvc . api . read ( path = \"<path-to-file-relative-to-repo-root>\" ) pickle_file = dvc . api . read ( path = \"<path-to-file-relative-to-repo-root>\" , model = \"rb\" ) pickle_file = pickle . loads ( pickle_file )","title":"DVC"},{"location":"dvc/#data-version-control","text":"DVC is a popular data version control library that implements data versioning together with Git. The purpose of data versioning in MLOps is that we can trace for each training run, what is the data version used.","title":"Data Version Control"},{"location":"dvc/#tldr","text":"# setting up ------- dvc init dvc remote add <remote-name> <s3-bucket-url> git add .dvc/ git commit -m \"add dvc config\" git push # store a data version ------- dvc add -r <remote-name> <data-directory> dvc push git add <data-directory>.dvc git commit -m \"data-version-commit-message\" git push # pull a data version ------- git pull # OR git checkout <git-commit-hash> dvc pull -r <remote-name> <data-directory>.dvc","title":"TL;DR"},{"location":"dvc/#setting-up","text":"","title":"Setting Up"},{"location":"dvc/#install","text":"pip install dvc [ s3 ]","title":"Install"},{"location":"dvc/#initialisation","text":"cd <repository-root> dvc init Initialisation creates the following: .dvcignore file: exclude files from DVC .dvc folder cache folder: stores data version and corresponding hash keys. They will not be uploaded to git as a .gitignorefile is added to this folder to exclude config file: stores all dvc remote urls (e.g. s3 bucket links)","title":"Initialisation"},{"location":"dvc/#add-remote-link","text":"We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket # dvc remote add <remote-name> <s3-bucket-link> dvc remote add tp1 s3://rre/data/tp1 This will update the .dvc/config file as shown below. [ 'remote \"tp1\"' ] url = s3://rre/data/tp1 To assign a default remote, add a -d option dvc remote add -d tp2 s3://rre/data/tp2 The updated config will be [ core ] remote = tp2 [ 'remote \"tp1\"' ] url = s3://rre/data/tp1 [ 'remote \"tp2\"' ] url = s3://rre/data/tp2 Commit the config file to your git repo git add .dvc/ git commit -m \"Add dvc config\" git push","title":"Add Remote Link"},{"location":"dvc/#store-data-version","text":"DVC manages data versioning using two ways Publishing a data version to a remote (i.e. S3) using DVC commands Committing the data version\u2019s hash key to a remote git repository using Git commands","title":"Store Data Version"},{"location":"dvc/#committing-a-data-version","text":"Place your first version of your data to a directory, e.g., /data Add this data version to staging dvc add <data-directory>/ A file <data-directory>.dvc is created, containing the hash key of this data version Push the data version to remote # if you have set a default remote # and only one <data-directory>.dvc file in directory dvc push # more specific command dvc push -r <remotelink-name> <data-directory>.dvc","title":"Committing a Data Version"},{"location":"dvc/#committing-a-data-version-hash","text":"We need to commit the .dvc file to git so that we can retrieve the hash key for the data version based on the git commit message, or any tags git add <data-directory>.dvc git commit -m \"data version 1\" git push","title":"Committing a Data Version Hash"},{"location":"dvc/#pull-data-version","text":"To extract a data version, we need to pull the appropriate <data-directory>.dvc which contains the data version hash, using git pull the data version from S3, using dvc # pull current data version hash of <data-directory>.dvc git pull # pull previous data version hash of <data-directory>.dvc git checkout <commit-hash> # pull the data version dvc pull # or more specific dvc pull -r <remotelink-name> <data-directory>.dvc","title":"Pull Data Version"},{"location":"dvc/#read-data-version","text":"The python API allows you to read single files (only) directly from the DVC remote. This enables you to easily integrate DVC to your data/training pipeline script. See this link for full list of arguments. import pickle import dvc.api text_file = dvc . api . read ( path = \"<path-to-file-relative-to-repo-root>\" ) pickle_file = dvc . api . read ( path = \"<path-to-file-relative-to-repo-root>\" , model = \"rb\" ) pickle_file = pickle . loads ( pickle_file )","title":"Read Data Version"},{"location":"git/","text":"Git Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code Access from Dev Env to Git Host In your terminal > ssh-keygen Accept all defaults Copy the public key cat ~/.ssh/id_rsa.pub Go to Github/Gitlab Settings > SSH Keys, and paste the keys here Remote URL When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git Branches Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. Commands # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname> Workflow Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch Merge Request Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request. Version Control Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> Commands To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request. Resets Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset HEAD <file.py> Revert If we want to revert commits in the remote, we can use the following. # git push -f origin <commit-hash>:<branch-name> git push -f origin 4a30214b819:master Workflow The various git commands and how they interact at different stages are as illustrated below. Release Tags We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git tag -a v1.0.0 -m \"1st msg\" -m \"2nd msg\" tag multiple msgs in new lines git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags Remove last Commit At times, we want to remove the last commit from remote. Use the following to do a hard reset of the head and force push to the origin. git reset --hard HEAD^ git push origin -f Squash all Commits If, for whatever reason, we wish to squash all commits to the head commit, e.g. to clear all commits for a template repo. git reset $( git commit-tree HEAD^ { tree } -m \"squash all commits to head\" ) git push --force Delete from Git History BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. We cannot delete specific files and folders based on path, the reasoning given by the developer of BFG in stackoverflow . CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git java -jar bfg-<version>.jar --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions java -jar bfg-<version>.jar --delete-folders <foldername> repo-name.git delete all folders with the folder model cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote After uploading the new git history to the repository, it is important to not push any of you or your team's legacy git history back to the repo.","title":"GIT"},{"location":"git/#git","text":"Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code","title":"Git"},{"location":"git/#access-from-dev-env-to-git-host","text":"In your terminal > ssh-keygen Accept all defaults Copy the public key cat ~/.ssh/id_rsa.pub Go to Github/Gitlab Settings > SSH Keys, and paste the keys here","title":"Access from Dev Env to Git Host"},{"location":"git/#remote-url","text":"When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git","title":"Remote URL"},{"location":"git/#branches","text":"Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option.","title":"Branches"},{"location":"git/#commands","text":"# check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname>","title":"Commands"},{"location":"git/#workflow","text":"Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch","title":"Workflow"},{"location":"git/#merge-request","text":"Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Merge Request"},{"location":"git/#version-control","text":"Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename>","title":"Version Control"},{"location":"git/#commands_1","text":"To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request.","title":"Commands"},{"location":"git/#resets","text":"Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset HEAD <file.py>","title":"Resets"},{"location":"git/#revert","text":"If we want to revert commits in the remote, we can use the following. # git push -f origin <commit-hash>:<branch-name> git push -f origin 4a30214b819:master","title":"Revert"},{"location":"git/#workflow_1","text":"The various git commands and how they interact at different stages are as illustrated below.","title":"Workflow"},{"location":"git/#release-tags","text":"We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git tag -a v1.0.0 -m \"1st msg\" -m \"2nd msg\" tag multiple msgs in new lines git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags","title":"Release Tags"},{"location":"git/#remove-last-commit","text":"At times, we want to remove the last commit from remote. Use the following to do a hard reset of the head and force push to the origin. git reset --hard HEAD^ git push origin -f","title":"Remove last Commit"},{"location":"git/#squash-all-commits","text":"If, for whatever reason, we wish to squash all commits to the head commit, e.g. to clear all commits for a template repo. git reset $( git commit-tree HEAD^ { tree } -m \"squash all commits to head\" ) git push --force","title":"Squash all Commits"},{"location":"git/#delete-from-git-history","text":"BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. We cannot delete specific files and folders based on path, the reasoning given by the developer of BFG in stackoverflow . CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git java -jar bfg-<version>.jar --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions java -jar bfg-<version>.jar --delete-folders <foldername> repo-name.git delete all folders with the folder model cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote After uploading the new git history to the repository, it is important to not push any of you or your team's legacy git history back to the repo.","title":"Delete from Git History"},{"location":"gpu/","text":"Using GPU Neural Networks are processed using GPU as they can better compute multiple parallel processes. Nvidia graphic cards are currently the defacto GPU used, with it supporting all neural network libraries. Unfortunately, it is sometimes a pain to install CUDA and get it working with our libraries, so hopefully this page serves as a guide to overcome the difficulties. Compatibility For tensorflow and pytorch, they have compatibility setups for CUDA versions, and also for the former; cudnn, python and the compiler. You can view the requirements from the docs of tensorflow and pytorch . GPU setups compatible with tensorflow versions Remove/Uninstall CUDA Sometimes, we need to remove legacy versions of CUDA so that we can install a new one to work with our libraries. To do that, we can use the following commands. sudo rm /etc/apt/sources.list.d/cuda* sudo apt remove --autoremove nvidia-cuda-toolkit sudo apt remove --autoremove nvidia-* Installing CUDA 10.1 in Ubuntu 18 & 20 A great help from this medium article, CUDA 10.1 installation on Ubuntu 20.04 . A more descriptive article can be viewed here First we setup the correct CUDA PPA on your system. sudo apt update sudo add-apt-repository ppa:graphics-drivers sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list' Then, install CUDA 10.1 packages sudo apt update sudo apt install cuda-10-1 sudo apt install libcudnn7 As the last step one need to specify PATH to CUDA in \u2018.profile\u2019 file. Open the file by running sudo nano ~/.profile And add the following lines at the end of the file. # set PATH for cuda 10.1 installation if [ -d \"/usr/local/cuda-10.1/bin/\" ] ; then export PATH = /usr/local/cuda-10.1/bin ${ PATH :+: ${ PATH }} export LD_LIBRARY_PATH = /usr/local/cuda-10.1/lib64 ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} fi Reboot the system. Installing CUDA For a more generic installation, we can do the following, though I have personally not succeeded in getting this work. Haha. After verifying the version, go to Nvidia's CUDA website , and select the version required. You will be led to a selection screen to specify your OS. As part of the CUDA installation, cudnn , the relevant driver, and nvidia-smi are installed. Verification System To check that CUDA is installed properly at the system level, we can use the following commands in the terminal to check. Noun Desc cat /proc/driver/nvidia/version Verify driver & gcc version nvcc -V Verify CUDA toolkit version nvidia-smi CUDA & driver versions, check memory usage Library To check that tensorflow and pytorch can detect the GPU and use it, we can use the following commands. # tensorflow import tensorflow as tf # return empty list if not available tf . config . list_physical_devices ( 'GPU' ) # pytorch import torch torch . cuda . is_available () # True torch . cuda . current_device () # 0 torch . cuda . device ( 0 ) # <torch.cuda.device at 0x7efce0b03be0> torch . cuda . device_count () # 1 torch . cuda . get_device_name ( 0 ) # 'GeForce GTX 950M' Kill GPU processes Sometimes our GPU might be out of memory. We can use nvidia-smi to check what processes and application used are taking up the memory so that we can close the app. If we are unable to locate the application, we can kill the process directly as follows. sudo kill - 9 < process id > Why so Hard? As to why it is so hard to get install CUDA. Watch this famous YouTube video .","title":"Using GPU"},{"location":"gpu/#using-gpu","text":"Neural Networks are processed using GPU as they can better compute multiple parallel processes. Nvidia graphic cards are currently the defacto GPU used, with it supporting all neural network libraries. Unfortunately, it is sometimes a pain to install CUDA and get it working with our libraries, so hopefully this page serves as a guide to overcome the difficulties.","title":"Using GPU"},{"location":"gpu/#compatibility","text":"For tensorflow and pytorch, they have compatibility setups for CUDA versions, and also for the former; cudnn, python and the compiler. You can view the requirements from the docs of tensorflow and pytorch . GPU setups compatible with tensorflow versions","title":"Compatibility"},{"location":"gpu/#removeuninstall-cuda","text":"Sometimes, we need to remove legacy versions of CUDA so that we can install a new one to work with our libraries. To do that, we can use the following commands. sudo rm /etc/apt/sources.list.d/cuda* sudo apt remove --autoremove nvidia-cuda-toolkit sudo apt remove --autoremove nvidia-*","title":"Remove/Uninstall CUDA"},{"location":"gpu/#installing-cuda-101-in-ubuntu-18-20","text":"A great help from this medium article, CUDA 10.1 installation on Ubuntu 20.04 . A more descriptive article can be viewed here First we setup the correct CUDA PPA on your system. sudo apt update sudo add-apt-repository ppa:graphics-drivers sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list' Then, install CUDA 10.1 packages sudo apt update sudo apt install cuda-10-1 sudo apt install libcudnn7 As the last step one need to specify PATH to CUDA in \u2018.profile\u2019 file. Open the file by running sudo nano ~/.profile And add the following lines at the end of the file. # set PATH for cuda 10.1 installation if [ -d \"/usr/local/cuda-10.1/bin/\" ] ; then export PATH = /usr/local/cuda-10.1/bin ${ PATH :+: ${ PATH }} export LD_LIBRARY_PATH = /usr/local/cuda-10.1/lib64 ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} fi Reboot the system.","title":"Installing CUDA 10.1 in Ubuntu 18 &amp; 20"},{"location":"gpu/#installing-cuda","text":"For a more generic installation, we can do the following, though I have personally not succeeded in getting this work. Haha. After verifying the version, go to Nvidia's CUDA website , and select the version required. You will be led to a selection screen to specify your OS. As part of the CUDA installation, cudnn , the relevant driver, and nvidia-smi are installed.","title":"Installing CUDA"},{"location":"gpu/#verification","text":"","title":"Verification"},{"location":"gpu/#system","text":"To check that CUDA is installed properly at the system level, we can use the following commands in the terminal to check. Noun Desc cat /proc/driver/nvidia/version Verify driver & gcc version nvcc -V Verify CUDA toolkit version nvidia-smi CUDA & driver versions, check memory usage","title":"System"},{"location":"gpu/#library","text":"To check that tensorflow and pytorch can detect the GPU and use it, we can use the following commands. # tensorflow import tensorflow as tf # return empty list if not available tf . config . list_physical_devices ( 'GPU' ) # pytorch import torch torch . cuda . is_available () # True torch . cuda . current_device () # 0 torch . cuda . device ( 0 ) # <torch.cuda.device at 0x7efce0b03be0> torch . cuda . device_count () # 1 torch . cuda . get_device_name ( 0 ) # 'GeForce GTX 950M'","title":"Library"},{"location":"gpu/#kill-gpu-processes","text":"Sometimes our GPU might be out of memory. We can use nvidia-smi to check what processes and application used are taking up the memory so that we can close the app. If we are unable to locate the application, we can kill the process directly as follows. sudo kill - 9 < process id >","title":"Kill GPU processes"},{"location":"gpu/#why-so-hard","text":"As to why it is so hard to get install CUDA. Watch this famous YouTube video .","title":"Why so Hard?"},{"location":"logging/","text":"Logging Flask, FastAPI, gunicorn, and even docker have some form of logging, or integrate the python logging library within them. This page specfically discusses about just the python default logging library. Basic Log Below describes a simple logging assignment with basic config. import logging logging . basicConfig ( filename = 'model.log' , filemode = 'a' , level = logging . DEBUG , format = ' %(asctime)s . %(msecs)03d %(levelname)s %(module)s - %(funcName)s : %(message)s ' , datefmt = '%Y-%m- %d %H:%M:%S' ) logging . debug ( \"A Debug Logging Message\" ) logging . info ( \"A Info Logging Message\" ) logging . warning ( \"A Warning Logging Message\" ) logging . error ( \"An Error Logging Message\" ) logging . critical ( \"A Critical Logging Message\" ) Logging comes with different levels, indicating their severity. This is what can be seen from the log file using the above example. 2021-10-11 14:32:33.445 DEBUG logging_test - main: A Debug Logging Message 22021-10-11 14:32:33.445 INFO logging_test - main: A Info Logging Message 32021-10-11 14:32:33.445 WARNING logging_test - main: A Warning Logging Message 42021-10-11 14:32:33.445 ERROR logging_test - main: An Error Logging Message 52021-10-11 14:32:33.445 CRITICAL logging_test - main: A Critical Logging Message Rotating Log The limitation of the basic log configuration is that it cannot set a hard limit on the size of logs being collected, and can overload the server's harddisk over time. We can use the RotatingFileHandler in this case. import logging from logging.handlers import RotatingFileHandler rfh = logging . handlers . RotatingFileHandler ( filename = 'response.log' , #path & name of logfile mode = 'a' , maxBytes = 100 * 1024 * 1024 , #100mb limit for each file backupCount = 3 , #max of 4 files to be stored, therefore 400mb of logs will be saved encoding = None , delay = 0 ) logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s %(message)s \" , datefmt = \"%y-%m- %d %H:%M:%S\" , handlers = [ rfh ] ) logger = logging . getLogger ( 'main' ) We can see that multiple log files with response.log* will be saved, each with a maximum of 100Mb. . \u251c\u2500\u2500 [ 18 ] requirements.txt \u251c\u2500\u2500 [ 95M ] response.log \u251c\u2500\u2500 [ 100M ] response.log.1 \u251c\u2500\u2500 [ 100M ] response.log.2 \u2514\u2500\u2500 [ 1 .0K ] rotatinglogger.py Other handlers There are many other handlers for specific use-cases.","title":"Logging"},{"location":"logging/#logging","text":"Flask, FastAPI, gunicorn, and even docker have some form of logging, or integrate the python logging library within them. This page specfically discusses about just the python default logging library.","title":"Logging"},{"location":"logging/#basic-log","text":"Below describes a simple logging assignment with basic config. import logging logging . basicConfig ( filename = 'model.log' , filemode = 'a' , level = logging . DEBUG , format = ' %(asctime)s . %(msecs)03d %(levelname)s %(module)s - %(funcName)s : %(message)s ' , datefmt = '%Y-%m- %d %H:%M:%S' ) logging . debug ( \"A Debug Logging Message\" ) logging . info ( \"A Info Logging Message\" ) logging . warning ( \"A Warning Logging Message\" ) logging . error ( \"An Error Logging Message\" ) logging . critical ( \"A Critical Logging Message\" ) Logging comes with different levels, indicating their severity. This is what can be seen from the log file using the above example. 2021-10-11 14:32:33.445 DEBUG logging_test - main: A Debug Logging Message 22021-10-11 14:32:33.445 INFO logging_test - main: A Info Logging Message 32021-10-11 14:32:33.445 WARNING logging_test - main: A Warning Logging Message 42021-10-11 14:32:33.445 ERROR logging_test - main: An Error Logging Message 52021-10-11 14:32:33.445 CRITICAL logging_test - main: A Critical Logging Message","title":"Basic Log"},{"location":"logging/#rotating-log","text":"The limitation of the basic log configuration is that it cannot set a hard limit on the size of logs being collected, and can overload the server's harddisk over time. We can use the RotatingFileHandler in this case. import logging from logging.handlers import RotatingFileHandler rfh = logging . handlers . RotatingFileHandler ( filename = 'response.log' , #path & name of logfile mode = 'a' , maxBytes = 100 * 1024 * 1024 , #100mb limit for each file backupCount = 3 , #max of 4 files to be stored, therefore 400mb of logs will be saved encoding = None , delay = 0 ) logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s %(message)s \" , datefmt = \"%y-%m- %d %H:%M:%S\" , handlers = [ rfh ] ) logger = logging . getLogger ( 'main' ) We can see that multiple log files with response.log* will be saved, each with a maximum of 100Mb. . \u251c\u2500\u2500 [ 18 ] requirements.txt \u251c\u2500\u2500 [ 95M ] response.log \u251c\u2500\u2500 [ 100M ] response.log.1 \u251c\u2500\u2500 [ 100M ] response.log.2 \u2514\u2500\u2500 [ 1 .0K ] rotatinglogger.py","title":"Rotating Log"},{"location":"logging/#other-handlers","text":"There are many other handlers for specific use-cases.","title":"Other handlers"},{"location":"ml-process/","text":"Machine Learning Life Cycle Andrew Ng used another similar flow to describe the process, and explained it quite succinctly using an example speech recognition project. Machine Learning Life Cycle Scoping * Decide to work on a speech recognition for voice search * Key metrics: accuracy, latency, throughput * Estimate resources & timeline Data * Is data labelled consistently? * How much silence before and after each clip? * How to perform volume normalization","title":"Machine Learning Life Cycle"},{"location":"ml-process/#machine-learning-life-cycle","text":"Andrew Ng used another similar flow to describe the process, and explained it quite succinctly using an example speech recognition project. Machine Learning Life Cycle Scoping * Decide to work on a speech recognition for voice search * Key metrics: accuracy, latency, throughput * Estimate resources & timeline Data * Is data labelled consistently? * How much silence before and after each clip? * How to perform volume normalization","title":"Machine Learning Life Cycle"},{"location":"mlflow/","text":"Model Version The process of model development usually involves various iterations. This can be due to changes in the data, model type, and model hyperparameters, all of which can influence the final evaluation metrics selected. Therefore, we need a system to track each of the model version we trained, as well as log all the different information we used for each training cycle. Together with model deployment & monitoring tools, this comprises of the entire ML lifecycle, also known as MLOps. MLFlow There are a number of different model version platforms, like AWS Sagemaker , and DVC Pipelines + CML . However, one of most popular open-source platform is MLFlow , which we will demostrate here. To start, we install MLFlow with pip install mlflow , and to launch the user interface, mlflow ui . For a more verbose specification, to indicate the database, artifacts folder and the server ip (and port) we can do as below. cd <project-directory> mlflow server \\ --backend-store-uri sqlite:///mlflow.db \\ --default-artifact-root ./artifacts \\ --host 0 .0.0.0 -p 5000 Python Code The mlflow tracking and model APIs are very easy to use. It involves the following. Starting CMD Desc mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") point to mlflow server mlflow.set_experiment(\"iris\") create experiment if not exist mlflow.start_run(run_name=\"test run\") give a name for each run (optional) Logging Params, Metrics, Artifacts CMD Desc log_param(\"n_estimators\", n_estimators) log some parameter log_metric(\"f1\", f1) log evaluation metric log_artifacts(\"logs\") log any files Note that we can also (log metrics based on epochs)[https://www.mlflow.org/docs/latest/tracking.html#performance-tracking-with-metrics] of a training so that we can plot the loss over each epoch. This is done by including the step argument in mlflow.log_metrics(key=\"loss\", value=loss, step=epoch) . Logging Model We also also auto-save the model using the model \"flavor\" API. The model signature, which is the input and output names and format, can also be recorded. All these information will also be stored under artifacts. signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) Code import os from statistics import mean import matplotlib.pyplot as plt import mlflow import mlflow.sklearn import pandas as pd from mlflow import log_artifacts , log_metric , log_param from mlflow.models.signature import infer_signature from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import ( f1_score , plot_confusion_matrix , precision_score , recall_score ) from sklearn.model_selection import train_test_split iris = load_iris () X = pd . DataFrame ( iris [ \"data\" ], columns = iris [ \"feature_names\" ]) y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) def train ( n_estimators ): model = RandomForestClassifier ( n_estimators = n_estimators ) model = model . fit ( X_train , y_train ) return model def evaluation ( model , y_predict ): f1 = mean ( f1_score ( y_test , y_predict , average = None )) precision = mean ( precision_score ( y_test , y_predict , average = None )) recall = mean ( recall_score ( y_test , y_predict , average = None )) confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) return acc , f1 , precision , recall , confusion def mlflow_logs ( n_estimators , acc , f1 , precision , recall ): log_param ( \"n_estimators\" , n_estimators ) log_metric ( \"f1\" , f1 ) log_metric ( \"precision\" , precision ) log_metric ( \"recall\" , recall ) log_artifacts ( \"logs\" ) if __name__ == \"__main__\" : # start mlflow mlflow . set_tracking_uri ( \"http://127.0.0.1:5000\" ) mlflow . set_experiment ( \"iris\" ) mlflow . start_run ( run_name = \"test run\" ) n_estimators = 99 # train, predict, evaluate model = train ( n_estimators ) y_predict = model . predict ( X_test ) acc , f1 , precision , recall , confusion = evaluation ( model , y_predict ) # mlflow logging mlflow_logs ( n_estimators , acc , f1 , precision , recall ) signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) MLFlow UI This is the interface seen when MLFlow launches. Main UI Clicking on a specific run leads to its details. Run Details And any artifacts stored. This can include any file, including images. Artifacts Or the model. Model artifact & signature If we registered the model, it will be recorded in the model registry. Model registry","title":"Model Version Control"},{"location":"mlflow/#model-version","text":"The process of model development usually involves various iterations. This can be due to changes in the data, model type, and model hyperparameters, all of which can influence the final evaluation metrics selected. Therefore, we need a system to track each of the model version we trained, as well as log all the different information we used for each training cycle. Together with model deployment & monitoring tools, this comprises of the entire ML lifecycle, also known as MLOps.","title":"Model Version"},{"location":"mlflow/#mlflow","text":"There are a number of different model version platforms, like AWS Sagemaker , and DVC Pipelines + CML . However, one of most popular open-source platform is MLFlow , which we will demostrate here. To start, we install MLFlow with pip install mlflow , and to launch the user interface, mlflow ui . For a more verbose specification, to indicate the database, artifacts folder and the server ip (and port) we can do as below. cd <project-directory> mlflow server \\ --backend-store-uri sqlite:///mlflow.db \\ --default-artifact-root ./artifacts \\ --host 0 .0.0.0 -p 5000","title":"MLFlow"},{"location":"mlflow/#python-code","text":"The mlflow tracking and model APIs are very easy to use. It involves the following. Starting CMD Desc mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") point to mlflow server mlflow.set_experiment(\"iris\") create experiment if not exist mlflow.start_run(run_name=\"test run\") give a name for each run (optional) Logging Params, Metrics, Artifacts CMD Desc log_param(\"n_estimators\", n_estimators) log some parameter log_metric(\"f1\", f1) log evaluation metric log_artifacts(\"logs\") log any files Note that we can also (log metrics based on epochs)[https://www.mlflow.org/docs/latest/tracking.html#performance-tracking-with-metrics] of a training so that we can plot the loss over each epoch. This is done by including the step argument in mlflow.log_metrics(key=\"loss\", value=loss, step=epoch) . Logging Model We also also auto-save the model using the model \"flavor\" API. The model signature, which is the input and output names and format, can also be recorded. All these information will also be stored under artifacts. signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature ) Code import os from statistics import mean import matplotlib.pyplot as plt import mlflow import mlflow.sklearn import pandas as pd from mlflow import log_artifacts , log_metric , log_param from mlflow.models.signature import infer_signature from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import ( f1_score , plot_confusion_matrix , precision_score , recall_score ) from sklearn.model_selection import train_test_split iris = load_iris () X = pd . DataFrame ( iris [ \"data\" ], columns = iris [ \"feature_names\" ]) y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) def train ( n_estimators ): model = RandomForestClassifier ( n_estimators = n_estimators ) model = model . fit ( X_train , y_train ) return model def evaluation ( model , y_predict ): f1 = mean ( f1_score ( y_test , y_predict , average = None )) precision = mean ( precision_score ( y_test , y_predict , average = None )) recall = mean ( recall_score ( y_test , y_predict , average = None )) confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) return acc , f1 , precision , recall , confusion def mlflow_logs ( n_estimators , acc , f1 , precision , recall ): log_param ( \"n_estimators\" , n_estimators ) log_metric ( \"f1\" , f1 ) log_metric ( \"precision\" , precision ) log_metric ( \"recall\" , recall ) log_artifacts ( \"logs\" ) if __name__ == \"__main__\" : # start mlflow mlflow . set_tracking_uri ( \"http://127.0.0.1:5000\" ) mlflow . set_experiment ( \"iris\" ) mlflow . start_run ( run_name = \"test run\" ) n_estimators = 99 # train, predict, evaluate model = train ( n_estimators ) y_predict = model . predict ( X_test ) acc , f1 , precision , recall , confusion = evaluation ( model , y_predict ) # mlflow logging mlflow_logs ( n_estimators , acc , f1 , precision , recall ) signature = infer_signature ( X_train , y_predict ) mlflow . sklearn . log_model ( sk_model = model , artifact_path = \"artifacts\" , registered_model_name = \"iris-randomforest-classifier\" , signature = signature )","title":"Python Code"},{"location":"mlflow/#mlflow-ui","text":"This is the interface seen when MLFlow launches. Main UI Clicking on a specific run leads to its details. Run Details And any artifacts stored. This can include any file, including images. Artifacts Or the model. Model artifact & signature If we registered the model, it will be recorded in the model registry. Model registry","title":"MLFlow UI"},{"location":"mlops/","text":"MLOps A machine learning lifecycle doesn't follow the typical software development lifecycle, though the essential parts of automation , traceability , and monitoring are still key to its success. This is where a modified form of CI/CD for machine learning comes in, i.e. MLOps. Maturity Model A maturity model is a tool that helps people assess the current effectiveness of a person or group and supports figuring out what capabilities they need to acquire next in order to improve their performance. We can use it as a metric for establishing the progressive requirements needed to measure the maturity of a machine learning production environment and its associated processes. Both Google and Azure provide a good detail description of an MLOps maturity model. I have taken part of Azure's and summarised it below. - 1: NO MLOps 2: DevOps, No MLOps 3: Auto Training 4: Auto Deployment 5: Auto Retraining Highlights Entire lifecycle is manual Automated app builds and unit-tests Training is centrally managed & traceable Full traceability from deployment back to original data Full system automation and monitoring","title":"MLOps"},{"location":"mlops/#mlops","text":"A machine learning lifecycle doesn't follow the typical software development lifecycle, though the essential parts of automation , traceability , and monitoring are still key to its success. This is where a modified form of CI/CD for machine learning comes in, i.e. MLOps.","title":"MLOps"},{"location":"mlops/#maturity-model","text":"A maturity model is a tool that helps people assess the current effectiveness of a person or group and supports figuring out what capabilities they need to acquire next in order to improve their performance. We can use it as a metric for establishing the progressive requirements needed to measure the maturity of a machine learning production environment and its associated processes. Both Google and Azure provide a good detail description of an MLOps maturity model. I have taken part of Azure's and summarised it below. - 1: NO MLOps 2: DevOps, No MLOps 3: Auto Training 4: Auto Deployment 5: Auto Retraining Highlights Entire lifecycle is manual Automated app builds and unit-tests Training is centrally managed & traceable Full traceability from deployment back to original data Full system automation and monitoring","title":"Maturity Model"},{"location":"monitoring/","text":"Monitoring After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source Performance Metrics - Latency - Memory Footprint Input Data - Drift in distribution of features - Kolmogorov-Smirnov (K-S) test - aka Data Drift Model - aka Concept / Target Drift Libraries Both scikit-multiflow & River have a list of algorithms for detecting concept drifts. Monitoring Systems Prometheus Prometheus is the go-to open source metrics monitoring platform for ML systems. Grafana Grafana can be integrated with Prometheus to display the metrics in interactive visualizations. Kibana Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. This helps to detect concept or data drifts. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use. Evidently Evidently helps evaluate and monitor machine learning models in production. It generates interactive reports or JSON profiles from pandas DataFramesor csv files. You can use visual reports for ad hoc analysis, debugging and team sharing, and JSON profiles to integrate Evidently in prediction pipelines or with other visualization tools.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source Performance Metrics - Latency - Memory Footprint Input Data - Drift in distribution of features - Kolmogorov-Smirnov (K-S) test - aka Data Drift Model - aka Concept / Target Drift","title":"Monitoring"},{"location":"monitoring/#libraries","text":"Both scikit-multiflow & River have a list of algorithms for detecting concept drifts.","title":"Libraries"},{"location":"monitoring/#monitoring-systems","text":"","title":"Monitoring Systems"},{"location":"monitoring/#prometheus","text":"Prometheus is the go-to open source metrics monitoring platform for ML systems.","title":"Prometheus"},{"location":"monitoring/#grafana","text":"Grafana can be integrated with Prometheus to display the metrics in interactive visualizations.","title":"Grafana"},{"location":"monitoring/#kibana","text":"Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. This helps to detect concept or data drifts. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use.","title":"Kibana"},{"location":"monitoring/#evidently","text":"Evidently helps evaluate and monitor machine learning models in production. It generates interactive reports or JSON profiles from pandas DataFramesor csv files. You can use visual reports for ad hoc analysis, debugging and team sharing, and JSON profiles to integrate Evidently in prediction pipelines or with other visualization tools.","title":"Evidently"},{"location":"opt-data/","text":"Data Structures Pandas Pandas documentation lists ways to enhance the performances in terms of both query and storage. Storage & Memory We can reduce the memory required to store the dataframes if we reduce the datatype to its minimal or appropriate ones. This can also make faster queries if categorical dtypes are assigned properly. def reduce_df ( df , cat_col = [], cat_percent = 0.5 , verbose = True ): \"\"\"\"maximium data memory optimization Args: cat_col (list): list of columm names to convert to category dtype cat_percent (float): only convert if percentage duplicates more than stated verbose (bool): prints reduction percentage \"\"\" mem_start = df . memory_usage () . sum () for col in df . columns : dtype_ = df [ col ] . dtype if dtype_ == float : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'float' ) elif dtype_ == int : min_ = df [ col ] . min () if min_ > 0 : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'unsigned' ) else : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'integer' ) elif dtype_ == object : if col in cat_col : dup_percent = 1 - ( len ( df [ col ] . unique ()) / size ) if dup_percent >= cat_percent : df [ col ] = df [ col ] . astype ( 'category' ) if verbose : mem_end = df . memory_usage () . sum () print ( \" {:.2f}% o f original size is reduced\" . format (( mem_start - mem_end ) / mem_start * 100 )) return df Looping As a rule of thumb, never use crude for loops to iterate and change values in the dataframe. The apply method is already internally optimized, like using iterators in Cython. We can also used vectorization on pandas series or numpy arrays as suggested by this author . import numpy as np import pandas as pd # Define a basic Haversine distance formula def haversine ( lat1 , lon1 , lat2 , lon2 ): MILES = 3959 lat1 , lon1 , lat2 , lon2 = map ( np . deg2rad , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a )) total_miles = MILES * c return total_miles # optimized on apply-lambda df [ 'distance' ] = df . apply ( lambda row : haversine ( 40.671 , - 73.985 , row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) # optimized pandas series df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ], df [ 'longitude' ]) # optimized numpy array df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ] . values , df [ 'longitude' ] . values ) Querying In pandas, there are various ways of querying, and the speed various with each type. In the example below, the fastest way is to convert the query column into a category data-type as it contains many duplicates, hence conversion into integers underthehood make it query more efficient. However, this might not always be the case, and using isin or in1d might suffice. More information here . # Unoptimized, 0.01288s result = df [ df [ \"antecedents\" ] == item ] # Optimized Queries # ------------------- # with pandas .query, 0.00722s, x1.8 result = df . query ( \"antecedents == @item\" ) # with isin, 0.00440s, x2.9 result = df [ df [ \"antecedents\" ] . isin ([ item ])] # with indexing, 0.004342s x3.0 df = df . set_index ( \"antecedents\" ) result = df [ df . index . isin ([ item ])] # with numpy, 0.004175s, x3.1 result = df [ np . in1d ( df [ \"antecedents\" ] . values , [ item ])] # using category datatype, 0.000608s, x211.8 df [ \"antecedents\" ] = df [ \"antecedents\" ] . astype ( 'category' ) result = df [ df [ \"antecedents\" ] == item ] Graph A graph data structure enables much faster query if there is a network of interconnectivity between each other, known as nodes . Information stored between each connection of two nodes are called links . A simple graph structure in python can be created using the networkx library. However, this does not scale when the data become huge. For that, a graph database like neo4j is more appropriate. import networkx as nx import pandas as pd df = pd . read_csv ( \"association_rules.csv\" ) # convert dataframe to networkx graph graph = nx . convert_matrix . from_pandas_edgelist ( df , \"antecedents\" , \"consequents\" , [ \"antecedent support\" , \"consequent support\" , \"support\" , \"confidence\" , \"lift\" , \"leverage\" , \"conviction\" , ], create_using = nx . DiGraph , ) # query result = list ( graph [ item ])","title":"Data"},{"location":"opt-data/#data-structures","text":"","title":"Data Structures"},{"location":"opt-data/#pandas","text":"Pandas documentation lists ways to enhance the performances in terms of both query and storage.","title":"Pandas"},{"location":"opt-data/#storage-memory","text":"We can reduce the memory required to store the dataframes if we reduce the datatype to its minimal or appropriate ones. This can also make faster queries if categorical dtypes are assigned properly. def reduce_df ( df , cat_col = [], cat_percent = 0.5 , verbose = True ): \"\"\"\"maximium data memory optimization Args: cat_col (list): list of columm names to convert to category dtype cat_percent (float): only convert if percentage duplicates more than stated verbose (bool): prints reduction percentage \"\"\" mem_start = df . memory_usage () . sum () for col in df . columns : dtype_ = df [ col ] . dtype if dtype_ == float : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'float' ) elif dtype_ == int : min_ = df [ col ] . min () if min_ > 0 : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'unsigned' ) else : df [ col ] = pd . to_numeric ( df [ col ], downcast = 'integer' ) elif dtype_ == object : if col in cat_col : dup_percent = 1 - ( len ( df [ col ] . unique ()) / size ) if dup_percent >= cat_percent : df [ col ] = df [ col ] . astype ( 'category' ) if verbose : mem_end = df . memory_usage () . sum () print ( \" {:.2f}% o f original size is reduced\" . format (( mem_start - mem_end ) / mem_start * 100 )) return df","title":"Storage &amp; Memory"},{"location":"opt-data/#looping","text":"As a rule of thumb, never use crude for loops to iterate and change values in the dataframe. The apply method is already internally optimized, like using iterators in Cython. We can also used vectorization on pandas series or numpy arrays as suggested by this author . import numpy as np import pandas as pd # Define a basic Haversine distance formula def haversine ( lat1 , lon1 , lat2 , lon2 ): MILES = 3959 lat1 , lon1 , lat2 , lon2 = map ( np . deg2rad , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a )) total_miles = MILES * c return total_miles # optimized on apply-lambda df [ 'distance' ] = df . apply ( lambda row : haversine ( 40.671 , - 73.985 , row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) # optimized pandas series df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ], df [ 'longitude' ]) # optimized numpy array df [ 'distance' ] = haversine ( 40.671 , - 73.985 , df [ 'latitude' ] . values , df [ 'longitude' ] . values )","title":"Looping"},{"location":"opt-data/#querying","text":"In pandas, there are various ways of querying, and the speed various with each type. In the example below, the fastest way is to convert the query column into a category data-type as it contains many duplicates, hence conversion into integers underthehood make it query more efficient. However, this might not always be the case, and using isin or in1d might suffice. More information here . # Unoptimized, 0.01288s result = df [ df [ \"antecedents\" ] == item ] # Optimized Queries # ------------------- # with pandas .query, 0.00722s, x1.8 result = df . query ( \"antecedents == @item\" ) # with isin, 0.00440s, x2.9 result = df [ df [ \"antecedents\" ] . isin ([ item ])] # with indexing, 0.004342s x3.0 df = df . set_index ( \"antecedents\" ) result = df [ df . index . isin ([ item ])] # with numpy, 0.004175s, x3.1 result = df [ np . in1d ( df [ \"antecedents\" ] . values , [ item ])] # using category datatype, 0.000608s, x211.8 df [ \"antecedents\" ] = df [ \"antecedents\" ] . astype ( 'category' ) result = df [ df [ \"antecedents\" ] == item ]","title":"Querying"},{"location":"opt-data/#graph","text":"A graph data structure enables much faster query if there is a network of interconnectivity between each other, known as nodes . Information stored between each connection of two nodes are called links . A simple graph structure in python can be created using the networkx library. However, this does not scale when the data become huge. For that, a graph database like neo4j is more appropriate. import networkx as nx import pandas as pd df = pd . read_csv ( \"association_rules.csv\" ) # convert dataframe to networkx graph graph = nx . convert_matrix . from_pandas_edgelist ( df , \"antecedents\" , \"consequents\" , [ \"antecedent support\" , \"consequent support\" , \"support\" , \"confidence\" , \"lift\" , \"leverage\" , \"conviction\" , ], create_using = nx . DiGraph , ) # query result = list ( graph [ item ])","title":"Graph"},{"location":"opt-model/","text":"Model Optimization There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision. We will concentrate on a few optimization techniques namely: Quantization CPU-GPU Flow TensorRT On the 2 popular libraries, pytorch & tensorflow. Keras is part of tensorflow's API already, so whatever tensorflow can do, keras should be able to too. Quantization Pytorch docs CPU Flow Control Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . import tensorflow as tf import numpy as np image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) For pytorch we can use the torch.autograd.profiler . import torch import torchvision.models as models import torch.autograd.profiler as profiler model = models . resnet18 () . cuda () inputs = torch . randn ( 5 , 3 , 224 , 224 ) . cuda () with profiler . profile ( record_shapes = True ) as prof : with profiler . record_function ( \"model_inference\" ): model ( inputs ) prof . export_chrome_trace ( \"trace.json\" ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0' References Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace Pytorch profiler Tensorflow profiler Tensorflow Model Optimization Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization . TensorRT TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below. Layer Fusion During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT Quantization Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. Save Model An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory> Benchmark Functions To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model { input_saved_model_dir } ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds TRT Conversion Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory> Others There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO) References Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"Model"},{"location":"opt-model/#model-optimization","text":"There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision. We will concentrate on a few optimization techniques namely: Quantization CPU-GPU Flow TensorRT On the 2 popular libraries, pytorch & tensorflow. Keras is part of tensorflow's API already, so whatever tensorflow can do, keras should be able to too.","title":"Model Optimization"},{"location":"opt-model/#quantization","text":"Pytorch docs","title":"Quantization"},{"location":"opt-model/#cpu-flow-control","text":"Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . import tensorflow as tf import numpy as np image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) For pytorch we can use the torch.autograd.profiler . import torch import torchvision.models as models import torch.autograd.profiler as profiler model = models . resnet18 () . cuda () inputs = torch . randn ( 5 , 3 , 224 , 224 ) . cuda () with profiler . profile ( record_shapes = True ) as prof : with profiler . record_function ( \"model_inference\" ): model ( inputs ) prof . export_chrome_trace ( \"trace.json\" ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0'","title":"CPU Flow Control"},{"location":"opt-model/#references","text":"Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace Pytorch profiler Tensorflow profiler","title":"References"},{"location":"opt-model/#tensorflow-model-optimization","text":"Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization .","title":"Tensorflow Model Optimization"},{"location":"opt-model/#tensorrt","text":"TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below.","title":"TensorRT"},{"location":"opt-model/#layer-fusion","text":"During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT","title":"Layer Fusion"},{"location":"opt-model/#quantization_1","text":"Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation.","title":"Quantization"},{"location":"opt-model/#save-model","text":"An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory>","title":"Save Model"},{"location":"opt-model/#benchmark-functions","text":"To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model { input_saved_model_dir } ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds","title":"Benchmark Functions"},{"location":"opt-model/#trt-conversion","text":"Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory>","title":"TRT Conversion"},{"location":"opt-model/#others","text":"There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO)","title":"Others"},{"location":"opt-model/#references_1","text":"Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"References"},{"location":"opt-para-concurr/","text":"Concurrency & Parallelism Parallel programming means executing operations while using multiple CPU's processes (cores) or using multiple threads in a process. This speeds up the operation multitudes faster based on the number of processes or threads launched. Python's GIL problem Python has a Global Interpreter Lock (GIL), which prevent two threads from executing simultaneously in the same program. However, libraries like numpy bypass this limitation by running external code in C. Because of this GIL limitation, threads provide no benefit for CPU intensive tasks , and is only used for Input/Ouput (IO) tasks . CPU-limiting processes refer to one where most part of the life is spent in cpu, while IO-limiting processes refer to one where most part of the life is spent in i/o state, i.e. instructing another program to run something. Why does IO task work when it can also only use a single thread in a process at a time? Let's take an example, one thread fires off a request to a URL and while it is waiting for a response, that thread can be swapped out for another thread that fires another request to another URL. Since a thread doesn\u2019t have to do anything until it receives a response, it doesn\u2019t really matter that only one thread is executing at a given time. This makes multi-threading in Python as a concurrent or asychronous process. Given all these, we know when to use multi-processes and threads to run tasks with improved latency. Multiprocess Multithread more overhead than threads as opening and closing processes takes more time fast as they share memory space and efficiently read and write to the same variables only for CPU-intensive tasks because of overheads only for IO-intensive tasks because fo GIL E.g. complex calculations E.g. networking, file read-write, database query True Parallel Process Concurrent Process Great detailed explanations are provided by Brendan Fortuner , Thilina Rajapakse , stackoverflow , and testdrive.io 1 & 2 . Examples Below are two examples of embarassingly parallel tasks, one a CPU-bound job... def compute_x ( x , y = 2 ): from time import sleep sleep ( 0.8 ) return ( x , y ) list_to_iterate = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] sum_all_x = [ compute_x ( i ) for i in list_to_iterate ] and an IO-bound job where it is requesting data from a website. import requests wiki_page_urls = [ \"https://en.wikipedia.org/wiki/Ocean\" , \"https://en.wikipedia.org/wiki/Island\" , \"https://en.wikipedia.org/wiki/this_page_does_not_exist\" , \"https://en.wikipedia.org/wiki/Shark\" , ] def get_wiki_page_existence ( wiki_page_url , timeout = 10 ): response = requests . get ( url = wiki_page_url , timeout = timeout ) page_status = \"unknown\" if response . status_code == 200 : page_status = \"exists\" elif response . status_code == 404 : page_status = \"does not exist\" return wiki_page_url + \" - \" + page_status Joblib Joblib makes embarrassingly parallel for loops written in a list comprehension very simple, with the following syntax. from joblib import Parallel , delayed variable_name = Parallel ( n_jobs =< no_of_jobs > )( delayed ( < func_name > )( < arg1 > , < arg2 > , < arg3 > ) for i in < list_to_iterate > ) Parallel We can run this in parallel with multiprocessing in joblib as this. By default it uses the backend loky , which is more robust than multiprocessing , taken from the older multiprocessing.Pool library. However, it might not always be faster. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 )( delayed ( compute_x )( i ) for i in list_to_iterate ) Concurrent However, if this is an I/O intensive task, using threading to run it asynchronously is preferred. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 , backend = \"threading\" )( delayed ( compute_x )( i ) for i in list_to_iterate ) Concurrent Futures concurrent.futures is an in-built library that can easily run multi-processing or threading tasks. Parallel from concurrent.futures import ProcessPoolExecutor , as_completed def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] for i in list_to_iterate : executor . submit ( compute_x , x = i ) result = [ future . result () for future in as_completed ( futures )] return result # alternatively, we can use the map function for a more concise result # however, it does not allow multiple args to the worker function def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = executor . map ( compute_x , list_to_iterate ) result = [ future for future in futures ] return result # to do that, we need to use another library to wrap the worker function # and additional args together from functools import partial def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : wrap = partial ( compute_x , y = 3 ) futures = executor . map ( wrap , list_to_iterate ) result = [ future for future in futures ] return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate ) Concurrent from concurrent.futures import ThreadPoolExecutor , as_completed with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [] for url in wiki_page_urls : futures . append ( executor . submit ( get_wiki_page_existence , wiki_page_url = url )) result = [ future . result () for future in as_completed ( futures )] Multiprocessing The multiprocessing module is another in-built library that supports both multi-processing multiprocessing.Pool or threading multiprocessing.pool.ThreadPool tasks. Parallel import multiprocessing as mp def multiproc ( list_to_iterate , workers = 2 ): pool = mp . Pool ( workers ) result = pool . map ( compute_x , list_to_iterate ) return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate ) Concurrent import multiprocessing as mp def multithread ( list_to_iterate , workers = 2 ): pool = mp . pool . ThreadPool ( workers ) result = pool . map ( get_wiki_page_existence , wiki_page_urls ) return result if __name__ == \"__main__\" : result = multithread ( list_to_iterate ) Threading Library The python default threading library provides a simple way to spin off a thread for an I/O bound task. import threading threading . Thread ( target =< function > ) . start () Numba Numba is one of the unique cases where has true parallel multi-threading, being able to overcome the GIL.","title":"Concurrency & Parallelism"},{"location":"opt-para-concurr/#concurrency-parallelism","text":"Parallel programming means executing operations while using multiple CPU's processes (cores) or using multiple threads in a process. This speeds up the operation multitudes faster based on the number of processes or threads launched.","title":"Concurrency &amp; Parallelism"},{"location":"opt-para-concurr/#pythons-gil-problem","text":"Python has a Global Interpreter Lock (GIL), which prevent two threads from executing simultaneously in the same program. However, libraries like numpy bypass this limitation by running external code in C. Because of this GIL limitation, threads provide no benefit for CPU intensive tasks , and is only used for Input/Ouput (IO) tasks . CPU-limiting processes refer to one where most part of the life is spent in cpu, while IO-limiting processes refer to one where most part of the life is spent in i/o state, i.e. instructing another program to run something. Why does IO task work when it can also only use a single thread in a process at a time? Let's take an example, one thread fires off a request to a URL and while it is waiting for a response, that thread can be swapped out for another thread that fires another request to another URL. Since a thread doesn\u2019t have to do anything until it receives a response, it doesn\u2019t really matter that only one thread is executing at a given time. This makes multi-threading in Python as a concurrent or asychronous process. Given all these, we know when to use multi-processes and threads to run tasks with improved latency. Multiprocess Multithread more overhead than threads as opening and closing processes takes more time fast as they share memory space and efficiently read and write to the same variables only for CPU-intensive tasks because of overheads only for IO-intensive tasks because fo GIL E.g. complex calculations E.g. networking, file read-write, database query True Parallel Process Concurrent Process Great detailed explanations are provided by Brendan Fortuner , Thilina Rajapakse , stackoverflow , and testdrive.io 1 & 2 .","title":"Python's GIL problem"},{"location":"opt-para-concurr/#examples","text":"Below are two examples of embarassingly parallel tasks, one a CPU-bound job... def compute_x ( x , y = 2 ): from time import sleep sleep ( 0.8 ) return ( x , y ) list_to_iterate = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] sum_all_x = [ compute_x ( i ) for i in list_to_iterate ] and an IO-bound job where it is requesting data from a website. import requests wiki_page_urls = [ \"https://en.wikipedia.org/wiki/Ocean\" , \"https://en.wikipedia.org/wiki/Island\" , \"https://en.wikipedia.org/wiki/this_page_does_not_exist\" , \"https://en.wikipedia.org/wiki/Shark\" , ] def get_wiki_page_existence ( wiki_page_url , timeout = 10 ): response = requests . get ( url = wiki_page_url , timeout = timeout ) page_status = \"unknown\" if response . status_code == 200 : page_status = \"exists\" elif response . status_code == 404 : page_status = \"does not exist\" return wiki_page_url + \" - \" + page_status","title":"Examples"},{"location":"opt-para-concurr/#joblib","text":"Joblib makes embarrassingly parallel for loops written in a list comprehension very simple, with the following syntax. from joblib import Parallel , delayed variable_name = Parallel ( n_jobs =< no_of_jobs > )( delayed ( < func_name > )( < arg1 > , < arg2 > , < arg3 > ) for i in < list_to_iterate > )","title":"Joblib"},{"location":"opt-para-concurr/#parallel","text":"We can run this in parallel with multiprocessing in joblib as this. By default it uses the backend loky , which is more robust than multiprocessing , taken from the older multiprocessing.Pool library. However, it might not always be faster. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 )( delayed ( compute_x )( i ) for i in list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent","text":"However, if this is an I/O intensive task, using threading to run it asynchronously is preferred. from joblib import Parallel , delayed sum_all_x = Parallel ( n_jobs = 4 , backend = \"threading\" )( delayed ( compute_x )( i ) for i in list_to_iterate )","title":"Concurrent"},{"location":"opt-para-concurr/#concurrent-futures","text":"concurrent.futures is an in-built library that can easily run multi-processing or threading tasks.","title":"Concurrent Futures"},{"location":"opt-para-concurr/#parallel_1","text":"from concurrent.futures import ProcessPoolExecutor , as_completed def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] for i in list_to_iterate : executor . submit ( compute_x , x = i ) result = [ future . result () for future in as_completed ( futures )] return result # alternatively, we can use the map function for a more concise result # however, it does not allow multiple args to the worker function def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = executor . map ( compute_x , list_to_iterate ) result = [ future for future in futures ] return result # to do that, we need to use another library to wrap the worker function # and additional args together from functools import partial def multiproc ( list_to_iterate , workers = 4 ): with ProcessPoolExecutor ( max_workers = workers ) as executor : wrap = partial ( compute_x , y = 3 ) futures = executor . map ( wrap , list_to_iterate ) result = [ future for future in futures ] return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent_1","text":"from concurrent.futures import ThreadPoolExecutor , as_completed with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [] for url in wiki_page_urls : futures . append ( executor . submit ( get_wiki_page_existence , wiki_page_url = url )) result = [ future . result () for future in as_completed ( futures )]","title":"Concurrent"},{"location":"opt-para-concurr/#multiprocessing","text":"The multiprocessing module is another in-built library that supports both multi-processing multiprocessing.Pool or threading multiprocessing.pool.ThreadPool tasks.","title":"Multiprocessing"},{"location":"opt-para-concurr/#parallel_2","text":"import multiprocessing as mp def multiproc ( list_to_iterate , workers = 2 ): pool = mp . Pool ( workers ) result = pool . map ( compute_x , list_to_iterate ) return result if __name__ == \"__main__\" : result = multiproc ( list_to_iterate )","title":"Parallel"},{"location":"opt-para-concurr/#concurrent_2","text":"import multiprocessing as mp def multithread ( list_to_iterate , workers = 2 ): pool = mp . pool . ThreadPool ( workers ) result = pool . map ( get_wiki_page_existence , wiki_page_urls ) return result if __name__ == \"__main__\" : result = multithread ( list_to_iterate )","title":"Concurrent"},{"location":"opt-para-concurr/#threading-library","text":"The python default threading library provides a simple way to spin off a thread for an I/O bound task. import threading threading . Thread ( target =< function > ) . start ()","title":"Threading Library"},{"location":"opt-para-concurr/#numba","text":"Numba is one of the unique cases where has true parallel multi-threading, being able to overcome the GIL.","title":"Numba"},{"location":"opt-spark/","text":"Spark Launching Spark The SparkConf object sets the configuration for the Spark Application. The SparkContext is the entry point of Spark functionality. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The SparkSession is the entry point into the Structured API. from pyspark import SparkContext , SparkConf from pyspark.sql import * from pyspark.sql.functions import * from pyspark.sql.types import * # set master can increase the cores cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) Read Files # from database cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) cnfg . set ( \"spark.jars\" , \"D: \\\\ mysql-connector-java-5.1.49.jar\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) df = ( spark . read . format ( \"jdbc\" ) . options ( url = \"jdbc:mysql://localhost/videoshop\" , driver = \"com.mysql.jdbc.Driver\" , dbtable = \"SomeTableName\" , user = \"venkat\" , password = \"P@ssw0rd1\" ) . load ()) # read csv df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( filepath )) # read json df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . json ( filepath )) We can also define a schema for a particular data. custschema = StructType ([ StructField ( \"Customerid\" , IntegerType (), True ), StructField ( \"CustName\" , StringType (), True ), StructField ( \"MemCat\" , StringType (), True ), StructField ( \"Age\" , IntegerType (), True ), StructField ( \"Gender\" , StringType (), True ), StructField ( \"AmtSpent\" , DoubleType (), True ), StructField ( \"Address\" , StringType (), True ), StructField ( \"City\" , StringType (), True ), StructField ( \"CountryID\" , StringType (), True ), StructField ( \"Title\" , StringType (), True ), StructField ( \"PhoneNo\" , StringType (), True ) ]) df . printSchema () df = ( spark . read . schema ( schema = custschema ) . csv ( inputFilePath )) df . show () Write Files Note that we write files to a folder. The file name is system generated, e.g. \"part-00000-2ee2d8b6-169a-4f48-a503-8b6a2fdedab0-c000.json\". df . write . json ( \"D: \\\\ CountryOUT\" ) Basic Summary # bool for col truncation df . show ( 20 , False ) # schema df . printSchema () # statistics df . describe ( \"MemberCategory\" , \"Gender\" , \"AmountSpent\" , \"Age\" ) . show () Spark SQL The Spark SQL API works in structure similar to SQL. This is used when the data format is well structured, and presentable in a tabular format. Spark SQL is a high level API compared to RDD, therefore is easy to use. # simple query df2 = df . orderBy ( \"Age\" ) . where ( \"Age>20\" ) . select ( df [ \"CustomerID\" ], df [ \"CustomerName\" ], df [ \"Age\" ]) df2 . show ( 200 , False ) # aggregate (single value) tot = df . agg ( sum ( \"AmountSpent\" )) . first ()[ 0 ] tot = df . agg ( avg ( \"Age\" )) . first ()[ 0 ] std = df . agg ( stddev_pop ( \"AmountSpent\" )) . first ()[ 0 ] skw = df . agg ( skewness ( \"AmountSpent\" )) . first ()[ 0 ] # group by df . groupBy ( \"MemberCategory\" ) . sum ( \"AmountSpent\" ) . show ( 200 , False ) We can also do joins from multiple dataframes. customerFilePath = r \"D:\\workspace\\Customer.csv\" countryFilePath = r \"D:\\workspace\\Country.csv\" dfCustomer = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( customerFilePath ) ) dfCountry = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( countryFilePath ) ) joinDF = dfCustomer . join ( dfCountry , \"CountryCode\" ) ( joinDF . select ( \"CustomerID\" , \"CustomerName\" , \"CountryCode\" , \"CountryName\" , \"Currency\" , \"TimeZone\" ) . show ( 300 , False ) ) ) RDD Resilient Distributed Dataset","title":"Spark"},{"location":"opt-spark/#spark","text":"","title":"Spark"},{"location":"opt-spark/#launching-spark","text":"The SparkConf object sets the configuration for the Spark Application. The SparkContext is the entry point of Spark functionality. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The SparkSession is the entry point into the Structured API. from pyspark import SparkContext , SparkConf from pyspark.sql import * from pyspark.sql.functions import * from pyspark.sql.types import * # set master can increase the cores cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc )","title":"Launching Spark"},{"location":"opt-spark/#read-files","text":"# from database cnfg = SparkConf () . setAppName ( \"CustomerApplication\" ) . setMaster ( \"local[2]\" ) cnfg . set ( \"spark.jars\" , \"D: \\\\ mysql-connector-java-5.1.49.jar\" ) sc = SparkContext ( conf = cnfg ) spark = SparkSession ( sc ) df = ( spark . read . format ( \"jdbc\" ) . options ( url = \"jdbc:mysql://localhost/videoshop\" , driver = \"com.mysql.jdbc.Driver\" , dbtable = \"SomeTableName\" , user = \"venkat\" , password = \"P@ssw0rd1\" ) . load ()) # read csv df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( filepath )) # read json df = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . json ( filepath )) We can also define a schema for a particular data. custschema = StructType ([ StructField ( \"Customerid\" , IntegerType (), True ), StructField ( \"CustName\" , StringType (), True ), StructField ( \"MemCat\" , StringType (), True ), StructField ( \"Age\" , IntegerType (), True ), StructField ( \"Gender\" , StringType (), True ), StructField ( \"AmtSpent\" , DoubleType (), True ), StructField ( \"Address\" , StringType (), True ), StructField ( \"City\" , StringType (), True ), StructField ( \"CountryID\" , StringType (), True ), StructField ( \"Title\" , StringType (), True ), StructField ( \"PhoneNo\" , StringType (), True ) ]) df . printSchema () df = ( spark . read . schema ( schema = custschema ) . csv ( inputFilePath )) df . show ()","title":"Read Files"},{"location":"opt-spark/#write-files","text":"Note that we write files to a folder. The file name is system generated, e.g. \"part-00000-2ee2d8b6-169a-4f48-a503-8b6a2fdedab0-c000.json\". df . write . json ( \"D: \\\\ CountryOUT\" )","title":"Write Files"},{"location":"opt-spark/#basic-summary","text":"# bool for col truncation df . show ( 20 , False ) # schema df . printSchema () # statistics df . describe ( \"MemberCategory\" , \"Gender\" , \"AmountSpent\" , \"Age\" ) . show ()","title":"Basic Summary"},{"location":"opt-spark/#spark-sql","text":"The Spark SQL API works in structure similar to SQL. This is used when the data format is well structured, and presentable in a tabular format. Spark SQL is a high level API compared to RDD, therefore is easy to use. # simple query df2 = df . orderBy ( \"Age\" ) . where ( \"Age>20\" ) . select ( df [ \"CustomerID\" ], df [ \"CustomerName\" ], df [ \"Age\" ]) df2 . show ( 200 , False ) # aggregate (single value) tot = df . agg ( sum ( \"AmountSpent\" )) . first ()[ 0 ] tot = df . agg ( avg ( \"Age\" )) . first ()[ 0 ] std = df . agg ( stddev_pop ( \"AmountSpent\" )) . first ()[ 0 ] skw = df . agg ( skewness ( \"AmountSpent\" )) . first ()[ 0 ] # group by df . groupBy ( \"MemberCategory\" ) . sum ( \"AmountSpent\" ) . show ( 200 , False ) We can also do joins from multiple dataframes. customerFilePath = r \"D:\\workspace\\Customer.csv\" countryFilePath = r \"D:\\workspace\\Country.csv\" dfCustomer = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( customerFilePath ) ) dfCountry = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . csv ( countryFilePath ) ) joinDF = dfCustomer . join ( dfCountry , \"CountryCode\" ) ( joinDF . select ( \"CustomerID\" , \"CustomerName\" , \"CountryCode\" , \"CountryName\" , \"Currency\" , \"TimeZone\" ) . show ( 300 , False ) ) )","title":"Spark SQL"},{"location":"opt-spark/#rdd","text":"Resilient Distributed Dataset","title":"RDD"},{"location":"security/","text":"Security Scans If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline. Note that a lot of such scans are now available as plugins if you are running in a CI/CD environment, e.g. Gitlab-CI or github workflow. Source Code Scan Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' ) Dependency Scan This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels, nor does it scan for libraries which have their own open-source dependencies. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json Secrets Scan detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/* License Scan Even if we are using open-sourced libraries, we must be aware of the licensing of each library, especially if we are using them for commercial purposes. There are various scanners out there that can help us to compile the license and if you are able to use or modify them freely. IaC Scan Infrastructure as Code (IaC) Scanning scans your IaC configuration files for known vulnerabilities. Kics , an open-source library, supports scanning of configuration files for Terraform, Ansible, Docker, AWS CloudFormation, Kubernetes, etc. Pre-Commit Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version: python: python3 repos: - repo: https://github.com/PyCQA/bandit rev: 1.7 hooks: - id: bandit args: [-lll] - repo: https://github.com/Yelp/detect-secrets rev: v0.13.0 hooks: - id: detect-secrets args: [--no-base64-string-scan] Another alternative which I prefer is using tox to compile all the testing and security scans together in a single file. See my section on testing for more. Synk Advisor Synk Advisor is a great site to check the security of open-source libraries. Besides checking using known security vulnerabilities as a metric, it also establishes an overall health score using the library's popularity, maintenance, and community.","title":"Security"},{"location":"security/#security-scans","text":"If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline. Note that a lot of such scans are now available as plugins if you are running in a CI/CD environment, e.g. Gitlab-CI or github workflow.","title":"Security Scans"},{"location":"security/#source-code-scan","text":"Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' )","title":"Source Code Scan"},{"location":"security/#dependency-scan","text":"This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels, nor does it scan for libraries which have their own open-source dependencies. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json","title":"Dependency Scan"},{"location":"security/#secrets-scan","text":"detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/*","title":"Secrets Scan"},{"location":"security/#license-scan","text":"Even if we are using open-sourced libraries, we must be aware of the licensing of each library, especially if we are using them for commercial purposes. There are various scanners out there that can help us to compile the license and if you are able to use or modify them freely.","title":"License Scan"},{"location":"security/#iac-scan","text":"Infrastructure as Code (IaC) Scanning scans your IaC configuration files for known vulnerabilities. Kics , an open-source library, supports scanning of configuration files for Terraform, Ansible, Docker, AWS CloudFormation, Kubernetes, etc.","title":"IaC Scan"},{"location":"security/#pre-commit","text":"Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version: python: python3 repos: - repo: https://github.com/PyCQA/bandit rev: 1.7 hooks: - id: bandit args: [-lll] - repo: https://github.com/Yelp/detect-secrets rev: v0.13.0 hooks: - id: detect-secrets args: [--no-base64-string-scan] Another alternative which I prefer is using tox to compile all the testing and security scans together in a single file. See my section on testing for more.","title":"Pre-Commit"},{"location":"security/#synk-advisor","text":"Synk Advisor is a great site to check the security of open-source libraries. Besides checking using known security vulnerabilities as a metric, it also establishes an overall health score using the library's popularity, maintenance, and community.","title":"Synk Advisor"},{"location":"testing-api-fuzz/","text":"API Fuzzing API Fuzzing uses your deployed API, together with your API documentation to test for bugs or vulnerabilities.","title":"API Fuzzing"},{"location":"testing-api-fuzz/#api-fuzzing","text":"API Fuzzing uses your deployed API, together with your API documentation to test for bugs or vulnerabilities.","title":"API Fuzzing"},{"location":"testing-api/","text":"API Tests Flask To conduct unit or integration tests in pytest, we can use Flask's test_client() method and call it as a fixture. import sys import pytest sys . path . append ( \"project\" ) import app @pytest . fixture ( scope = \"session\" ) def client (): app_ = app . app . test_client () yield app_ def test_health_check ( client ): \"\"\"status check\"\"\" response = client . get ( '/' ) assert response . status_code == 200 assert response . json == { \"status\" : \"ok\" } There are often functions within the request route, which we will need to mock. To do that, we can use the following. # project/server.py from flask import Flask , request from inference import predict app = Flask ( __name__ ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): content = request . json input_sku_list = content [ \"sku\" ] x = predict ( input_sku_list ) return { \"results\" : x } if __name__ == \"__main__\" : app . run () # tests/unit_tests/api_test.py import sys from unittest.mock import patch sys . path . append ( \"project\" ) import app @patch ( \"server.predict\" , return_value = 2 ) def test_api ( predict ): with app . app . test_client () as client : response = client . post ( '/predict' , json = { \"sku\" : \"abc\" }) . json assert response == { \"sku\" : 2 } Gunicorn We can also have a smoke test to ensure that gunicorn can be spun up successfully. import os import json import time import requests url = \"http://localhost:5001/ocr\" wait = 10 cmd = \"gunicorn -c config/gunicorn.conf.py app:app --workers=1 --daemon\" sample_request = { \"something\" : \"something\" } sample_response = { \"somethingelse\" : \"somethingelse\" } def test_gunicorn (): os . system ( f \"cd project; { cmd } \" ) time . sleep ( wait ) response = requests . post ( url , json = sample_request ) response = json . loads ( response . content ) assert response == sample_response os . system ( \"pkill gunicorn\" ) Pytest Docker API We can use pytest-docker plugin to do automated integrated tests for containerised APIs within pytest. What it does is to launch the container using docker-compose, and apply as a fixture so that all tests can have access to the container(s). It then shuts down the container(s) automatically after all tests are completed. The integration test code below generates a dummy docker-compose-test.yml , so do include this in .gitignore Note that we will have to add a dummy GET request to the root endpoint, where pytest-docker will ping at this line docker_services.wait_until_responsive . Test Code import os import json import pytest import requests import yaml from requests.exceptions import ConnectionError # ---------------- # Global Variables service_name = \"model-prediction\" container_port = 5000 endpoint = \"prediction\" docker_compose_name = \"docker-compose-api.yml\" dockerfile_name = \"Dockerfile-api\" model_source_path = os . environ [ \"SOURCE_MOUNT\" ] MODEL_FILE = \"model.pkl\" request = \\ { \"your_request\" : \"xxxx\" } # ----------------- # Utility Functions def gen_docker_compose_test (): \"\"\"change docker-compose.yml for testing\"\"\" with open ( docker_compose_name ) as f : list_doc = yaml . safe_load ( f ) # delete image url del list_doc [ \"services\" ][ service_name ][ \"image\" ] # add build folder & dockerfile name list_doc [ \"services\" ][ service_name ][ \"build\" ] = \\ { \"context\" : \"project/\" , \"dockerfile\" : dockerfile_name } # edit model source path folder list_doc [ \"services\" ][ service_name ][ \"volumes\" ][ 0 ][ \"source\" ] = \\ model_source_path # change model name in env list_doc [ \"services\" ][ service_name ][ \"environment\" ] = \\ [ 'WORKERS=max' , f 'MODEL_NAME= { MODEL_FILE } ' ] with open ( \"docker-compose-test.yml\" , \"w\" ) as f : yaml . dump ( list_doc , f ) def is_responsive ( url ): try : response = requests . get ( url ) if response . status_code == 200 : return True except ConnectionError : return False # ------------------------------------- # Setup Fixtures to Launch Container(s) @pytest . fixture ( scope = \"session\" ) def docker_compose_file ( pytestconfig ): \"\"\"set docker-compose*.yml file path\"\"\" gen_docker_compose_test () compose_filepath = os . path . join ( str ( pytestconfig . rootdir ), \"docker-compose-test.yml\" ) return compose_filepath @pytest . fixture ( scope = \"session\" ) def http_service ( docker_ip , docker_services ): \"\"\"ensure HTTP service is up and responsive\"\"\" host_port = docker_services . port_for ( service_name , container_port ) url = f \"http:// { docker_ip } : { host_port } \" docker_services . wait_until_responsive ( timeout = 60.0 , pause = 0.1 , check = lambda : is_responsive ( url ) ) return url @pytest . fixture ( scope = \"session\" ) def docker_cleanup (): \"\"\"remove images & containers, default only 'down -v'\"\"\" return \"down -v --rmi all\" # ------------------------------- # Integration Test Cases def test_sample_request ( http_service ): \"\"\"test sample request and returns more than 1 sku\"\"\" # http_service refers to previous fixture, returning the url url = f \" { http_service } / { endpoint } \" response = requests . post ( url , json = request ) content = json . loads ( response . content ) sku_count = len ( content [ \"sku\" ]) assert sku_count > 0 Another way to do the same thing without using the plugin is as follows. def docker_get_logs ( service_nm ): import subprocess logs = subprocess . check_output ( f \"docker logs { service_nm } \" , shell = True ) return logs CI Pipeline Below is an example Gitlab-CI job to run this integration test. There are various points to note: base image to use the stated which have all the essential libraries bundled. If we are mounting a file/model to the API, we need to use the SOURCE_MOUNT path indicated so that docker-in-docker service can access it If we are using tox, in tox.ini, we need to enable access to env variables from host os to access the SOURCE_MOUNT variable using this command passenv = * integration-tests : stage : test timeout : 10m image : sassy19a/dockercompose-py3-awscli2 services : - docker:dind variables : SOURCE_MOUNT : \"/builds/shared/$CI_PROJECT_PATH\" before_script : - pip3 install tox # download model file for mounting - aws s3 cp \"${S3_MODEL_DIRECTORY}/association/rules.pkl\" $SOURCE_MOUNT/rules.pkl script : - tox -e integration artifacts : when : always reports : junit : report.xml rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' changes : - project/*.py - project/Dockerfile-api Postman Postman allows us to write unit-test cases easily. This is done by creating a folder called collections from the left panel, and inside it, we can create individual requests, with a proper unit-test name for them. For each request, under the test tab, we need to define the condition to pass each test. There are some helper scripts on the right panel to assist you. For the collection, we can define variables which we can call in our tests with a double curly brackets {{variable_name}} . To run all the tests in the collection, click on your collection folder, and click Run at the top right corner. We can export the entire unit-tests script as a file json, and pass it to someone. Newman With the collections in json format, we can run that script in the commandline in our CI/CD pipeline, requiring installing npm & then newman, npm install newman . See more from Postman's website . Memory Leak Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker . Load Test Load testing is the process of determination of behavior of system when multiple users access it at the same time. Locust is a popular open source load testing library developed in python. We install it using pip install locust . And write the following script, saving it as locustfile.py . from locust import HttpUser , task , between import json with open ( 'test_data/sample_request.json' ) as f : request = json . loads ( f . read ()) class APIUser ( HttpUser ): host = 'http://localhost:5000' wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): apikey = \"yourapikey\" self . client . post ( '/predict' , headers = { \"x-api-key\" : apikey } json = request ) Another example using different requests. from locust import HttpUser , task , between import base64 host = \"https://something/api\" mode = [ \"keyvalue\" , \"barcode\" ] image_path1 = \"image_002.jpg\" image_path2 = \"image_000.jpg\" image_path3 = \"image_001.jpg\" def to_base64 ( image_path ): \"\"\"convert image to json request input\"\"\" image = open ( image_path , 'rb' ) base64_encoded = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) return base64_encoded request1 = { \"requestImage\" : to_base64 ( image_path1 ), \"mode\" : mode } request2 = { \"requestImage\" : to_base64 ( image_path2 ), \"mode\" : mode } request3 = { \"requestImage\" : to_base64 ( image_path3 ), \"mode\" : mode } class APIUser ( HttpUser ): wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): response = self . client . post ( '/api' , json = request1 ) response = self . client . post ( '/api' , json = request2 ) response = self . client . post ( '/api' , json = request3 ) We then launch it in the terminal using the command locust , and then the GUI can be access from localhost:8089 . We can test the total number of users as 10, with a spawn rate of 1. This means that the API will be launched with only 1 user requesting, and after every 3-5 seconds (the wait_time), another new user will be added, until the total of 10 users are reached. During the process of running, we can also change the number of users. Start screen Going to the statistics, a summary of the total users, failures and request per seconds Summary We can also have a visual representation in line charts. Charts For an overall report, with a more detailed statistics and response percentile levels with charts all within a single page, go to Download Data > Download Report to download a html version of the report. Report Other commands are as follows: locust -f locust_files/my_locust_file.py --headless -u 1000 -r 100 -run-time 1h30m CMD Desc -f specify file name & location if they are diff from default --headless only print in console, with no GUI -u no. users to spawn -r spawn rate; no users to add per second -run-time time limit for test","title":"API Testing"},{"location":"testing-api/#api-tests","text":"","title":"API Tests"},{"location":"testing-api/#flask","text":"To conduct unit or integration tests in pytest, we can use Flask's test_client() method and call it as a fixture. import sys import pytest sys . path . append ( \"project\" ) import app @pytest . fixture ( scope = \"session\" ) def client (): app_ = app . app . test_client () yield app_ def test_health_check ( client ): \"\"\"status check\"\"\" response = client . get ( '/' ) assert response . status_code == 200 assert response . json == { \"status\" : \"ok\" } There are often functions within the request route, which we will need to mock. To do that, we can use the following. # project/server.py from flask import Flask , request from inference import predict app = Flask ( __name__ ) @app . route ( \"/predict\" , methods = [ \"POST\" ]) def predict (): content = request . json input_sku_list = content [ \"sku\" ] x = predict ( input_sku_list ) return { \"results\" : x } if __name__ == \"__main__\" : app . run () # tests/unit_tests/api_test.py import sys from unittest.mock import patch sys . path . append ( \"project\" ) import app @patch ( \"server.predict\" , return_value = 2 ) def test_api ( predict ): with app . app . test_client () as client : response = client . post ( '/predict' , json = { \"sku\" : \"abc\" }) . json assert response == { \"sku\" : 2 }","title":"Flask"},{"location":"testing-api/#gunicorn","text":"We can also have a smoke test to ensure that gunicorn can be spun up successfully. import os import json import time import requests url = \"http://localhost:5001/ocr\" wait = 10 cmd = \"gunicorn -c config/gunicorn.conf.py app:app --workers=1 --daemon\" sample_request = { \"something\" : \"something\" } sample_response = { \"somethingelse\" : \"somethingelse\" } def test_gunicorn (): os . system ( f \"cd project; { cmd } \" ) time . sleep ( wait ) response = requests . post ( url , json = sample_request ) response = json . loads ( response . content ) assert response == sample_response os . system ( \"pkill gunicorn\" )","title":"Gunicorn"},{"location":"testing-api/#pytest-docker-api","text":"We can use pytest-docker plugin to do automated integrated tests for containerised APIs within pytest. What it does is to launch the container using docker-compose, and apply as a fixture so that all tests can have access to the container(s). It then shuts down the container(s) automatically after all tests are completed. The integration test code below generates a dummy docker-compose-test.yml , so do include this in .gitignore Note that we will have to add a dummy GET request to the root endpoint, where pytest-docker will ping at this line docker_services.wait_until_responsive .","title":"Pytest Docker API"},{"location":"testing-api/#test-code","text":"import os import json import pytest import requests import yaml from requests.exceptions import ConnectionError # ---------------- # Global Variables service_name = \"model-prediction\" container_port = 5000 endpoint = \"prediction\" docker_compose_name = \"docker-compose-api.yml\" dockerfile_name = \"Dockerfile-api\" model_source_path = os . environ [ \"SOURCE_MOUNT\" ] MODEL_FILE = \"model.pkl\" request = \\ { \"your_request\" : \"xxxx\" } # ----------------- # Utility Functions def gen_docker_compose_test (): \"\"\"change docker-compose.yml for testing\"\"\" with open ( docker_compose_name ) as f : list_doc = yaml . safe_load ( f ) # delete image url del list_doc [ \"services\" ][ service_name ][ \"image\" ] # add build folder & dockerfile name list_doc [ \"services\" ][ service_name ][ \"build\" ] = \\ { \"context\" : \"project/\" , \"dockerfile\" : dockerfile_name } # edit model source path folder list_doc [ \"services\" ][ service_name ][ \"volumes\" ][ 0 ][ \"source\" ] = \\ model_source_path # change model name in env list_doc [ \"services\" ][ service_name ][ \"environment\" ] = \\ [ 'WORKERS=max' , f 'MODEL_NAME= { MODEL_FILE } ' ] with open ( \"docker-compose-test.yml\" , \"w\" ) as f : yaml . dump ( list_doc , f ) def is_responsive ( url ): try : response = requests . get ( url ) if response . status_code == 200 : return True except ConnectionError : return False # ------------------------------------- # Setup Fixtures to Launch Container(s) @pytest . fixture ( scope = \"session\" ) def docker_compose_file ( pytestconfig ): \"\"\"set docker-compose*.yml file path\"\"\" gen_docker_compose_test () compose_filepath = os . path . join ( str ( pytestconfig . rootdir ), \"docker-compose-test.yml\" ) return compose_filepath @pytest . fixture ( scope = \"session\" ) def http_service ( docker_ip , docker_services ): \"\"\"ensure HTTP service is up and responsive\"\"\" host_port = docker_services . port_for ( service_name , container_port ) url = f \"http:// { docker_ip } : { host_port } \" docker_services . wait_until_responsive ( timeout = 60.0 , pause = 0.1 , check = lambda : is_responsive ( url ) ) return url @pytest . fixture ( scope = \"session\" ) def docker_cleanup (): \"\"\"remove images & containers, default only 'down -v'\"\"\" return \"down -v --rmi all\" # ------------------------------- # Integration Test Cases def test_sample_request ( http_service ): \"\"\"test sample request and returns more than 1 sku\"\"\" # http_service refers to previous fixture, returning the url url = f \" { http_service } / { endpoint } \" response = requests . post ( url , json = request ) content = json . loads ( response . content ) sku_count = len ( content [ \"sku\" ]) assert sku_count > 0 Another way to do the same thing without using the plugin is as follows. def docker_get_logs ( service_nm ): import subprocess logs = subprocess . check_output ( f \"docker logs { service_nm } \" , shell = True ) return logs","title":"Test Code"},{"location":"testing-api/#ci-pipeline","text":"Below is an example Gitlab-CI job to run this integration test. There are various points to note: base image to use the stated which have all the essential libraries bundled. If we are mounting a file/model to the API, we need to use the SOURCE_MOUNT path indicated so that docker-in-docker service can access it If we are using tox, in tox.ini, we need to enable access to env variables from host os to access the SOURCE_MOUNT variable using this command passenv = * integration-tests : stage : test timeout : 10m image : sassy19a/dockercompose-py3-awscli2 services : - docker:dind variables : SOURCE_MOUNT : \"/builds/shared/$CI_PROJECT_PATH\" before_script : - pip3 install tox # download model file for mounting - aws s3 cp \"${S3_MODEL_DIRECTORY}/association/rules.pkl\" $SOURCE_MOUNT/rules.pkl script : - tox -e integration artifacts : when : always reports : junit : report.xml rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' changes : - project/*.py - project/Dockerfile-api","title":"CI Pipeline"},{"location":"testing-api/#postman","text":"Postman allows us to write unit-test cases easily. This is done by creating a folder called collections from the left panel, and inside it, we can create individual requests, with a proper unit-test name for them. For each request, under the test tab, we need to define the condition to pass each test. There are some helper scripts on the right panel to assist you. For the collection, we can define variables which we can call in our tests with a double curly brackets {{variable_name}} . To run all the tests in the collection, click on your collection folder, and click Run at the top right corner. We can export the entire unit-tests script as a file json, and pass it to someone.","title":"Postman"},{"location":"testing-api/#newman","text":"With the collections in json format, we can run that script in the commandline in our CI/CD pipeline, requiring installing npm & then newman, npm install newman . See more from Postman's website .","title":"Newman"},{"location":"testing-api/#memory-leak","text":"Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker .","title":"Memory Leak"},{"location":"testing-api/#load-test","text":"Load testing is the process of determination of behavior of system when multiple users access it at the same time. Locust is a popular open source load testing library developed in python. We install it using pip install locust . And write the following script, saving it as locustfile.py . from locust import HttpUser , task , between import json with open ( 'test_data/sample_request.json' ) as f : request = json . loads ( f . read ()) class APIUser ( HttpUser ): host = 'http://localhost:5000' wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): apikey = \"yourapikey\" self . client . post ( '/predict' , headers = { \"x-api-key\" : apikey } json = request ) Another example using different requests. from locust import HttpUser , task , between import base64 host = \"https://something/api\" mode = [ \"keyvalue\" , \"barcode\" ] image_path1 = \"image_002.jpg\" image_path2 = \"image_000.jpg\" image_path3 = \"image_001.jpg\" def to_base64 ( image_path ): \"\"\"convert image to json request input\"\"\" image = open ( image_path , 'rb' ) base64_encoded = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) return base64_encoded request1 = { \"requestImage\" : to_base64 ( image_path1 ), \"mode\" : mode } request2 = { \"requestImage\" : to_base64 ( image_path2 ), \"mode\" : mode } request3 = { \"requestImage\" : to_base64 ( image_path3 ), \"mode\" : mode } class APIUser ( HttpUser ): wait_time = between ( 3 , 5 ) @task () def predict_endpoint ( self ): response = self . client . post ( '/api' , json = request1 ) response = self . client . post ( '/api' , json = request2 ) response = self . client . post ( '/api' , json = request3 ) We then launch it in the terminal using the command locust , and then the GUI can be access from localhost:8089 . We can test the total number of users as 10, with a spawn rate of 1. This means that the API will be launched with only 1 user requesting, and after every 3-5 seconds (the wait_time), another new user will be added, until the total of 10 users are reached. During the process of running, we can also change the number of users. Start screen Going to the statistics, a summary of the total users, failures and request per seconds Summary We can also have a visual representation in line charts. Charts For an overall report, with a more detailed statistics and response percentile levels with charts all within a single page, go to Download Data > Download Report to download a html version of the report. Report Other commands are as follows: locust -f locust_files/my_locust_file.py --headless -u 1000 -r 100 -run-time 1h30m CMD Desc -f specify file name & location if they are diff from default --headless only print in console, with no GUI -u no. users to spawn -r spawn rate; no users to add per second -run-time time limit for test","title":"Load Test"},{"location":"testing-ml/","text":"Machine Learning Tests Fake Data Sometimes we are unable to store a subset of the data in our repository for testing, due to the confidentiality of data. In this case, we can instead generate fake data using the popular library called Faker .","title":"ML Testing"},{"location":"testing-ml/#machine-learning-tests","text":"","title":"Machine Learning Tests"},{"location":"testing-ml/#fake-data","text":"Sometimes we are unable to store a subset of the data in our repository for testing, due to the confidentiality of data. In this case, we can instead generate fake data using the popular library called Faker .","title":"Fake Data"},{"location":"testing-profiling/","text":"Profiling After detecting potential issues, we will need to profile the code to see how to can resolve or improve on the issue. Jake VanderPlas provides an excellent description on how to do it using magic commands in jupyter notebooks. Latency Profiler If the latency is not up to par, we can profile our code to identify bottlenecks in the latency line by line. This is done using the library line-profiler . To install, pip install line_profiler To use the library, we first add the decorator @profile to the functions or methods you want to analyse. @profile def zvalue ( self , df , item , length ): vec = [ 0 ] * ( length + 1 ) start = time () df_filtered = df [ np . in1d ( df [ \"SKU\" ] . values , [ item ])] if len ( df_filtered ) == 1 : z_value = - np . inf else : loc = [ length + i for i in df_filtered . lag . tolist ()] value = df_filtered . quantity . tolist () for i , j in zip ( loc , range ( len ( value ))): vec [ i ] = value [ j ] z_value = self . rolling_zscore ( vec [: - 1 ], vec [ - 1 :], decay = self . decay ) return z_value Then we create the report by using the kernprof -l command; a .lprof report is generated. Following which, we view the report with the python -m line_profiler command. kernprof -l <script_name>.py python -m line_profiler <script_name>.py.lprof Profile Result Memory Profiler Another libray that works similarly but for memory is memory_profiler . Same as before, we just need to add @profile at the function or method, followed by the command python -m memory_profiler <script>.py This will generate a report in the terminal as shown. For variables that consume a lot of memory, but are not required downstream, we can delete it via del <variable-name> . Line # Mem usage Increment Occurences Line Contents ============================================================ 12 76 .180 MiB 76 .180 MiB 1 @profile 13 def load_model ( config ) : 14 76 .180 MiB 0 .000 MiB 1 api_model_name = os.path.join ( config.modeldir, config.model_name ) 15 16 230 .598 MiB 154 .418 MiB 1 model = pickle.load ( open ( api_model_name, \"rb\" )) 17 230 .598 MiB 0 .000 MiB 1 if not isinstance ( model, dict ) : 18 raise ValueError ( \"WARNING: this pickle file at {} is not a dict\" .format ( api_model_name )) 19 20 319 .242 MiB 88 .645 MiB 1 predictor = Predict ( config.PARAMS [ 0 :5 ] , config.l0, api_model_name ) 21 319 .242 MiB 0 .000 MiB 1 return predictor Scalene The new kid on the block for profiling is Scalene . It can profile latency, memory of RAM and GPU in a single library. We either use the usual @profile as mentioned in previous libraries, or use the following code to scan parts or the entirety of a script. We start the profiler using the command scalene <script_name>.py . from scalene import scalene_profiler # Turn profiling on scalene_profiler . start () # Turn profiling off scalene_profiler . stop () The results show both profiling in a single report. However, the latency is shown in percentages, and the memory appears to miss out certain initial RAM usage from library imports. Profile Result","title":"Profiling"},{"location":"testing-profiling/#profiling","text":"After detecting potential issues, we will need to profile the code to see how to can resolve or improve on the issue. Jake VanderPlas provides an excellent description on how to do it using magic commands in jupyter notebooks.","title":"Profiling"},{"location":"testing-profiling/#latency-profiler","text":"If the latency is not up to par, we can profile our code to identify bottlenecks in the latency line by line. This is done using the library line-profiler . To install, pip install line_profiler To use the library, we first add the decorator @profile to the functions or methods you want to analyse. @profile def zvalue ( self , df , item , length ): vec = [ 0 ] * ( length + 1 ) start = time () df_filtered = df [ np . in1d ( df [ \"SKU\" ] . values , [ item ])] if len ( df_filtered ) == 1 : z_value = - np . inf else : loc = [ length + i for i in df_filtered . lag . tolist ()] value = df_filtered . quantity . tolist () for i , j in zip ( loc , range ( len ( value ))): vec [ i ] = value [ j ] z_value = self . rolling_zscore ( vec [: - 1 ], vec [ - 1 :], decay = self . decay ) return z_value Then we create the report by using the kernprof -l command; a .lprof report is generated. Following which, we view the report with the python -m line_profiler command. kernprof -l <script_name>.py python -m line_profiler <script_name>.py.lprof Profile Result","title":"Latency Profiler"},{"location":"testing-profiling/#memory-profiler","text":"Another libray that works similarly but for memory is memory_profiler . Same as before, we just need to add @profile at the function or method, followed by the command python -m memory_profiler <script>.py This will generate a report in the terminal as shown. For variables that consume a lot of memory, but are not required downstream, we can delete it via del <variable-name> . Line # Mem usage Increment Occurences Line Contents ============================================================ 12 76 .180 MiB 76 .180 MiB 1 @profile 13 def load_model ( config ) : 14 76 .180 MiB 0 .000 MiB 1 api_model_name = os.path.join ( config.modeldir, config.model_name ) 15 16 230 .598 MiB 154 .418 MiB 1 model = pickle.load ( open ( api_model_name, \"rb\" )) 17 230 .598 MiB 0 .000 MiB 1 if not isinstance ( model, dict ) : 18 raise ValueError ( \"WARNING: this pickle file at {} is not a dict\" .format ( api_model_name )) 19 20 319 .242 MiB 88 .645 MiB 1 predictor = Predict ( config.PARAMS [ 0 :5 ] , config.l0, api_model_name ) 21 319 .242 MiB 0 .000 MiB 1 return predictor","title":"Memory Profiler"},{"location":"testing-profiling/#scalene","text":"The new kid on the block for profiling is Scalene . It can profile latency, memory of RAM and GPU in a single library. We either use the usual @profile as mentioned in previous libraries, or use the following code to scan parts or the entirety of a script. We start the profiler using the command scalene <script_name>.py . from scalene import scalene_profiler # Turn profiling on scalene_profiler . start () # Turn profiling off scalene_profiler . stop () The results show both profiling in a single report. However, the latency is shown in percentages, and the memory appears to miss out certain initial RAM usage from library imports. Profile Result","title":"Scalene"},{"location":"testing-pytest/","text":"Pytest Testing Overview Testing is an important aspect for any software system. The real value of testing occurs with system changes. Done correctly, each tests reduces uncertainty when analysing a change to the system A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source Pytest Basics Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ss show more print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest . Directory Structure Our repository directory is usually partitioned as follows below, with the testing and source code directory in separate folders. This is because the deployment project code is independent of the testing code. . \u251c\u2500\u2500 project \u2502 \u251c\u2500\u2500 app.py \u2502 \u2514\u2500\u2500 func.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 unit_tests \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 test_func.py \u2514\u2500\u2500 integration_tests \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_docker_api.py Importing Function to Test Scripts To launch pytest, we will need to launch from the root, so that the test script can access the app.py pytest test/unit_test/test_app.py -v In the test script we then append the project folder to it. This will make the import more straightforward, especially when there are many cross imports between scripts in the project folder. # test_app.py sys . path . append ( \"project\" ) from app import function_a An easier, and recommended method is to add an __init__.py to each of the test folders, which redirects to the modules in the project folders. # __init__.py import sys from os.path import dirname , join , normpath THIS_DIR = dirname ( __file__ ) PROJ_DIR = normpath ( join ( THIS_DIR , '..' , '..' , 'project' )) sys . path . append ( PROJ_DIR ) We can also create setup.py at the root, and pip install --editable . . This will create a folder project.egg.info like all the libraries installed using pip, and make all imports under the project folder to be imported anywhere from the root. # setup.py from setuptools import setup , find_packages setup ( name = \"project\" , packages = find_packages ()) # test_app.py from project.app import function_a INI File We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Class We can place our classes in classes to better organise them. Note that the class name must start with a Test_* for pytest to register, and each method name must have test_ . Below is an example class Test_personalised : def test_ucf_present_no_customer_id ( self ): try : out = RequestSchema ( ** data1 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 1\" def test_adl_present_no_customer_id ( self ): try : out = RequestSchema ( ** data2 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 2\" Fixtures When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called. Parameterize Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( \"inputs\" , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int ) If we have multiple variables for each iteration, we can do the following. @pytest . mark . parametrize ( \"image, expect\" , [ ( \"cone_dsl_00009.jpg\" , 2 ), ( \"cone_dsl_00016.jpg\" , 3 ), ( \"cone_dsl_00017.jpg\" , 2 ), ( \"cone_dsl_00018.jpg\" , 3 )]) def test_predict_validation ( image , expect ): \"\"\"test model with a few key test images\"\"\" img_path = os . path . join ( \"tests/data/images\" , image ) img_arr = cv2 . imread ( img_path ) prediction = detectObj ( img_arr ) detection_cnt = len ( prediction ) assert detection_cnt == expect Function Annotation While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>} Mocking Mocking is often used in unit tests where we can mock or \"fake\" a library, function, return value etc., as we only want to test the unit/function itself, not other related stuff that are bundled with it. Patching Using the in-built unittest.mock.patch we can easily mock functions and their return values. One useful note is that we need to patch where the object/function is looked up rather than where it is defined . # project/app.py import time def api_run (): time . sleep ( 2 ) # project/func.py def function_a (): api_run_a () return 1 def return_a_value (): return 1 def function_b (): y = return_a_value () return y In this sample unittest script, the api_call has a latency of 2 sec. After mocking it, we can see that the test runs instantly. We can also call patch without a decorator, in case we have other decorators that made the test function's arguments confusing. Last, we can also have a return value for the patched function. import sys from unittest.mock import patch sys . path . append ( \"project\" ) from func import function_a , function_b def test_no_mock (): ret = function_a () assert 1 == ret @patch ( 'func.api_run' ) def test_mock_func ( mock_api_run ): ret = function_a () assert 1 == ret def test_mock_func_no_deco (): with patch ( 'func.api_run' ) as mock_api_run : ret = function_a () assert 1 == ret @patch ( 'func.return_a_value' , return_value = 5 ) def test_mock_func_return ( mock_return_a_value ): \"\"\"api_run_b will return 5 to function b\"\"\" ret = function_b () assert 5 == ret We can also stack the patches, but note that first argument in the test function correspondings to the last patch decorator given. @patch ( \"app.load_model_item2vec_sku\" ) @patch ( \"app.query_sku\" ) @patch ( \"app.sort_by_cos_sim\" , return_value = \"abc\" ) def test_api ( sort_by_cos_sim , query_sku , load_model_item2vec_sku ): expected = sort_by_cos_sim . return_value with app . app . test_client () as client : response = client . post ( '/product-similarity' , json = { \"resultSize\" : 10 , \"sku\" : \"x\" }) . json assert response == { \"sku\" : expected } To mock a variable in the function, we can use the same patch. We don't need to pass any arguments to the test function. from unittest.mock import patch from app import function @patch ( \"app.variable_name\" , \"new_variable_value\" ) def test_script (): result = call_function () assert \"new_variable_value\" == result To have a cleaner interface without multiple with or decorators, we can instead (and recommended) use the pytest extension of mocking pip install pytest-mock . from pipeline import pipeline def test_mocker ( mocker ): mocker . patch ( 'pipeline.preprocess' ) predict = mocker . patch ( 'pipeline.training' , return_value = 5 ) expected = predict . return_value output = pipeline ( 5 ) assert output == expected Global Variables & Functions Sometimes, we might need to have global variables or functions defined in a script. An example is shown below, whereby we are opening a model file that is not present in the repository. # func.py import pickle f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Hence, if we use the test script below, a FileNotFoundError: [Errno 2] No such file or directory: 'model.pkl'. will be prompted. We cannot mock the global functions (or variables) ; they will only be mocked if they are called within the function/method tested, in this case is function_a . # test_func.py from func import function_a def test_function_a (): ret = function_a () assert 1 == ret There are a few ways to overcome this: Use if \"pytest\" not in sys.modules: import pickle import sys if \"pytest\" not in sys . modules : f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Place the global file calling under if __name__ == \"__main__\" import pickle import sys def function_a (): # do something return 1 if __name__ == \"__main__\" : f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) * Have a prod (and/or dev ) and test config variables, the latter which includes the path to a dummy test model. # ------ in test_func.py ------ import os os . environ [ \"ENV\" ] = TEST from func import function_a def test_function_a (): ret = function_a () assert 1 == ret # ------ in config.py ------ conf = \\ { \"ENV\" : { { \"PROD\" : { \"MODEL_NAME\" : \"model.pkl\" } { \"MODEL_PATH\" : \"./\" } }, { \"TEST\" : { \"MODEL_NAME\" : \"model.pkl\" } { \"MODEL_PATH\" : \"../tests/unit_tests/data\" } } }} # ------ func.py ------ import os import pickle import config env = os . environ [ \"ENV\" ] if not env : # if $ENV not given, set to PROD env = \"PROD\" conf = conf [ env ] model_path = os . path . join ( conf [ \"MODEL_PATH\" ], conf [ \"MODEL_NAME\" ]) f = open ( model_path , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Set the paths and names via environment variables Tox Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To reinstall the libraries, run tox -r . To run a specific environment, run tox -e integration . Note that skipsdist = True has to be added if you don't have a setup.py file present. [tox] skipsdist = True envlist = unit, integration [testenv:unit] deps = pytest = =5.3.5 pytest-cov = =2.10.1 -r{toxinidir}/tests/unit_tests/requirements.txt commands = pytest tests/unit_tests/ -v --cov=project --junitxml=report.xml [testenv:integration] deps = pytest = =5.3.5 Cython = =0.29.17 numpy = =1.19.2 commands = pip install -r {toxinidir}/project/requirements-serve.txt pytest tests/integration_tests/ -v --junitxml = report.xml # side tests ----------------------------- [testenv:lint] deps = flake8 commands = flake8 --select E,F,W association [testenv:bandit] deps = bandit commands = bandit -r -ll association [testenv:safety] deps = safety -r{toxinidir}/association/requirements.txt commands = safety check [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Test Coverage A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Overview & Pytest"},{"location":"testing-pytest/#pytest","text":"","title":"Pytest"},{"location":"testing-pytest/#testing-overview","text":"Testing is an important aspect for any software system. The real value of testing occurs with system changes. Done correctly, each tests reduces uncertainty when analysing a change to the system A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source","title":"Testing Overview"},{"location":"testing-pytest/#pytest-basics","text":"Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ss show more print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest .","title":"Pytest Basics"},{"location":"testing-pytest/#directory-structure","text":"Our repository directory is usually partitioned as follows below, with the testing and source code directory in separate folders. This is because the deployment project code is independent of the testing code. . \u251c\u2500\u2500 project \u2502 \u251c\u2500\u2500 app.py \u2502 \u2514\u2500\u2500 func.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 unit_tests \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 test_func.py \u2514\u2500\u2500 integration_tests \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_docker_api.py","title":"Directory Structure"},{"location":"testing-pytest/#importing-function-to-test-scripts","text":"To launch pytest, we will need to launch from the root, so that the test script can access the app.py pytest test/unit_test/test_app.py -v In the test script we then append the project folder to it. This will make the import more straightforward, especially when there are many cross imports between scripts in the project folder. # test_app.py sys . path . append ( \"project\" ) from app import function_a An easier, and recommended method is to add an __init__.py to each of the test folders, which redirects to the modules in the project folders. # __init__.py import sys from os.path import dirname , join , normpath THIS_DIR = dirname ( __file__ ) PROJ_DIR = normpath ( join ( THIS_DIR , '..' , '..' , 'project' )) sys . path . append ( PROJ_DIR ) We can also create setup.py at the root, and pip install --editable . . This will create a folder project.egg.info like all the libraries installed using pip, and make all imports under the project folder to be imported anywhere from the root. # setup.py from setuptools import setup , find_packages setup ( name = \"project\" , packages = find_packages ()) # test_app.py from project.app import function_a","title":"Importing Function to Test Scripts"},{"location":"testing-pytest/#ini-file","text":"We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"INI File"},{"location":"testing-pytest/#class","text":"We can place our classes in classes to better organise them. Note that the class name must start with a Test_* for pytest to register, and each method name must have test_ . Below is an example class Test_personalised : def test_ucf_present_no_customer_id ( self ): try : out = RequestSchema ( ** data1 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 1\" def test_adl_present_no_customer_id ( self ): try : out = RequestSchema ( ** data2 ) except Exception as e : msg = ast . literal_eval ( e . json ())[ 0 ][ \"msg\" ] assert msg == \"error msg 2\"","title":"Class"},{"location":"testing-pytest/#fixtures","text":"When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called.","title":"Fixtures"},{"location":"testing-pytest/#parameterize","text":"Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( \"inputs\" , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int ) If we have multiple variables for each iteration, we can do the following. @pytest . mark . parametrize ( \"image, expect\" , [ ( \"cone_dsl_00009.jpg\" , 2 ), ( \"cone_dsl_00016.jpg\" , 3 ), ( \"cone_dsl_00017.jpg\" , 2 ), ( \"cone_dsl_00018.jpg\" , 3 )]) def test_predict_validation ( image , expect ): \"\"\"test model with a few key test images\"\"\" img_path = os . path . join ( \"tests/data/images\" , image ) img_arr = cv2 . imread ( img_path ) prediction = detectObj ( img_arr ) detection_cnt = len ( prediction ) assert detection_cnt == expect","title":"Parameterize"},{"location":"testing-pytest/#function-annotation","text":"While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>}","title":"Function Annotation"},{"location":"testing-pytest/#mocking","text":"Mocking is often used in unit tests where we can mock or \"fake\" a library, function, return value etc., as we only want to test the unit/function itself, not other related stuff that are bundled with it.","title":"Mocking"},{"location":"testing-pytest/#patching","text":"Using the in-built unittest.mock.patch we can easily mock functions and their return values. One useful note is that we need to patch where the object/function is looked up rather than where it is defined . # project/app.py import time def api_run (): time . sleep ( 2 ) # project/func.py def function_a (): api_run_a () return 1 def return_a_value (): return 1 def function_b (): y = return_a_value () return y In this sample unittest script, the api_call has a latency of 2 sec. After mocking it, we can see that the test runs instantly. We can also call patch without a decorator, in case we have other decorators that made the test function's arguments confusing. Last, we can also have a return value for the patched function. import sys from unittest.mock import patch sys . path . append ( \"project\" ) from func import function_a , function_b def test_no_mock (): ret = function_a () assert 1 == ret @patch ( 'func.api_run' ) def test_mock_func ( mock_api_run ): ret = function_a () assert 1 == ret def test_mock_func_no_deco (): with patch ( 'func.api_run' ) as mock_api_run : ret = function_a () assert 1 == ret @patch ( 'func.return_a_value' , return_value = 5 ) def test_mock_func_return ( mock_return_a_value ): \"\"\"api_run_b will return 5 to function b\"\"\" ret = function_b () assert 5 == ret We can also stack the patches, but note that first argument in the test function correspondings to the last patch decorator given. @patch ( \"app.load_model_item2vec_sku\" ) @patch ( \"app.query_sku\" ) @patch ( \"app.sort_by_cos_sim\" , return_value = \"abc\" ) def test_api ( sort_by_cos_sim , query_sku , load_model_item2vec_sku ): expected = sort_by_cos_sim . return_value with app . app . test_client () as client : response = client . post ( '/product-similarity' , json = { \"resultSize\" : 10 , \"sku\" : \"x\" }) . json assert response == { \"sku\" : expected } To mock a variable in the function, we can use the same patch. We don't need to pass any arguments to the test function. from unittest.mock import patch from app import function @patch ( \"app.variable_name\" , \"new_variable_value\" ) def test_script (): result = call_function () assert \"new_variable_value\" == result To have a cleaner interface without multiple with or decorators, we can instead (and recommended) use the pytest extension of mocking pip install pytest-mock . from pipeline import pipeline def test_mocker ( mocker ): mocker . patch ( 'pipeline.preprocess' ) predict = mocker . patch ( 'pipeline.training' , return_value = 5 ) expected = predict . return_value output = pipeline ( 5 ) assert output == expected","title":"Patching"},{"location":"testing-pytest/#global-variables-functions","text":"Sometimes, we might need to have global variables or functions defined in a script. An example is shown below, whereby we are opening a model file that is not present in the repository. # func.py import pickle f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Hence, if we use the test script below, a FileNotFoundError: [Errno 2] No such file or directory: 'model.pkl'. will be prompted. We cannot mock the global functions (or variables) ; they will only be mocked if they are called within the function/method tested, in this case is function_a . # test_func.py from func import function_a def test_function_a (): ret = function_a () assert 1 == ret There are a few ways to overcome this: Use if \"pytest\" not in sys.modules: import pickle import sys if \"pytest\" not in sys . modules : f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Place the global file calling under if __name__ == \"__main__\" import pickle import sys def function_a (): # do something return 1 if __name__ == \"__main__\" : f = open ( \"model.pkl\" , \"r\" ) model = pickle . load ( f ) * Have a prod (and/or dev ) and test config variables, the latter which includes the path to a dummy test model. # ------ in test_func.py ------ import os os . environ [ \"ENV\" ] = TEST from func import function_a def test_function_a (): ret = function_a () assert 1 == ret # ------ in config.py ------ conf = \\ { \"ENV\" : { { \"PROD\" : { \"MODEL_NAME\" : \"model.pkl\" } { \"MODEL_PATH\" : \"./\" } }, { \"TEST\" : { \"MODEL_NAME\" : \"model.pkl\" } { \"MODEL_PATH\" : \"../tests/unit_tests/data\" } } }} # ------ func.py ------ import os import pickle import config env = os . environ [ \"ENV\" ] if not env : # if $ENV not given, set to PROD env = \"PROD\" conf = conf [ env ] model_path = os . path . join ( conf [ \"MODEL_PATH\" ], conf [ \"MODEL_NAME\" ]) f = open ( model_path , \"r\" ) model = pickle . load ( f ) def function_a (): # do something return 1 Set the paths and names via environment variables","title":"Global Variables &amp; Functions"},{"location":"testing-pytest/#tox","text":"Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To reinstall the libraries, run tox -r . To run a specific environment, run tox -e integration . Note that skipsdist = True has to be added if you don't have a setup.py file present. [tox] skipsdist = True envlist = unit, integration [testenv:unit] deps = pytest = =5.3.5 pytest-cov = =2.10.1 -r{toxinidir}/tests/unit_tests/requirements.txt commands = pytest tests/unit_tests/ -v --cov=project --junitxml=report.xml [testenv:integration] deps = pytest = =5.3.5 Cython = =0.29.17 numpy = =1.19.2 commands = pip install -r {toxinidir}/project/requirements-serve.txt pytest tests/integration_tests/ -v --junitxml = report.xml # side tests ----------------------------- [testenv:lint] deps = flake8 commands = flake8 --select E,F,W association [testenv:bandit] deps = bandit commands = bandit -r -ll association [testenv:safety] deps = safety -r{toxinidir}/association/requirements.txt commands = safety check [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"Tox"},{"location":"testing-pytest/#test-coverage","text":"A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Test Coverage"},{"location":"testing-schema/","text":"Schema Validation There are quite a number of popular schema validation libraries available, but I will just stick to Pydantic , for the reasons that it is popular, touted to be the fastest , and used natively in FastAPI. Config The config file is one of the most frequently changed file in the repository, and for that reason, it is important to include in our unit-tests. The below shows an example using a config yaml file & pydantic, with a custom validator. # config.yml modeldir: model modelname: nameofname format: dataframe # graph or dataframe import pytest import yaml from pydantic import BaseModel , ValidationError , validator class configSchema ( BaseModel ): modeldir : str modelname : str format : str @validator ( 'format' ) def format_list ( cls , v ): if v not in [ \"dataframe\" , \"graph\" ]: raise ValueError ( \"must be either 'dataframe or 'graph'\" ) return v def test_config (): \"\"\"test for all key-values in config file\"\"\" cf = yaml . safe_load ( open ( \"foldername/config.yml\" )) try : out = configSchema ( ** cf ) print ( out ) except ValidationError as e : pytest . raises ( e ) API Request Pydantic can also be used in Flask to validate all incoming requests. To validate the request schema before passing to the code, we can write a decorator function. import logging from functools import wraps from typing import List , Optional from flask import Flask , abort , jsonify , make_response , request , logging as flog from pydantic import BaseModel , ValidationError , confloat , conint , validator app = Flask ( __name__ ) class _weightage ( BaseModel ): recommender : str weight : confloat ( ge = 0 , le = 1 ) @validator ( \"recommender\" ) def recommender_list ( cls , v ): if v not in recommender_list : raise ValueError ( \"Recommender is not part of {} \" . format ( str ( recommender_list ))) class RequestSchema ( BaseModel ): productSKU : List [ str ] storeId : str weightage : List [ _weightage ] customerId : Optional [ str ] preceding_time_window : Optional [ str ] resultSize : Optional [ conint ( ge = 1 , le = 50 )] = 20 @validator ( \"weightage\" ) def weight_sum ( cls , v ): sumw = 0 for i in range ( len ( v )): weight = v [ i ] . weight sumw = weight + sumw if sumw != 1 : raise ValueError ( \"sum of weightage is not equals to 1\" ) def validate_request ( requestschema ): \"\"\"decorator to validate request schema\"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : requestschema ( ** request . json ) except ValidationError as e : app . logger . error ( ' {} - {} ' . format ( e , [ 422 ])) err_json = json . loads ( e . json ()) abort ( make_response ( jsonify ( err_json ), 422 )) return func ( * args , ** kwargs ) return wrapper return decorator @app . route ( \"/recommendation\" , methods = [ \"POST\" ]) @validate_request ( RequestSchema ) def fusion_api (): req_content = request . json # do something return predicted_results JSON While pydantic works fine for JSON validation and we can fine-tune to test at quite a grandular level, it can be hard to grasp at start. Using something like pytest-schema ( pip install pytest-schema ) makes writing test cases much easier in pytest . from pytest_schema import schema my_schema = { \"key1\" : int \"key2\" : float \"key3\" : { \"key4\" : str , \"key5\" : str } } response = { \"key1\" : 111 \"key2\" : 0.1 \"key3\" : { \"key4\" : \"test\" , \"key5\" : \"test } } def test_schema (): assert schema ( my_schema ) == response Pandas Ok, I take back what I said on Pydantic. For validating pandas dataframes, it is slightly difficult to use that library, hence we will use a library heavily inspired by Pydantic, called pandera . from pandera import DataFrameSchema , Column , Check schema = DataFrameSchema ({ \"antecedents\" : Column ( \"category\" ), \"consequents\" : Column ( object ), \"antecedent support\" : Column ( \"float32\" ), \"consequent support\" : Column ( \"float32\" ), \"support\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , \\ error = \"range checker [0, 1]\" )), \"confidence\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , error = \"range checker [0, 1]\" )), \"lift\" : Column ( \"float32\" ), \"leverage\" : Column ( \"float32\" ), \"conviction\" : Column ( \"float32\" ), \"storeid\" : Column ( \"category\" ), }) try : validated_df = schema ( df , lazy = True ) except Exception as e : print ( e ) The validated_df is the same dataframe that can be used to continue coding. If we are validating in pytest, we can add a try, except to catch the error in a graceful way. The argument lazy=True should added to ensure that it captures all errors before giving a validation error report as shown below. A total of 1 schema errors were found . Error Counts ------------ - schema_component_check : 1 Schema Error Summary -------------------- failure_cases n_failure_cases schema_context column check Column consequents pandas_dtype ( 'float64' ) [ object ] 1 Usage Tip --------- Directly inspect all errors by catching the exception : try : schema . validate ( dataframe , lazy = True ) except SchemaErrors as err : err . failure_cases # dataframe of schema errors err . data # invalid dataframe","title":"Schema Testing"},{"location":"testing-schema/#schema-validation","text":"There are quite a number of popular schema validation libraries available, but I will just stick to Pydantic , for the reasons that it is popular, touted to be the fastest , and used natively in FastAPI.","title":"Schema Validation"},{"location":"testing-schema/#config","text":"The config file is one of the most frequently changed file in the repository, and for that reason, it is important to include in our unit-tests. The below shows an example using a config yaml file & pydantic, with a custom validator. # config.yml modeldir: model modelname: nameofname format: dataframe # graph or dataframe import pytest import yaml from pydantic import BaseModel , ValidationError , validator class configSchema ( BaseModel ): modeldir : str modelname : str format : str @validator ( 'format' ) def format_list ( cls , v ): if v not in [ \"dataframe\" , \"graph\" ]: raise ValueError ( \"must be either 'dataframe or 'graph'\" ) return v def test_config (): \"\"\"test for all key-values in config file\"\"\" cf = yaml . safe_load ( open ( \"foldername/config.yml\" )) try : out = configSchema ( ** cf ) print ( out ) except ValidationError as e : pytest . raises ( e )","title":"Config"},{"location":"testing-schema/#api-request","text":"Pydantic can also be used in Flask to validate all incoming requests. To validate the request schema before passing to the code, we can write a decorator function. import logging from functools import wraps from typing import List , Optional from flask import Flask , abort , jsonify , make_response , request , logging as flog from pydantic import BaseModel , ValidationError , confloat , conint , validator app = Flask ( __name__ ) class _weightage ( BaseModel ): recommender : str weight : confloat ( ge = 0 , le = 1 ) @validator ( \"recommender\" ) def recommender_list ( cls , v ): if v not in recommender_list : raise ValueError ( \"Recommender is not part of {} \" . format ( str ( recommender_list ))) class RequestSchema ( BaseModel ): productSKU : List [ str ] storeId : str weightage : List [ _weightage ] customerId : Optional [ str ] preceding_time_window : Optional [ str ] resultSize : Optional [ conint ( ge = 1 , le = 50 )] = 20 @validator ( \"weightage\" ) def weight_sum ( cls , v ): sumw = 0 for i in range ( len ( v )): weight = v [ i ] . weight sumw = weight + sumw if sumw != 1 : raise ValueError ( \"sum of weightage is not equals to 1\" ) def validate_request ( requestschema ): \"\"\"decorator to validate request schema\"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : requestschema ( ** request . json ) except ValidationError as e : app . logger . error ( ' {} - {} ' . format ( e , [ 422 ])) err_json = json . loads ( e . json ()) abort ( make_response ( jsonify ( err_json ), 422 )) return func ( * args , ** kwargs ) return wrapper return decorator @app . route ( \"/recommendation\" , methods = [ \"POST\" ]) @validate_request ( RequestSchema ) def fusion_api (): req_content = request . json # do something return predicted_results","title":"API Request"},{"location":"testing-schema/#json","text":"While pydantic works fine for JSON validation and we can fine-tune to test at quite a grandular level, it can be hard to grasp at start. Using something like pytest-schema ( pip install pytest-schema ) makes writing test cases much easier in pytest . from pytest_schema import schema my_schema = { \"key1\" : int \"key2\" : float \"key3\" : { \"key4\" : str , \"key5\" : str } } response = { \"key1\" : 111 \"key2\" : 0.1 \"key3\" : { \"key4\" : \"test\" , \"key5\" : \"test } } def test_schema (): assert schema ( my_schema ) == response","title":"JSON"},{"location":"testing-schema/#pandas","text":"Ok, I take back what I said on Pydantic. For validating pandas dataframes, it is slightly difficult to use that library, hence we will use a library heavily inspired by Pydantic, called pandera . from pandera import DataFrameSchema , Column , Check schema = DataFrameSchema ({ \"antecedents\" : Column ( \"category\" ), \"consequents\" : Column ( object ), \"antecedent support\" : Column ( \"float32\" ), \"consequent support\" : Column ( \"float32\" ), \"support\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , \\ error = \"range checker [0, 1]\" )), \"confidence\" : Column ( \"float32\" , checks = Check ( lambda x : 0 <= x <= 1 , \\ element_wise = True , error = \"range checker [0, 1]\" )), \"lift\" : Column ( \"float32\" ), \"leverage\" : Column ( \"float32\" ), \"conviction\" : Column ( \"float32\" ), \"storeid\" : Column ( \"category\" ), }) try : validated_df = schema ( df , lazy = True ) except Exception as e : print ( e ) The validated_df is the same dataframe that can be used to continue coding. If we are validating in pytest, we can add a try, except to catch the error in a graceful way. The argument lazy=True should added to ensure that it captures all errors before giving a validation error report as shown below. A total of 1 schema errors were found . Error Counts ------------ - schema_component_check : 1 Schema Error Summary -------------------- failure_cases n_failure_cases schema_context column check Column consequents pandas_dtype ( 'float64' ) [ object ] 1 Usage Tip --------- Directly inspect all errors by catching the exception : try : schema . validate ( dataframe , lazy = True ) except SchemaErrors as err : err . failure_cases # dataframe of schema errors err . data # invalid dataframe","title":"Pandas"},{"location":"tf-serving/","text":"Tensorflow Serving Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things. Save Model as Protobuf We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables TensorFlow Serving with Docker It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind,source = /path/to/model_folder/,target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container REST API As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction ) gRPC Client To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"Tensorflow Serving"},{"location":"tf-serving/#tensorflow-serving","text":"Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things.","title":"Tensorflow Serving"},{"location":"tf-serving/#save-model-as-protobuf","text":"We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables","title":"Save Model as Protobuf"},{"location":"tf-serving/#tensorflow-serving-with-docker","text":"It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind,source = /path/to/model_folder/,target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container","title":"TensorFlow Serving with Docker"},{"location":"tf-serving/#rest-api","text":"As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction )","title":"REST API"},{"location":"tf-serving/#grpc-client","text":"To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"gRPC Client"},{"location":"virtual_env/","text":"Virtual Environment Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment. Anaconda Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate # in some IDE, like in WSL2 ubuntu, have to use source instead of conda for activation conda env list conda env remove -n <yourenvname> VENV venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. The downside of venv is that you cannot install a particular python version. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate # check env; via its path pip -V Jupyter Notebook Sometimes we need to experiment with certain code or data, and not ready to convert into python scripts; hence the use of jupyter notebooks. To change the environment or kernel (as named in jupyter), we first setup our virtual env using venv or conda, and install the necessary library as follows. pip install ipykernel python -m ipykernel install --user --name = <env_name> In the jupyter notebook, we then select the kernel option from the top right and switch to the virtual env we launched earlier. Sourced from this article Pip pypip is one of the main python package manager, another being conda. To see what are the packages being install in pip, we can use the command pip freeze . To check if a specified package is being installed, we can use pip show <package-name> , e.g. pip show flask Name: Flask Version: 1 .1.2 Summary: A simple framework for building complex web applications. Home-page: https://palletsprojects.com/p/flask/ Author: Armin Ronacher Author-email: armin.ronacher@active-4.com License: BSD-3-Clause Location: /Users/siyang/opt/anaconda3/lib/python3.8/site-packages Requires: itsdangerous, Werkzeug, click, Jinja2 Required-by: locust, Flask-BasicAuth","title":"Virtual Environment"},{"location":"virtual_env/#virtual-environment","text":"Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment.","title":"Virtual Environment"},{"location":"virtual_env/#anaconda","text":"Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate # in some IDE, like in WSL2 ubuntu, have to use source instead of conda for activation conda env list conda env remove -n <yourenvname>","title":"Anaconda"},{"location":"virtual_env/#venv","text":"venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. The downside of venv is that you cannot install a particular python version. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate # check env; via its path pip -V","title":"VENV"},{"location":"virtual_env/#jupyter-notebook","text":"Sometimes we need to experiment with certain code or data, and not ready to convert into python scripts; hence the use of jupyter notebooks. To change the environment or kernel (as named in jupyter), we first setup our virtual env using venv or conda, and install the necessary library as follows. pip install ipykernel python -m ipykernel install --user --name = <env_name> In the jupyter notebook, we then select the kernel option from the top right and switch to the virtual env we launched earlier. Sourced from this article","title":"Jupyter Notebook"},{"location":"virtual_env/#pip","text":"pypip is one of the main python package manager, another being conda. To see what are the packages being install in pip, we can use the command pip freeze . To check if a specified package is being installed, we can use pip show <package-name> , e.g. pip show flask Name: Flask Version: 1 .1.2 Summary: A simple framework for building complex web applications. Home-page: https://palletsprojects.com/p/flask/ Author: Armin Ronacher Author-email: armin.ronacher@active-4.com License: BSD-3-Clause Location: /Users/siyang/opt/anaconda3/lib/python3.8/site-packages Requires: itsdangerous, Werkzeug, click, Jinja2 Required-by: locust, Flask-BasicAuth","title":"Pip"}]}