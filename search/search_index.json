{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Engineer? Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of the bosses, serenaded with smooches for executing seemingly difficult but yet copied from the latest SOTA open-sourced models. But who cares, the model works, the attention & prestige is nice, & hey, they have PhDs even though their theses, more often than not, has nothing to do with AI. AI Engineers though, are the unsung heros. They understand modelling, they understand engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, application testing, deployment, and certainly how to train a SOTA model from an open-sourced repository too. Imagine a small data science team or during a crisis period, will they hire/retain an AI Scientist? Or an AI Engineer? The good thing is, you can always change your job role as AI Scientist anytime! Can't say that for the former. Besides their full stack technical capabilities, good AI Engineers also need to have certain attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's start with some basics.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-engineer","text":"Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of the bosses, serenaded with smooches for executing seemingly difficult but yet copied from the latest SOTA open-sourced models. But who cares, the model works, the attention & prestige is nice, & hey, they have PhDs even though their theses, more often than not, has nothing to do with AI. AI Engineers though, are the unsung heros. They understand modelling, they understand engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, application testing, deployment, and certainly how to train a SOTA model from an open-sourced repository too. Imagine a small data science team or during a crisis period, will they hire/retain an AI Scientist? Or an AI Engineer? The good thing is, you can always change your job role as AI Scientist anytime! Can't say that for the former. Besides their full stack technical capabilities, good AI Engineers also need to have certain attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's start with some basics.","title":"So You Wana Be an AI Engineer?"},{"location":"code-standards/","text":"","title":"Code standards"},{"location":"demo/","text":"Demo Site","title":"Demo Site"},{"location":"demo/#demo-site","text":"","title":"Demo Site"},{"location":"docker/","text":"Docker","title":"Docker"},{"location":"docker/#docker","text":"","title":"Docker"},{"location":"flask/","text":"Flask Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task. Simple Flask App Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run () Gunicorn Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it gunicorn --bind 0 .0.0.0:5000 -w 2 serve_http:app Testing Python Requests The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Flask"},{"location":"flask/#flask","text":"Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task.","title":"Flask"},{"location":"flask/#simple-flask-app","text":"Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run ()","title":"Simple Flask App"},{"location":"flask/#gunicorn","text":"Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it gunicorn --bind 0 .0.0.0:5000 -w 2 serve_http:app","title":"Gunicorn"},{"location":"flask/#testing","text":"","title":"Testing"},{"location":"flask/#python-requests","text":"The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Python Requests"},{"location":"git/","text":"Git Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Version Control : allows rollback of code Branches : allows different components to be developed concurrently and merged to a single branch later The process works roughly like this: Create a new branch Work on a new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch Branches Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <featurename> Commit Code Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request. Merge Request Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Code Version Control"},{"location":"git/#git","text":"Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Version Control : allows rollback of code Branches : allows different components to be developed concurrently and merged to a single branch later The process works roughly like this: Create a new branch Work on a new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch","title":"Git"},{"location":"git/#branches","text":"Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <featurename>","title":"Branches"},{"location":"git/#commit-code","text":"Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request.","title":"Commit Code"},{"location":"git/#merge-request","text":"Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Merge Request"},{"location":"security/","text":"","title":"Security"},{"location":"virtual_env/","text":"Virtual Environment Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment. Anaconda Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname> VENV venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"Virtual Environment"},{"location":"virtual_env/#virtual-environment","text":"Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment.","title":"Virtual Environment"},{"location":"virtual_env/#anaconda","text":"Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname>","title":"Anaconda"},{"location":"virtual_env/#venv","text":"venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"VENV"}]}