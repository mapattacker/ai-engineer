{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Engineer? Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-engineer","text":"Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet training the latest SOTA neural network architecture using transfer learning. But who cares, the model works, the attention & prestige is nice. AI Engineers though, are the unsung heros. They understand modelling, they understand software engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, testing, deployment, and certainly how to do transfer learning too. Jokes aside, it is important to note that a mature ML system may contain only 5% of ML code, highlighting the need for a diversified skillset. The Hidden Debt in Machine Learning Systems. Source The AI engineer will also need to be familiar with the processes of the ML life cycle, and what to do within and between each process. Machine Learning Life Cycle. Source Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's get started.","title":"So You Wana Be an AI Engineer?"},{"location":"cicd/","text":"CI/CD","title":"CI/CD"},{"location":"cicd/#cicd","text":"","title":"CI/CD"},{"location":"code-standards/","text":"Code Standards requirements.txt requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101 DocStrings DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco ISort Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py Black Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py Flake8 A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Code Standards"},{"location":"code-standards/#code-standards","text":"","title":"Code Standards"},{"location":"code-standards/#requirementstxt","text":"requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101","title":"requirements.txt"},{"location":"code-standards/#docstrings","text":"DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco","title":"DocStrings"},{"location":"code-standards/#isort","text":"Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py","title":"ISort"},{"location":"code-standards/#black","text":"Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py","title":"Black"},{"location":"code-standards/#flake8","text":"A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Flake8"},{"location":"cuda/","text":"Nvidia GPU Installing CUDA Go to Nvidia's CUDA website sudo sh cuda_10.0.130_410.48_linux.run Verification cat /proc/driver/nvidia/version : Verify Nvidia driver version nvcc -V : Verify CUDA toolkit version","title":"Nvidia GPU"},{"location":"cuda/#nvidia-gpu","text":"","title":"Nvidia GPU"},{"location":"cuda/#installing-cuda","text":"Go to Nvidia's CUDA website sudo sh cuda_10.0.130_410.48_linux.run","title":"Installing CUDA"},{"location":"cuda/#verification","text":"cat /proc/driver/nvidia/version : Verify Nvidia driver version nvcc -V : Verify CUDA toolkit version","title":"Verification"},{"location":"demo/","text":"Demo Site For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"demo/#demo-site","text":"For every model, a demo site should be created to demonstrate the reliability and use of the service, especially to product owners & clients. Streamlit is an amazing python library used to create ML demo sites fast, while providing a beautiful & consistent template. To facilitate deployment, we should always ensure both requirements.txt & Dockerfile are created & tested. Below is an example how we can develop a simple object detection demo site. \"\"\"streamlit server for demo site\"\"\" import json import time import requests import streamlit as st from PIL import Image from utils_image import encode_image , draw_on_image , json2array_yolo json_data = \\ { \"requests\" : [ { \"features\" : [ { \"maxResults\" : 20 , \"min_height\" : 0.03 , \"min_width\" : 0.03 , \"score_th\" : 0.3 , \"nms_iou\" : 0.4 , } ], \"image\" : { \"content\" : None } } ] } def hide_navbar (): \"\"\"hide navbar so its not apparent this is from streamlit\"\"\" hide_streamlit_style = \"\"\" <style> #MainMenu {visibility: hidden;} footer {visibility: hidden;} </style> \"\"\" st . markdown ( hide_streamlit_style , unsafe_allow_html = True ) def send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ): \"\"\"Sends JSON request & recieve a JSON response Args ---- api (str): API endpoint image (image file): opened image file json_data (dict): json request template token (str): API token maxfeatures (int): max no. of objects to detect in image min_height (float): min height of bounding box (relative to H) to be included min_width (float): min width of bounding box (relative to W) to be included score_th (float): min prediciton score for bounding box to be included nms_iou (float): intersection over union, for non-max suppression Returns ------- json_response (dict): API response \"\"\" base64_bytes = encode_image ( image ) token = { \"X-Bedrock-Api-Token\" : token } json_data [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"maxResults\" ] = maxfeatures json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_height\" ] = min_height json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"min_width\" ] = min_width json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_data [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou response = requests . post ( api , headers = token , json = json_data ) # response = requests.post(api, json=json_data) json_response = response . content . decode ( 'utf-8' ) json_response = json . loads ( json_response ) return json_response def main ( api ): \"\"\"design streamlit fronend\"\"\" st . title ( \"SafetyCone Detection\" ) token = st . text_input ( \"API Token\" ) uploaded_file = st . file_uploader ( \"Upload an image.\" ) if uploaded_file is not None and api != \"\" : image = Image . open ( uploaded_file ) # header st . subheader ( \"Uploaded Image\" ) st . image ( image , width = 400 ) # sidebar st . sidebar . title ( \"Change Parameters\" ) maxfeatures = st . sidebar . slider ( \"Max Features\" , min_value = 1 , max_value = 50 , value = 20 , step = 1 ) min_height = st . sidebar . slider ( \"Min Height\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) min_width = st . sidebar . slider ( \"Min Width\" , min_value = 0.01 , max_value = 0.05 , value = 0.03 , step = 0.01 ) score_th = st . sidebar . slider ( \"Score Th\" , min_value = 0.1 , max_value = 0.5 , value = 0.3 , step = 0.1 ) nms_iou = st . sidebar . slider ( \"NMS IOU\" , min_value = 0.1 , max_value = 0.5 , value = 0.4 , step = 0.1 ) if st . button ( \"Send API Request\" ): st . title ( \"Results\" ) # send request start_time = time . time () json_response = send2api ( api , image , json_data , token , \\ maxfeatures , min_height , min_width , \\ score_th , nms_iou ) latency = time . time () - start_time st . write ( \"**Est. latency = ` {:.3f} s`**\" . format ( latency )) # result image st . subheader ( \"Visualize Output\" ) bboxes_json = json_response [ \"safetycone\" ][ \"boundingPoly\" ][ \"normalizedVertices\" ] bboxes_array = json2array_yolo ( bboxes_json ) class_mapper = { 0 : \"safetycone\" } image_res = draw_on_image ( image , bboxes_array , class_mapper ) st . image ( image_res , width = 400 ) # api response st . subheader ( \"API Response\" ) st . json ( json . dumps ( json_response , indent = 2 )) if __name__ == \"__main__\" : api = \"http://localhost:5000\" hide_navbar () main ( api ) To launch the app, use streamlit run app.py . After uploading a picture, the results are shown as such.","title":"Demo Site"},{"location":"docker/","text":"Docker Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian Docker is not the only container service available, but the most widely used Basics There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network Dockerfile The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] Intel-GPU If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename> ARM-GPU The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename> Common Commands Build The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu Run For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Map the Flask port from container to the outside world --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id> Check Status This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats Clean To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker system prune Debug We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash Network For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. Docker-Compose When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . The official Docker blog post gives a good introduction on this. version: \"3\" services: facedetection: build: ./facedetection container_name: facedetection ports: - 5001:5000 restart: always maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 restart: always When we are ready, we use the commands docker-compose build & docker-compose up -d to build & launch the containers respectively.","title":"Docker"},{"location":"docker/#docker","text":"Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian Docker is not the only container service available, but the most widely used","title":"Docker"},{"location":"docker/#basics","text":"There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network","title":"Basics"},{"location":"docker/#dockerfile","text":"The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ]","title":"Dockerfile"},{"location":"docker/#intel-gpu","text":"If the host has Nvidia GPU, we should make use of it so that the inference time is much faster; x10 faster for this example. We will need to choose a base image that has CUDA & CUDNN installed so that GPU can be utilised. FROM pytorch/pytorch:1.5.1-cuda10.1-cudnn7-devel RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run --gpus all --ipc = host -d -p 5000 :5000 --name <containername> <imagename>","title":"Intel-GPU"},{"location":"docker/#arm-gpu","text":"The ARM architecture requires a little more effort; the below installation is for Nvidia Jetson Series Kit. # From https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch # it contains Pytorch v1.5 and torchvision v0.6.0 FROM nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3 ARG DEBIAN_FRONTEND = noninteractive RUN apt-get -y update && apt-get -y upgrade RUN apt-get install -y wget python3-setuptools python3-pip libfreetype6-dev # Install OpenCV; from https://github.com/JetsonHacksNano/buildOpenCV RUN apt-get -y install qt5-default COPY ./build/OpenCV-4.1.1-dirty-aarch64.sh . RUN ./OpenCV-4.1.1-dirty-aarch64.sh --prefix = /usr/local/ --skip-license && ldconfig # Install other Python libraries required by Module COPY requirements.txt . RUN pip3 install -r requirements-serve.txt # Copy Python source codes COPY . . RUN apt-get clean && rm -rf /var/lib/apt/lists/* && rm OpenCV-4.1.1-dirty-aarch64.sh ENTRYPOINT [ \"python3\" , \"-u\" , \"serve_http.py\" ] The run command is: sudo docker run -d -p 5000 :5000 --runtime nvidia --name <containername> <imagename>","title":"ARM-GPU"},{"location":"docker/#common-commands","text":"","title":"Common Commands"},{"location":"docker/#build","text":"The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu","title":"Build"},{"location":"docker/#run","text":"For an AI microservice in Docker, there are five main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Map the Flask port from container to the outside world --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container The full command is as such. sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> We can also stop, start or restart the container if required. sudo docker stop <container-name/id> sudo docker start <container-name/id> sudo docker restart <container-name/id>","title":"Run"},{"location":"docker/#check-status","text":"This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats","title":"Check Status"},{"location":"docker/#clean","text":"To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker system prune","title":"Clean"},{"location":"docker/#debug","text":"We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash","title":"Debug"},{"location":"docker/#network","text":"For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network.","title":"Network"},{"location":"docker/#docker-compose","text":"When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . The official Docker blog post gives a good introduction on this. version: \"3\" services: facedetection: build: ./facedetection container_name: facedetection ports: - 5001:5000 restart: always maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 restart: always When we are ready, we use the commands docker-compose build & docker-compose up -d to build & launch the containers respectively.","title":"Docker-Compose"},{"location":"dvc/","text":"Data Version Control DVC is a library for ML versioning of blobs, which can include data and/or models. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ] DVC INIT dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded to the repository, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs Add S3 Keys To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\" Add Remote Link We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin Push Data to S3 We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link. Update Git We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version. Retrieve Data from S3 We can pull from specific dvc folders by using dvc pull -r <remote name> <foldername>.dvc Full Code Example # add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Data Version Control"},{"location":"dvc/#data-version-control","text":"DVC is a library for ML versioning of blobs, which can include data and/or models. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ]","title":"Data Version Control"},{"location":"dvc/#dvc-init","text":"dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded to the repository, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs","title":"DVC INIT"},{"location":"dvc/#add-s3-keys","text":"To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\"","title":"Add S3 Keys"},{"location":"dvc/#add-remote-link","text":"We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin","title":"Add Remote Link"},{"location":"dvc/#push-data-to-s3","text":"We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link.","title":"Push Data to S3"},{"location":"dvc/#update-git","text":"We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version.","title":"Update Git"},{"location":"dvc/#retrieve-data-from-s3","text":"We can pull from specific dvc folders by using dvc pull -r <remote name> <foldername>.dvc","title":"Retrieve Data from S3"},{"location":"dvc/#full-code-example","text":"# add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Full Code Example"},{"location":"flask/","text":"Flask Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task. Simple Flask App Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" ) Gunicorn Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. The binding of the host as mentioned previously in flask, can be done in gunicorn as described below. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it # to see all flask stdout, we can change the log level to debug gunicorn --bind 0 .0.0.0:5000 -w 2 --timeout = 300 --log-level info serve_http:app Testing Python Requests The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j ) Postman Postman is a popular GUI to easily send requests and see the responses.","title":"Flask"},{"location":"flask/#flask","text":"Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task.","title":"Flask"},{"location":"flask/#simple-flask-app","text":"Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . It is important to set the host=\"0.0.0.0\" , so that it binds to all network interfaces of the container, and will be callable from the outside. \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run ( host = \"0.0.0.0\" )","title":"Simple Flask App"},{"location":"flask/#gunicorn","text":"Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. The binding of the host as mentioned previously in flask, can be done in gunicorn as described below. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it # to see all flask stdout, we can change the log level to debug gunicorn --bind 0 .0.0.0:5000 -w 2 --timeout = 300 --log-level info serve_http:app","title":"Gunicorn"},{"location":"flask/#testing","text":"","title":"Testing"},{"location":"flask/#python-requests","text":"The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Python Requests"},{"location":"flask/#postman","text":"Postman is a popular GUI to easily send requests and see the responses.","title":"Postman"},{"location":"git/","text":"Git Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code Remote URL When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git Branches Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. Commands # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname> Workflow Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch Merge Request Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request. Version Control Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> Commands To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request. Resets Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset <file.py> Workflow The various git commands and how they interact at different stages are as illustrated below. Release Tags We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags Delete from Git History BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git bfg --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote","title":"Code Version Control"},{"location":"git/#git","text":"Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Branches : allows different components to be developed concurrently and merged to a single branch later Version Control : allows rollback of code","title":"Git"},{"location":"git/#remote-url","text":"When using the first git clone to pull the repository to your local machine, we need to specify to use https or ssh. To me it is better for the latter to avoid multiple logins whenever we want to push changes to the remote repository. # git repository via ssh git clone git@github.com:mapattacker/ai-engineer.git # show remote url git remote -v # switch remote url to https git remote set-url origin https://github.com/mapattacker/ai-engineer.git","title":"Remote URL"},{"location":"git/#branches","text":"Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option.","title":"Branches"},{"location":"git/#commands","text":"# check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <branchname> # delete local branch git branch -d <branchname> # delete remote branch git push origin -d <branchname>","title":"Commands"},{"location":"git/#workflow","text":"Below is a workflow typical in a project team. Image sourced from Buddy . Create a new branch for a new feature Work on the new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch","title":"Workflow"},{"location":"git/#merge-request","text":"Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Merge Request"},{"location":"git/#version-control","text":"Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename>","title":"Version Control"},{"location":"git/#commands_1","text":"To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request.","title":"Commands"},{"location":"git/#resets","text":"Sometimes, we might accidentally add or commit our changes erroneously. To reset them, we need to use the reset command. # put latest commit history to staging git reset --soft HEAD~1 # remove all staging files git reset # remove specific staging file git reset <file.py>","title":"Resets"},{"location":"git/#workflow_1","text":"The various git commands and how they interact at different stages are as illustrated below.","title":"Workflow"},{"location":"git/#release-tags","text":"We can add tags, usually for release versions, so that it is easy to revert back to a specific version within a branch. CMD Desc git tag -a v1.0.0 -m \"1st prod version\" tag in local git push origin v1.0.0 push to remote git tag -d v1.0 delete local tag ONLY git push --delete origin v1.0.0 delete remote tag ONLY git tag list tags","title":"Release Tags"},{"location":"git/#delete-from-git-history","text":"BFG Repo-Cleaner is a 3rd party java file used to remove files that are not accidentally uploaded, e.g., passwords, blobs. Download their file from the website and follow the instructions there, or from the example below. CMD Desc git clone --mirror git@gitlab.com:project/repo-name.git clone only the .git bfg --delete-files \"*.{png,jpg,gif}\" repo-name.git delete certain file extensions cd repo-name.git go into git directory git reflog expire --expire=now --all && git gc --prune=now --aggressive delete old files git push --force push updated git to remote","title":"Delete from Git History"},{"location":"model-opt/","text":"Model Optimization There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision. CPU Flow Control Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0' References Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace Tensorflow Model Optimization Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization . TensorRT TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below. Layer Fusion During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT Quantization Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. Save Model An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory> Benchmark Functions To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model { input_saved_model_dir } ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds TRT Conversion Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory> Others There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO) References Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"Model Optimization"},{"location":"model-opt/#model-optimization","text":"There are various ways to optimize a trained neural network model such that we can improve the inference performance with little impact on the precision.","title":"Model Optimization"},{"location":"model-opt/#cpu-flow-control","text":"Some operations in the neural network cannot run in GPU, hence data sometimes have to be transferred from CPU-GPU. This transfer if occurred many times, increases the latency. We can profile this by recording the events and time as a json file, and view in Chrome at the URL, chrome://tracing . image = get_image_by_url ( \"https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg\" ) image_tensor = image . resize (( 300 , 300 )) image_tensor = np . array ( image_tensor ) image_tensor = np . expand_dims ( image_tensor , axis = 0 ) input_tensor_name = \"image_tensor:0\" output_tensor_names = [ 'detection_boxes:0' , 'detection_classes:0' , 'detection_scores:0' , 'num_detections:0' ] ssd_mobilenet_v2_graph_def = load_graph_def ( frozen_model_path ) with tf . Graph () . as_default () as g : tf . import_graph_def ( ssd_mobilenet_v2_graph_def , name = '' ) input_tensor = g . get_tensor_by_name ( input_tensor_name ) output_tensors = [ g . get_tensor_by_name ( name ) for name in output_tensor_names ] with tf . Session ( graph = g ) as sess : options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) run_metadata = tf . RunMetadata () outputs = sess . run ( output_tensors , feed_dict = { input_tensor : image_tensor }, options = options , run_metadata = run_metadata ) inference_time = ( time . time () - start ) * 1000. # in ms # Write metadata fetched_timeline = timeline . Timeline ( run_metadata . step_stats ) chrome_trace = fetched_timeline . generate_chrome_trace_format () with open ( 'ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename , 'w' ) as f : f . write ( chrome_trace ) We can specify certain nodes that are more CPU efficient, to run within CPU, thereby decreasing the data transfer and improving the inference performance. For example all the NonMaxSuppression are placed for CPU processing since most of the flow operations happen in this block. for node in ssd_mobilenet_v2_optimized_graph_def . node : if 'NonMaxSuppression' in node . name : node . device = '/device:CPU:0'","title":"CPU Flow Control"},{"location":"model-opt/#references","text":"Optimize NVIDIA GPU performance for efficient model inference HowTo profile TensorFlow Find the bottleneck of your Keras model using TF trace","title":"References"},{"location":"model-opt/#tensorflow-model-optimization","text":"Tensorflow has has developed its own library for model optimization, which includes quantization, sparsity and pruning, and clustering. It can be installed via pip install --user --upgrade tensorflow-model-optimization .","title":"Tensorflow Model Optimization"},{"location":"model-opt/#tensorrt","text":"TensorFlow Integration for TensorRT (TF-TRT) is developed by Nvidia, which is a deep learning framework based on CUDA for inference acceleration. It optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Source: Nvidia TensorRT Two of the most important optimizations are described below.","title":"TensorRT"},{"location":"model-opt/#layer-fusion","text":"During the TF-TRT optimization, TensorRT performs several important transformations and optimizations to the neural network graph. First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible, certain layers (such as convolution, bias, and ReLU) are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Source: Speed up Tensorflow inference on GPUs - TensorRT","title":"Layer Fusion"},{"location":"model-opt/#quantization","text":"Typically, model training is performed using 32-bit floating point ( FP32 ) mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16 ) as the neural network architecture only requires a feed-forward network. Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput. There are certain requirements using quantization in TensorRT FP16 requires Nvidia GPUs that have hardware tensor cores INT8 is more complex , and requires a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation.","title":"Quantization"},{"location":"model-opt/#save-model","text":"An example is used from a keras' model, and then saving it as a tensorflow protobuf model. import tensorflow as tf from tensorflow.keras.applications.inception_v3 import InceptionV3 model = InceptionV3 ( weights = 'imagenet' ) tf . saved_model . save ( model , 'inceptionv3_saved_model' ) We can view the model details using the saved_model_cli . !saved_model_cli show --all --dir <model-directory>","title":"Save Model"},{"location":"model-opt/#benchmark-functions","text":"To check that the new optimized model has faster inference & throughput, we want to prepare a function for loading the model... def load_tf_saved_model_infer ( input_saved_model_dir ): \"\"\"load model for inference\"\"\" print ( f 'Loading saved model { input_saved_model_dir } ...' ) saved_model_loaded = tf . saved_model . load ( input_saved_model_dir , tags = [ tag_constants . SERVING ]) infer = saved_model . signatures [ 'serving_default' ] print ( infer . structured_outputs ) return infer We can use batch inference to send many images to the GPU at once promotes parallel processing and improve throughput. def batch_input ( batch_size = 8 ): batched_input = np . zeros (( batch_size , 299 , 299 , 3 ), dtype = np . float32 ) for i in range ( batch_size ): img_path = './data/img %d .JPG' % ( i % 4 ) img = image . load_img ( img_path , target_size = ( 299 , 299 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) batched_input [ i , :] = x batched_input = tf . constant ( batched_input ) return batched_input ... and lastly, a function for benchmarking the latency & throughput. def benchmark ( batched_input , infer , N_warmup_run = 50 , N_run = 1000 ): \"\"\"benchmark latency & throughput Args batched_input: infer (tf.float32): tensorflow model for inference N_warmup_run (int): no. of runs to warm up GPU N_run (int): no. of runs after warmup to benchmark Rets all_preds (list): predicted output \"\"\" elapsed_time = [] all_preds = [] batch_size = batched_input . shape [ 0 ] for i in range ( N_warmup_run ): labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () for i in range ( N_run ): start_time = time . time () labeling = infer ( batched_input ) preds = labeling [ 'predictions' ] . numpy () end_time = time . time () elapsed_time = np . append ( elapsed_time , end_time - start_time ) all_preds . append ( preds ) if i % 50 == 0 : print ( 'Steps {} - {} average: {:4.1f} ms' . format ( i , i + 50 , ( elapsed_time [ - 50 :] . mean ()) * 1000 )) print ( 'Throughput: {:.0f} images/s' . format ( N_run * batch_size / elapsed_time . sum ())) return all_preds","title":"Benchmark Functions"},{"location":"model-opt/#trt-conversion","text":"Tensorflow library has integrated Tensorrt, called TrtGraphConverterV2 so we can call its API to convert the existing model. Below is a simple snippet on how to use it. from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = None , conversion_params = TrtConversionParams ( precision_mode = 'FP32' , max_batch_size = 1 minimum_segment_size = 3 , max_workspace_size_bytes = 8000000000 , use_calibration = True , maximum_cached_engines = 1 , is_dynamic_op = True , rewriter_config_template = None , ) ) converter . convert () converter . save ( output_saved_model_dir ) While below gives a function that allows more flexibility to change between various precision, and also calibrate the dataset when going to int8 . from tensorflow.python.compiler.tensorrt import trt_convert as trt def convert_to_trt_graph_and_save ( precision_mode = 'float32' , input_saved_model_dir = 'inceptionv3_saved_model' , max_workspace_size_bytes = 8000000000 calibration_data = None ): # select precision if precision_mode == 'float32' : precision_mode = trt . TrtPrecisionMode . FP32 converted_save_suffix = '_TFTRT_FP32' elif precision_mode == 'float16' : precision_mode = trt . TrtPrecisionMode . FP16 converted_save_suffix = '_TFTRT_FP16' elif precision_mode == 'int8' : precision_mode = trt . TrtPrecisionMode . INT8 converted_save_suffix = '_TFTRT_INT8' output_saved_model_dir = input_saved_model_dir + converted_save_suffix conversion_params = trt . DEFAULT_TRT_CONVERSION_PARAMS . _replace ( precision_mode = precision_mode , max_workspace_size_bytes = max_workspace_size_bytes ) converter = trt . TrtGraphConverterV2 ( input_saved_model_dir = input_saved_model_dir , conversion_params = conversion_params ) # calibrate data if using int8 if precision_mode == trt . TrtPrecisionMode . INT8 : def calibration_input_fn (): yield ( calibration_data , ) converter . convert ( calibration_input_fn = calibration_input_fn ) else : converter . convert () # save tf-trt model converter . save ( output_saved_model_dir = output_saved_model_dir ) We can check the signature of the new model again using the saved_model_cli . !saved_model_cli show --all --dir <new-model-directory>","title":"TRT Conversion"},{"location":"model-opt/#others","text":"There are many other 3rd party optimization libraries, including: torch2trt Keras inference time optimizer (KITO) The Tensor Algebra SuperOptimizer (TASO)","title":"Others"},{"location":"model-opt/#references_1","text":"Optimizing TensorFlow Serving performance with NVIDIA TensorRT","title":"References"},{"location":"monitoring/","text":"Monitoring After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source Monitoring Systems Prometheus Prometheus is the go-to open source metrics monitoring platform for ML systems. Grafana Grafana can be integrated with Prometheus to display the metrics in interactive visualizations. Kibana Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"After model deploying, it is essential to monitor the system as life-real scenario is different from initial model training and testing. In essence, they can be summarize into 3 parts. 3 Key Metrics to Monitor. Source","title":"Monitoring"},{"location":"monitoring/#monitoring-systems","text":"","title":"Monitoring Systems"},{"location":"monitoring/#prometheus","text":"Prometheus is the go-to open source metrics monitoring platform for ML systems.","title":"Prometheus"},{"location":"monitoring/#grafana","text":"Grafana can be integrated with Prometheus to display the metrics in interactive visualizations.","title":"Grafana"},{"location":"monitoring/#kibana","text":"Kibana is a free and open user interface that lets you visualize your logs, or model input distributions. It can also create alerts. AWS has an integrated service called Amazon Elasticsearch Service which sets up all the infrastructure ready for use.","title":"Kibana"},{"location":"security/","text":"Security Scans If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline. Source Code Scan Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' ) Dependency Scan This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json Secrets Scan detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/* Pre-Commit Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version : python : python3 repos : - repo : https :// github . com /PyCQA/ bandit rev : 1.7 hooks : - id : bandit args : [- lll ] - repo : https :// github . com /Yelp/ detect - secrets rev : v0 . 13.0 hooks : - id : detect - secrets args : [-- no - base64 - string - scan ]","title":"Security"},{"location":"security/#security-scans","text":"If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline.","title":"Security Scans"},{"location":"security/#source-code-scan","text":"Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Desc CMD Installation pip install bandit Single File bandit your_file.py Directory bandit -r ~/your_repos/project Display only High Severities bandit -r ~/your_repos/project -lll Json Report bandit --format json --recursive project/ -l --output bandit.json We can skip certain vulnerabilities, by placing .bandit file at directory to check with the contents as such. [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' )","title":"Source Code Scan"},{"location":"security/#dependency-scan","text":"This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels. Desc CMD Installation pip install safety Check installed packages in VM safety check Check requirements.txt, does not include dependencies safety check -r requirements.txt Full Report safety check Json Report safety check --json --output insecure_report.json","title":"Dependency Scan"},{"location":"security/#secrets-scan","text":"detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. There are other libraries that does the same job, like aws's git-secrets . Desc CMD Installation pip install detect-secrets Directory detect-secrets scan directory/*","title":"Secrets Scan"},{"location":"security/#pre-commit","text":"Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. This prevents the hassle of committing sensitive info, or avoid the hassle of cleaning up the git history later. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version : python : python3 repos : - repo : https :// github . com /PyCQA/ bandit rev : 1.7 hooks : - id : bandit args : [- lll ] - repo : https :// github . com /Yelp/ detect - secrets rev : v0 . 13.0 hooks : - id : detect - secrets args : [-- no - base64 - string - scan ]","title":"Pre-Commit"},{"location":"serverless/","text":"Serverless Architecture Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice. Lambda & API Gateway In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins . Zappa Zappa is a popular python library used to automatically launch python lambda functions & api-gateways. VENV Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works. Setup AWS Policy & Keys for Zappa Deployment The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] } How to Use After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user policy is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only. Commands Cmd Desc zappa init create a zappa_settings.json file zappa deploy deploy stage as specified in json zappa update update stage zappa undeploy delete lambda & API Gateway zappa package zip all files together zappa tail print tail logs from CloudWatch zappa status check status Lambda Execution Role The default execution role created by zappa is too liberal. From the author \"It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources\". To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] }","title":"Serverless"},{"location":"serverless/#serverless-architecture","text":"Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice.","title":"Serverless Architecture"},{"location":"serverless/#lambda-api-gateway","text":"In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins .","title":"Lambda &amp; API Gateway"},{"location":"serverless/#zappa","text":"Zappa is a popular python library used to automatically launch python lambda functions & api-gateways.","title":"Zappa"},{"location":"serverless/#venv","text":"Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works.","title":"VENV"},{"location":"serverless/#setup-aws-policy-keys-for-zappa-deployment","text":"The most difficult part of Zappa is to setup a user & policies for zappa to deploy your lambda/gateway application. First, we need to create a new user in AWS, and then define a policy which allows zappa to do all the work for deployment. Then we create the AWS access & secret keys, and port it in our local environment variables. A healthy discussion on various minimium policies can be viewed here . A working liberal example is given below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:CreateFunction\" , \"lambda:ListVersionsByFunction\" , \"lambda:DeleteFunction\" , \"lambda:GetFunctionConfiguration\" , \"lambda:GetAlias\" , \"lambda:InvokeFunction\" , \"lambda:GetFunction\" , \"lambda:UpdateFunctionConfiguration\" , \"lambda:RemovePermission\" , \"lambda:GetPolicy\" , \"lambda:AddPermission\" , \"lambda:DeleteFunctionConcurrency\" , \"lambda:UpdateFunctionCode\" , \"events:PutRule\" , \"events:ListRuleNamesByTarget\" , \"events:ListRules\" , \"events:RemoveTargets\" , \"events:ListTargetsByRule\" , \"events:DescribeRule\" , \"events:DeleteRule\" , \"events:PutTargets\" , \"logs:DescribeLogStreams\" , \"logs:FilterLogEvents\" , \"logs:DeleteLogGroup\" , \"apigateway:DELETE\" , \"apigateway:PATCH\" , \"apigateway:GET\" , \"apigateway:PUT\" , \"apigateway:POST\" , \"cloudformation:DescribeStackResource\" , \"cloudformation:UpdateStack\" , \"cloudformation:ListStackResources\" , \"cloudformation:DescribeStacks\" , \"cloudformation:CreateStack\" , \"cloudformation:DeleteStack\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:GetRole\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:AttachRolePolicy\" , \"iam:PutRolePolicy\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:iam::1234567890:role/*-ZappaLambdaExecutionRole\" , \"arn:aws:s3:::<python-serverless-deployment-s3>\" ] }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : \"arn:aws:s3:::<python-serverless-deployment-s3>/*\" } ] }","title":"Setup AWS Policy &amp; Keys for Zappa Deployment"},{"location":"serverless/#how-to-use","text":"After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"<python-serverless-deployment-s3>\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user policy is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only.","title":"How to Use"},{"location":"serverless/#commands","text":"Cmd Desc zappa init create a zappa_settings.json file zappa deploy deploy stage as specified in json zappa update update stage zappa undeploy delete lambda & API Gateway zappa package zip all files together zappa tail print tail logs from CloudWatch zappa status check status","title":"Commands"},{"location":"serverless/#lambda-execution-role","text":"The default execution role created by zappa is too liberal. From the author \"It grants access to all actions for all resources for types CloudWatch, S3, Kinesis, SNS, SQS, DynamoDB, and Route53; lambda:InvokeFunction for all Lambda resources; Put to all X-Ray resources; and all Network Interface operations to all EC2 resources\". To set a manual policy, we need to set the \"manage_roles\" to false and include either the \"role_name\" or \"role_arn\". We then create the role & policies in AWS. { \"dev\" : { ... \"manage_roles\" : false , // Disable Zappa client managing roles. \"role_name\" : \"MyLambdaRole\" , // Name of your Zappa execution role. Optional, default: <project_name>-<env>-ZappaExecutionRole. \"role_arn\" : \"arn:aws:iam::12345:role/app-ZappaLambdaExecutionRole\" , // ARN of your Zappa execution role. Optional. ... }, ... } The trick is to first generate the default role, then to filter down the default policy to your specific usecase. Below is an example, which retrieves & updates S3 & DynamnoDB. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaInvoke\" , \"Effect\" : \"Allow\" , \"Action\" : \"lambda:InvokeFunction\" , \"Resource\" : \"arn:aws:lambda:ap-southeast-1:123456780:function:complianceai-lambda-dev\" }, { \"Sid\" : \"Logs\" , \"Effect\" : \"Allow\" , \"Action\" : \"logs:*\" , \"Resource\" : \"arn:aws:logs:ap-southeast-1:123456780:log-group:/aws/lambda/complianceai-lambda-dev:*\" }, { \"Sid\" : \"S3ListObjectsInBucket\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::compliance-bucket\" }, { \"Sid\" : \"S3AllObjectActions\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*Object\" , \"Resource\" : \"arn:aws:s3:::compliance-bucket/*\" }, { \"Sid\" : \"DynamoDBListAndDescribe\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:List*\" , \"dynamodb:DescribeReservedCapacity*\" , \"dynamodb:DescribeLimits\" , \"dynamodb:DescribeTimeToLive\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"DynamoDBSpecificTable\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:BatchGet*\" , \"dynamodb:DescribeStream\" , \"dynamodb:DescribeTable\" , \"dynamodb:Get*\" , \"dynamodb:Query\" , \"dynamodb:Scan\" , \"dynamodb:BatchWrite*\" , \"dynamodb:CreateTable\" , \"dynamodb:Delete*\" , \"dynamodb:Update*\" , \"dynamodb:PutItem\" ], \"Resource\" : \"arn:aws:dynamodb:*:*:table/ComplianceTable\" } ] }","title":"Lambda Execution Role"},{"location":"test-other/","text":"Other Tests These are other tests not included in pytest. Memory Leak Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker .","title":"Other Tests"},{"location":"test-other/#other-tests","text":"These are other tests not included in pytest.","title":"Other Tests"},{"location":"test-other/#memory-leak","text":"Memory leak is an insidious fault in the code that can consume the entire memory (RAM or GPU) over time & crash the application, together with other applications in the same server. To detect this, we usually need to run multiple requests over a period of time to the API to see if the memory builds up. We can use an app I developed to do this; memoryleak-checker .","title":"Memory Leak"},{"location":"test/","text":"Testing Testing is an important aspect for any software system as it reduces uncertainty and increases reliability when ungoing refactoring processes. A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source Much of my knowledge in machine learning testing is through this udemy course called Testing & Monitoring Machine Learning Model Deployments . I would highly recommend you to sign up for it. Pytest Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here Basics A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest . INI File We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Fixtures When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called. Parameterize Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( inputs , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int ) Function Annotation While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>} Unit-Test Integration Test https://towardsdatascience.com/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d Tox Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To run a specific environment, run tox -e integration . [tox] envlist = py37 [testenv] deps = pytest commands = pytest [testenv:integration] deps = black commands = black --check --diff mylibrary [pytest] filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning Test Coverage A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Testing"},{"location":"test/#testing","text":"Testing is an important aspect for any software system as it reduces uncertainty and increases reliability when ungoing refactoring processes. A traditional testing pyramid from Google includes the type of tests as well as their quantity for each test as illustrated below. Software engineering test pyramid. Source With a machine learning system being more complex, as it does not just contain code , but also data and model . We will also need to test for these. Martin Fowler's test pyramid for ML. Source Much of my knowledge in machine learning testing is through this udemy course called Testing & Monitoring Machine Learning Model Deployments . I would highly recommend you to sign up for it.","title":"Testing"},{"location":"test/#pytest","text":"Pytest is one of the python libraries used for automated testing your code. Refactoring & improvements can thus be easier to validate, debug, and also included in your CI/CD pipeline. An good tutorial on using pytest can be found here","title":"Pytest"},{"location":"test/#basics","text":"A few basic conventions of pytest includes: Test scripts must start or end with the word test , e.g. test_data.py Functions must start with the word test , e.g. def test_filepath_exits() Pytest uses assert to provide more context in the test result Here are a few commands in the cli after running pytest CMD Desc -v report in verbose, but truncated mode -vv report in verbose, non-truncated mode -q report in quiet mode, useful when there are hundreds of tests -s show print statements in console -ignore ignore the specified path when discovering paths -maxfail stop test after specified no. of failures <test_example.py> run only tests in this test script -m <name> run only tests marked with <name> For the last command, we can mark each of our test functions, e.g. pytest.mark.integration so that we can call them later using pytest -m integration . To run other unmarked test, we use pytest -m 'not integration' Note that pytest runs from the existing python library, and can ignore your virtual environment setup. As a result, you might face import errors even though the libraries are already installed in the virtual env. To ask it to run in the virtual env, use python -m pytest .","title":"Basics"},{"location":"test/#ini-file","text":"We can add more configurations to pytest through a pytest.ini file. The commands can also be placed in a tox.ini file if using tox to launch pytest. [pytest] junit_family = xunit1 filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"INI File"},{"location":"test/#fixtures","text":"When creating test scripts, we often need to run some common code before we create the test case itself. Instead of repeating the same code in every test, we create fixtures that establish a baseline code for our tests. Some common fixtures include data loaders, or initialize database connections. Fixtures are define in the file conftest.py so that tests from multiple test modules in the directory can access the fixture function. # conftest.py import pytest @pytest . fixture def input_value (): return 4 # test_module.py def test_function ( input_value ): subject = square ( input_value ) assert subject == 16 The test fixture have a scope argument, e.g. @pytest.fixture(scope=\"session\") which set when the fixture will be destroyed. For example, the session scope is destroyed at the end of the test session (i.e. all the tests). By default, the scope is set to function , which mean the fixture is destroyed at the end of the test function when it was called.","title":"Fixtures"},{"location":"test/#parameterize","text":"Parameterizing allows multiple inputs to be iterate over a test function. For the example below, there will be 3 tests being run. import pytest @pytest . mark . parametrize ( inputs , [ 2 , 3 , 4.5 ]) def test_function ( inputs ): subject = square ( inputs ) assert isinstance ( subject , int )","title":"Parameterize"},{"location":"test/#function-annotation","text":"While not a pytest functionality, Function Annotations are useful and easy to implement for checking the input and return types in pytest. def foo ( a : int , b : float = 5.0 ) -> bool : return \"something\" foo . __annotations__ # {'a': <class 'int'>, 'b': <class 'float'>, 'return': <class 'bool'>}","title":"Function Annotation"},{"location":"test/#unit-test","text":"","title":"Unit-Test"},{"location":"test/#integration-test","text":"https://towardsdatascience.com/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d","title":"Integration Test"},{"location":"test/#tox","text":"Tox is a generic virtualenv for commandline execution, and is particularly useful for running tests. Think of it as a CI/CD running in your laptop. Tox can be installed via pip install tox A file called tox.ini is created and set at the root, with the contents as follows. To run pytest using tox, just run tox . To run a specific environment, run tox -e integration . [tox] envlist = py37 [testenv] deps = pytest commands = pytest [testenv:integration] deps = black commands = black --check --diff mylibrary [pytest] filterwarnings = ignore::DeprecationWarning ignore::RuntimeWarning ignore::UserWarning ignore::FutureWarning","title":"Tox"},{"location":"test/#test-coverage","text":"A common question is how much should I test? An analogy would be the wearing of armor to cover oneself. Too much armor will weight down a person, while too little will be very vulnerable. A right balance needs to be struck. A test coverage report show how much of your code base is tested, and allows you to spot any potential part of your code that you have missed testing. We can use the library pytest-cov for this, and it can be installed via pip install pytest-cov . Once test cases are written with pytest, we can use it to generate the coverage report. An example, using the command pytest --cov=<project-folder> <test-folder> is shown below. pytest -- cov = project tests / -------------------- coverage : ... --------------------- Name Stmts Miss Cover ---------------------------------------- myproj / __init__ 2 0 100 % myproj / myproj 257 13 94 % myproj / feature4286 94 7 92 % ---------------------------------------- TOTAL 353 20 94 % To ignore certain files or folders, add a .coveragerc file to where you call pytest, and add the following. [run] omit = project/train.py Various report formats can be produced, and are as stated in their documentation .","title":"Test Coverage"},{"location":"tf-serving/","text":"Tensorflow Serving Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things. Save Model as Protobuf We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables TensorFlow Serving with Docker It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind, \\ source = /path/to/model_folder/, \\ target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container REST API As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction ) gRPC Client To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"Tensorflow Serving"},{"location":"tf-serving/#tensorflow-serving","text":"Tensorflow Serving, developed by Google, allows fast inference using gRPC (and also REST). It eliminates the need for a Flask web server, and talks directly to the model. Some of the other advantages, stated from the official github site includes: Can serve multiple models, or multiple versions of the same model simultaneously Exposes both gRPC as well as HTTP inference endpoints Allows deployment of new model versions without changing any client code Supports canarying new versions and A/B testing experimental models Adds minimal latency to inference time due to efficient, low-overhead implementation Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models Much of the learnings of this page came from a course from Coursera called TensorFlow Serving with Docker for Model Deployment . Do sign up for this free course for a better sense of things.","title":"Tensorflow Serving"},{"location":"tf-serving/#save-model-as-protobuf","text":"We need to use tensorflow.save_mode.save() , or tf.keras's model.save(filepath=file_path, save_format='tf') API to save the trained model in a protobuf format, e.g. model.pb . import os import time import tensorflow as tf base_path = \"amazon_review/\" path = os . path . join ( base_path , str ( int ( time . time ()))) tf . saved_model . save ( model , path ) This is how a model directory & its contents look like, with each model version stored in a time-stamped folder. With the timestamp, it allows automated canary deployment when a new version is created. \u251c\u2500\u2500 amazon_review \u2502 \u251c\u2500\u2500 1600788643 \u2502 \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 saved_model.pb \u2502 \u2502 \u2514\u2500\u2500 variables","title":"Save Model as Protobuf"},{"location":"tf-serving/#tensorflow-serving-with-docker","text":"It is easiest to serve the model with docker, as described from the official website . Below is an example, where we link the model to the dockerised tensorflow-serving image, and expose both gRPC & REST ports. docker pull tensorflow/serving docker run -p 8500 :8500 \\ -p 8501 :8501 \\ --mount type = bind, \\ source = /path/to/model_folder/, \\ target = /models/model_folder \\ -e MODEL_NAME = model_name \\ -t tensorflow/serving --name amazonreview CMD Desc -p 8500:8500 expose gRPC port -p 8501:8501 expose REST port --mount type=bind,source=/path/to/model_folder/,target=/models/model_folder copy model from local folder to docker container folder -e MODEL_NAME=model_name name of the model, also used to define serving endpoint --name amazonreview name of docker container","title":"TensorFlow Serving with Docker"},{"location":"tf-serving/#rest-api","text":"As with all REST APIs, we can use python, CURL or Postman to send our requests. However, we need to be aware that, by default: The request JSON is {\"instances\": [model_input]} , with model_input as a list The endpoint is http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB} CMD Desc HOST domain name or IP address -p 8501:8501 default 8501 MODEL_NAME name of model defined in docker instance VERB model signature. either predict , classify , or regress Below is an example using CURL curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\ -X POST http://localhost:8501/v1/models/amazon_review:predict Below is an example using python. More here import json import requests import sys def get_rest_url ( model_name , host = '127.0.0.1' , port = '8501' , verb = 'predict' , version = None ): \"\"\" generate the URL path\"\"\" url = \"http:// {host} : {port} /v1/models/ {model_name} \" . format ( host = host , port = port , model_name = model_name ) if version : url += 'versions/ {version} ' . format ( version = version ) url += ': {verb} ' . format ( verb = verb ) return url def get_model_prediction ( model_input , model_name = 'amazon_review' , signature_name = 'serving_default' ): url = get_rest_url ( model_name ) data = { \"instances\" : [ model_input ]} rv = requests . post ( url , data = json . dumps ( data )) if rv . status_code != requests . codes . ok : rv . raise_for_status () return rv . json ()[ 'predictions' ] if __name__ == '__main__' : url = get_rest_url ( model_name = 'amazon_review' ) model_input = \"This movie is great! :D\" model_prediction = get_model_prediction ( model_input ) print ( model_prediction )","title":"REST API"},{"location":"tf-serving/#grpc-client","text":"To use gRPC for tensorflow-serving, we need to first install it via pip install grpc . There are certain requirements needed for this protocol, namely: Prediction data has to be converted to the Protobuf format Request types have designated types, e.g. float, int, bytes Payloads need to be converted to base64 Connect to the server via gRPC stubs Below is an example of a gRPC implementation in python. import sys import grpc from grpc.beta import implementations import tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 , get_model_metadata_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc def get_stub ( host = '127.0.0.1' , port = '8500' ): channel = grpc . insecure_channel ( '127.0.0.1:8500' ) stub = prediction_service_pb2_grpc . PredictionServiceStub ( channel ) return stub def get_model_prediction ( model_input , stub , model_name = 'amazon_review' , signature_name = 'serving_default' ): request = predict_pb2 . PredictRequest () request . model_spec . name = model_name request . model_spec . signature_name = signature_name request . inputs [ 'input_input' ] . CopyFrom ( tf . make_tensor_proto ( model_input )) response = stub . Predict . future ( request , 5.0 ) # 5 seconds return response . result () . outputs [ \"output\" ] . float_val def get_model_version ( model_name , stub ): request = get_model_metadata_pb2 . GetModelMetadataRequest () request . model_spec . name = 'amazon_review' request . metadata_field . append ( \"signature_def\" ) response = stub . GetModelMetadata ( request , 10 ) # signature of loaded model is available here: response.metadata['signature_def'] return response . model_spec . version . value if __name__ == '__main__' : print ( \" \\n Create RPC connection ...\" ) stub = get_stub () while True : print ( \" \\n Enter an Amazon review [:q for Quit]\" ) if sys . version_info [ 0 ] <= 3 : sentence = raw_input () if sys . version_info [ 0 ] < 3 else input () if sentence == ':q' : break model_input = [ sentence ] model_prediction = get_model_prediction ( model_input , stub ) print ( \"The model predicted ...\" ) print ( model_prediction )","title":"gRPC Client"},{"location":"virtual_env/","text":"Virtual Environment Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment. Anaconda Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname> VENV venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"Virtual Environment"},{"location":"virtual_env/#virtual-environment","text":"Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment.","title":"Virtual Environment"},{"location":"virtual_env/#anaconda","text":"Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname>","title":"Anaconda"},{"location":"virtual_env/#venv","text":"venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"VENV"}]}