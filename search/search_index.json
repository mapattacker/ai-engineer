{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Engineer? Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet git clone from the latest SOTA open-sourced repositories. But who cares, the model works, the attention & prestige is nice, & hey, they have PhDs even though their theses, more often than not, has nothing to do with AI. AI Engineers though, are the unsung heros. They understand modelling, they understand engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, application testing, deployment, and certainly how to train a SOTA model from an open-sourced repository too. Imagine a small data science team or during a crisis period, will they hire/retain an AI Scientist? Or an AI Engineer? The good thing is, you can always change your job name as AI Scientist anytime & breeze through an interview! Can't say that for the former. Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's start with some basics.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-engineer","text":"Ok, I think most people do not aspire to be an AI Engineer. Being an AI Scientist is sexier & is always at the front & center of bosses, consistently serenaded with smooches for executing seemingly difficult, but yet git clone from the latest SOTA open-sourced repositories. But who cares, the model works, the attention & prestige is nice, & hey, they have PhDs even though their theses, more often than not, has nothing to do with AI. AI Engineers though, are the unsung heros. They understand modelling, they understand engineering, and heck, they know how to do almost anything! Database, web development, cloud engineering, application testing, deployment, and certainly how to train a SOTA model from an open-sourced repository too. Imagine a small data science team or during a crisis period, will they hire/retain an AI Scientist? Or an AI Engineer? The good thing is, you can always change your job name as AI Scientist anytime & breeze through an interview! Can't say that for the former. Besides their full stack technical capabilities, good AI Engineers also possess certain critical attributes. An incredible intolerance for slobby work Great organizational abilities A demand for reproducible work with strong documentation. Such a wide range of expertise is difficult to find, so a company who values AI Engineers will not devalue them in terms of compensation. Still not convinced? Well, I would say that I have seen a number of AI Scientists starting to learn more engineering aspects of AI. Heck, even they secretly aspire to be an AI Engineer, though they still want to wear their fancy scientists' hats. Now that I'm done with the rambling, if you are ready to hop on the bandwagon, let's start with some basics.","title":"So You Wana Be an AI Engineer?"},{"location":"code-standards/","text":"Code Standards requirements.txt requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101 DocStrings DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco ISort Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py Black Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py Flake8 A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py Pre-Commit Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version : python : python3 repos : - repo : https :// github . com /psf/ black rev : stable hooks : - id : black - repo : https :// gitlab . com /pycqa/ flake8 rev : 3.8 . 3 hooks : - id : flake8 - repo : https :// github . com /PyCQA/ bandit rev : 1.6 . 2 hooks : - id : bandit - repo : https :// github . com /timothycrosley/is ort rev : 4.3 . 21 hooks : - id : isort","title":"Code Standards"},{"location":"code-standards/#code-standards","text":"","title":"Code Standards"},{"location":"code-standards/#requirementstxt","text":"requirements.txt contains the list of python libraries & their versions that is needed for the application to work. This file must be present for every repository. To see how it works together in a virtual environment, refer to the earlier section . Use the library pip install pipreqs to auto-generate using the command pipreqs directory_path . Note to test with pip install -r requirements.txt , to ensure installation works. Put the python version in the first line in the file (and comment out). e.g., # python=3.8. This will ensure what python version to use. Below is an example of how different types of libraries should be placed. By default pipreqs can only auto-populate libraries that are pip installed. For others which are installed via github or a website, you will need to specify as below. # python=3.7 pika == 1.1 . 0 scipy == 1.4 . 1 scikit_image == 0.16 . 2 numpy == 1.18 . 1 # package from github, not present in pip git + https : // github . com / cftang0827 / pedestrian_detection_ssdlite # wheel file stored in a website -- find - links https : // dl . fbaipublicfiles . com / detectron2 / wheels / cu101 / index . html detectron2 -- find - links https : // download . pytorch . org / whl / torch_stable . html torch == 1.5 . 0 + cu101 torchvision == 0.6 . 0 + cu101","title":"requirements.txt"},{"location":"code-standards/#docstrings","text":"DocStrings should be present for every function & method of a class. For primary functions, ensure that it provides 1) Description of the function, 2) Argument name, data type, and description, 3) Return description & data type. For secondary functions, they should minimally contain the function description. def yolo2coco_bb ( size , yolo ): \"\"\"convert yolo format to coco bbox Args ---- size (tuple): (width, height) yolo (tuple): (ratio-bbox-centre-x, ratio-bbox-centre-y, ratio-bbox-w, ratio-bbox-h) Returns ------- coco: xmin, ymin, boxw, boxh \"\"\" width = size [ 0 ] height = size [ 1 ] centrex = yolo [ 0 ] * width centrey = yolo [ 1 ] * height boxw = yolo [ 2 ] * width boxh = yolo [ 3 ] * height halfw = boxw / 2 halfh = boxh / 2 xmin = centrex - halfw ymin = centrey - halfh coco = xmin , ymin , boxw , boxh return coco","title":"DocStrings"},{"location":"code-standards/#isort","text":"Sorts by alphabetical order the imported libraries, while splitting python base libraries as first order, with the 3rd party libraries as second order, and local script imports as the third. Installation: pip install isort Single File: isort your_file.py","title":"ISort"},{"location":"code-standards/#black","text":"Black auto-formats python files to adhere to PEP8 format as well as other styles that the team felt is useful. This includes removing whitespaces, new lines, change single to double quotes, and etc. Installation: pip install black Format Files Single File: black my_file.py Directory: black directory_name Check Changes w/o Formating black --diff my_file.py","title":"Black"},{"location":"code-standards/#flake8","text":"A wrapper of 3 libraries that checks (but does not change) , against python standard styling (PEP8), programming errors (like \u201clibrary imported but unused\u201d and \u201cUndefined name\u201d) and to check cyclomatic complexity. Code Desc E/W pep8 errors and warnings F*** PyFlakes codes (see below) C9** McCabe complexity plugin mccabe N8** Naming Conventions plugin pep8-naming Installation: pip install flake8 Current Project: flake8 Single File (and all imported scripts): flake8 my_file.py","title":"Flake8"},{"location":"code-standards/#pre-commit","text":"Pre-commit is a git hook that you preconfig to run certain scripts, in this case, the above ones before committing to git. A useful compilation is done by laac.dev . Installation: pip install pre-commit create config file at root of project: .pre-commit-config.yaml Installation (into git hook): pre-commit install Uninstallation (from git hook): pre-commit uninstall Add Files for Commit: git add files.py Run Commit, and Precommit will autorun: git commit -m 'something' Skip Hook: SKIP=flake8 git commit -m \"something\" Here is an example of the .pre-commit-config.yaml default_language_version : python : python3 repos : - repo : https :// github . com /psf/ black rev : stable hooks : - id : black - repo : https :// gitlab . com /pycqa/ flake8 rev : 3.8 . 3 hooks : - id : flake8 - repo : https :// github . com /PyCQA/ bandit rev : 1.6 . 2 hooks : - id : bandit - repo : https :// github . com /timothycrosley/is ort rev : 4.3 . 21 hooks : - id : isort","title":"Pre-Commit"},{"location":"demo/","text":"Demo Site","title":"Demo Site"},{"location":"demo/#demo-site","text":"","title":"Demo Site"},{"location":"docker/","text":"Docker Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian Docker is not the only container service available, but the most widely used Basics There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network Dockerfile The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ] Common Commands Build The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu Run For AI microservice in Docker there are four main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Map the Flask port from container to the outside world --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename> Check Status This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats Clean To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker system prune Debug We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash Network For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network. Docker-Compose When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . The official Docker blog post gives a good introduction on this. version: \"3\" services: facedetection: build: ./facedetection container_name: facedetection ports: - 5001:5000 restart: always maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 restart: always When we are ready, we use the commands docker-compose build & docker-compose up -d to build & launch the containers respectively.","title":"Docker"},{"location":"docker/#docker","text":"Ah... the new world of microservices. Docker is the company that stands at the forefront of modular applications, also known as microservices, where each of them can & usually communicate via APIs. This \"container\" service is wildly popular, because it is OS-agnostic & resource light, with the only dependency is that docker needs to be installed. Some facts: All Docker images are linux, usually either alpine or debian Docker is not the only container service available, but the most widely used","title":"Docker"},{"location":"docker/#basics","text":"There are various nouns that are important in the Docker world. Noun Desc Image Installed version of an application Container Launched instance of an application from an image Container Registry a hosting platform to store your images. E.g. Docker Container Registry (DCR), AWS Elastic Container Registry (ECR) Dockerfile A file containing instructions to build an image Also, these are the various verbs that are important in the Docker world. Verb Desc build Copy & install necessary packages & scripts to create an image run Launch a container from an image rm or rmi remove a container or image respectively prune clear obsolete images, containers or network","title":"Basics"},{"location":"docker/#dockerfile","text":"The Dockerfile is the essence of Docker, where it contains instructions on how to build an image. There are four main instructions: CMD Desc FROM base image to build on, pulled from Docker Container Registry RUN install dependencies COPY copy files & scripts CMD or ENTRYPOINT command to launch the application Each line in the Dockerfile is cached in memory by sequence, so that a rebuilding of image will not need to start from the beginning but the last line where there are changes. Therefore it is always important to always (copy and) install the dependencies first before copying over the rest of the source codes, as shown below. FROM python:3.7 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY requirements-serve.txt . RUN pip install --upgrade pip RUN pip install -r requirements-serve.txt COPY . . ENTRYPOINT [ \"python\" , \"-u\" , \"serve_http.py\" ]","title":"Dockerfile"},{"location":"docker/#common-commands","text":"","title":"Common Commands"},{"location":"docker/#build","text":"The basic build command is quite straight forward. However, if we have various docker builds in a repo, it is best to name it with and extension representing the function, e.g. Dockerfile.cpu . For that, we will need to direct Docker to this specific file name. sudo docker build -t <imagename> . sudo docker build -t <imagename> . -f Dockerfile.cpu","title":"Build"},{"location":"docker/#run","text":"For AI microservice in Docker there are four main run commands to launch the container. Cmd Desc -d detached mode -p 5000:5000 Map the Flask port from container to the outside world --log-opt max-size=5m --log-opt max-file=5 limit the logs stored by Docker, by default it is unlimited --restart always in case the server crash & restarts, the container will also restart --name <containername> as a rule of thumb, always name the image & container sudo docker run -d -p 5000 :5000 --log-opt max-size = 5m --log-opt max-file = 5 --restart always --name <containername> <imagename>","title":"Run"},{"location":"docker/#check-status","text":"This checks the statuses of each running container, or all containers. sudo docker ps sudo docker ps -a Shows resource usage for each running container. sudo docker stats","title":"Check Status"},{"location":"docker/#clean","text":"To manually remove a container or image, used the below commands. Add -f to forcefully remove if the container is still running. sudo docker rm <container-id/name> sudo docker rmi <image-id/name> Each iteration of rebuilding and relaunching of containers will create a lot of intermediate images, and stopped containers. These can occupy a lot of space, so using prune can remove all these obsolete items at one go. sudo docker image prune sudo docker container prune sudo docker system prune","title":"Clean"},{"location":"docker/#debug","text":"We can check the console logs of the Flask app by opening up docker logs in live mode using -f . The logs are stored in /var/lib/docker/containers/[container-id]/[container-id]-json . sudo docker logs -f container_name At times, we might need to check what is inside the container. sudo docker exec -it <container name/id> bash","title":"Debug"},{"location":"docker/#network","text":"For different containers to communicate to each other, we need to set a fixed network as the IP will change with each instance. We can do so by creating a network and setting it in when the container is launched. docker network create <name> docker run -d --network <name> --name = <containera> <imagea> docker run -d --network <name> --name = <containerb> <imageb> # list networks docker network ls Alternatively, we can launch the containers together using docker-compose, and they will automatically be in the same network.","title":"Network"},{"location":"docker/#docker-compose","text":"When we need to manage multiple containers, it will be easier to set the build & run configurations using Docker-Compose, in a ymal file called docker-compose.yml . The official Docker blog post gives a good introduction on this. version: \"3\" services: facedetection: build: ./facedetection container_name: facedetection ports: - 5001:5000 restart: always maskdetection: build: ./maskdetection container_name: maskdetection ports: - 5001:5000 restart: always When we are ready, we use the commands docker-compose build & docker-compose up -d to build & launch the containers respectively.","title":"Docker-Compose"},{"location":"dvc/","text":"Data Version Control DVC is a library for ML versioning. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ] DVC INIT dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs Add S3 Keys To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\" Add Remote Link We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin Push Data to S3 We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link. Update Git We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version. Retrieve Data from S3 We can pull from specific dvc folders by using dvc <foldername>.dvc pull Full Code Example # add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Data Version Control"},{"location":"dvc/#data-version-control","text":"DVC is a library for ML versioning. For this documentation, we will be using an example where we store the data versions into AWS S3 bucket. To start off, install the dvc library, together with AWS S3's accompanying library pip install dvc [ s3 ]","title":"Data Version Control"},{"location":"dvc/#dvc-init","text":"dvc init will add a .dvcignore file, and more importantly, a .dvc folder. Within the latter, it contains: cache folder: stores the data versions & corresponding hashes, which will be uploaded to the S3. These will not be uploaded, as there is a .gitignore file automatically created, that excludes this folder. config file: stores the s3 bucket URLs","title":"DVC INIT"},{"location":"dvc/#add-s3-keys","text":"To allow DVC to push & pull data via S3 bucket, we need to set the AWS Access & Secret Keys, either through AWS CLI aws configure , or save them as environment variables in the OS. export AWS_SECRET_ACCESS_KEY = \"xxx\" export AWS_ACCESS_KEY_ID = \"xxx\"","title":"Add S3 Keys"},{"location":"dvc/#add-remote-link","text":"We then add the remote links, by giving it a name, and the S3 URL. You can see that we are classifying the links as folders within an S3 bucket dvc remote add babystroller s3://images/babystroller dvc remote add rubbishbin s3://images/rubbishbin # add as a default remote dvc remote add -d safetycone s3://images/safetycone This will update the .dvc/config file as shown below. [core] remote = safetycone ['remote \"babystroller\"'] url = s3://images/babystroller ['remote \"safetycone\"'] url = s3://images/safetycone ['remote \"rubbishbin\"'] url = s3://images/rubbishbin","title":"Add Remote Link"},{"location":"dvc/#push-data-to-s3","text":"We create a folder and add the datasets in there. This will create a version of the data. The details are as follows: Add folder with data dvc add <foldername> a file called <foldername>.dvc will be created .dvc/cache folder containing the data version & hashes will be created the <foldername> is automatically added to .gitignore so that the data will not be uploaded to the repository Push data version to S3: dvc push -r <remote name> <foldername>.dvc We need to be mindful to set the remote name to the correct link else it will just use the default (core) link.","title":"Push Data to S3"},{"location":"dvc/#update-git","text":"We then git commit the <foldername>.dvc , .gitignore & .dvc/config (if there are any additions of the remote links) to the repository. The git commit message/hash will be the basis to access the data version.","title":"Update Git"},{"location":"dvc/#retrieve-data-from-s3","text":"We can pull from specific dvc folders by using dvc <foldername>.dvc pull","title":"Retrieve Data from S3"},{"location":"dvc/#full-code-example","text":"# add remote link dvc remote add rubbishbin s3://images/rubbishbin # assume folder called rubbishbin is added with data inside # store data version to S3 dvc add rubbishbin dvc push -r rubbishbin rubbishbin.dvc # git commit git add rubbishbin.dvc .dvc/config .gitignore git commit -m \"Add init rubbishbin dataset\" git push # pull data dvc pull -r rubbishbin rubbishbin.dvc","title":"Full Code Example"},{"location":"flask/","text":"Flask Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task. Simple Flask App Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run () Gunicorn Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it gunicorn --bind 0 .0.0.0:5000 -w 2 serve_http:app Testing Python Requests The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j ) Postman Postman is a popular GUI to easily send requests and see the responses.","title":"Flask"},{"location":"flask/#flask","text":"Flask is a micro web framework written in Python. It is easy and fast to implement. How is it relevant to AI? Sometimes, it might be necessary to run models in the a server or cloud, and the only way is to wrap the model in a web application. The input containing the data for predicting & the model parameters, aka Request will be send to this API containing the model which does the prediction, and the Response containing the results will be returned. Flask is the most popular library for such a task.","title":"Flask"},{"location":"flask/#simple-flask-app","text":"Below is a simple flask app to serve an ML model's prediction. Assuming this app is named serve_http.py , we can launch this flask app locally via python serve_http.py . The API can be accessed via http://localhost:5000/ . \"\"\"flask app for model prediction\"\"\" from flask import Flask , request from predict import detectObj from utils_serve import array2json , from_base64 app = Flask ( __name__ ) @app . route ( \"/\" , methods = [ \"POST\" ]) def get_predictions (): \"\"\"Returns pred output in json\"\"\" try : req_json = request . json # get image array encodedImage = req_json [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] decodedImage = from_base64 ( encodedImage ) # get input arguments features = req_json [ \"requests\" ][ 0 ][ \"features\" ][ 0 ] min_height = features . get ( \"min_height\" ) or 0.03 min_width = features . get ( \"min_width\" ) or 0.03 maxResults = features . get ( \"maxResults\" ) or 20 score_th = features . get ( \"score_th\" ) or 0.3 nms_iou = features . get ( \"nms_iou\" ) or 0.4 # get pred-output pred_bbox = detectObj ( decodedImage , min_height , min_width , maxResults , score_th , nms_iou ) # format to response json output json_output = array2json ( pred_bbox , class_mapper ) return json_output except Exception as e : print ( e ) if __name__ == '__main__' : app . run ()","title":"Simple Flask App"},{"location":"flask/#gunicorn","text":"Flask as a server is meant for development, as it tries to remind you everytime you launch it. It has very limited parameters to manage the server, but luckily wrappers are available to connect Flask to a feature rich web server. Examples include timeout, and multiple workers. Gunicorn is one of the most popular, and also probably the easiest to use. # gunicorn -w 2 flaskscript:flaskapp # it uses port 8000 by default, but we can change it gunicorn --bind 0 .0.0.0:5000 -w 2 serve_http:app","title":"Gunicorn"},{"location":"flask/#testing","text":"","title":"Testing"},{"location":"flask/#python-requests","text":"The python requests library provides a convenient function to test your model API. Its function is basically just a one-liner, e.g. response = requests.post(url, headers=token, json=json_data) , but below provides a complete script on how to use it with a JSON request to send over an image with model parameters. \"\"\"python template to send request to ai-microservice\"\"\" import base64 import json import requests json_template = \\ { \"requests\" : [ { \"features\" : [ { \"score_th\" : None , \"nms_iou\" : None } ], \"image\" : { \"content\" : None } } ] } def send2api ( url , token , image , score_th = 0.35 , nms_iou = 0.40 ): \"\"\"Sends JSON request to AI-microservice and recieve a JSON response Args ---- url (str): URL of server where AI-microservice is hosted image (image file): opened image file score_th (float): Minimum prediction score for bounding box to be accepted nms_ious (float): IOU threshold for non-max suppression Rets ---- (dict): JSON response \"\"\" base64_bytes = base64 . b64encode ( image . read ()) . decode ( \"utf-8\" ) json_template [ \"requests\" ][ 0 ][ \"image\" ][ \"content\" ] = base64_bytes json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"score_th\" ] = score_th json_template [ \"requests\" ][ 0 ][ \"features\" ][ 0 ][ \"nms_iou\" ] = nms_iou token = { \"X-Api-Token\" : token } response = requests . post ( url , headers = token , json = json_template ) json_response = response . content j = json . loads ( json_response ) return j if __name__ == \"__main__\" : url = \"http://localhost:5000/\" token = \"xxx\" image_path = \"sample_images/20200312_171718.jpg\" image = open ( image_path , 'rb' ) j = send2api ( url , token , image ) print ( j )","title":"Python Requests"},{"location":"flask/#postman","text":"Postman is a popular GUI to easily send requests and see the responses.","title":"Postman"},{"location":"git/","text":"Git Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Version Control : allows rollback of code Branches : allows different components to be developed concurrently and merged to a single branch later The process works roughly like this: Create a new branch Work on a new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch Branches Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <featurename> Below is a workflow typical in a project team. Image sourced from Buddy . Commit Code Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request. Merge Request Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request. Git CMD Workflow The various git commands and how they interact at different stages are as illustrated below.","title":"Code Version Control"},{"location":"git/#git","text":"Git is the most popular source code version control system. Popular software development hosts like Github, Gitlab, Bitbucket all uses git. It is essential for two main purposes: Version Control : allows rollback of code Branches : allows different components to be developed concurrently and merged to a single branch later The process works roughly like this: Create a new branch Work on a new feature Commit & push a small working component Send a merge request to e.g. dev branch Tech lead approve, code is merged to dev branch","title":"Git"},{"location":"git/#branches","text":"Branches plays a key role in git. This allows multiple users to coordinate in a project, which each working on a separate branch. As a rule of thumb, we do not push any codes directly to the master branch. The owner of the repository will usually disable this option. # check which branch you are at git branch # pull updates from remote git fetch # change to dev branch git checkout dev # create a new local branch git checkout -b <featurename> Below is a workflow typical in a project team. Image sourced from Buddy .","title":"Branches"},{"location":"git/#commit-code","text":"Prior to commit our code, we should check the changes we made. # check for any changes for all files git status # check for code changes in a file git diff <filename> To commit your code, we first want to resolve any potential conflicts from the branch we want to merge to later (in case, new updates occurred). Then, we add the file or all files in a folder, followed by a commit msg, and then push to the remote (hosting service like github). git merge <branchtomergelater> git add <fileOrfolder> git commit -m \"your commit msg\" git push It is essential to commit small, bite-size code changes to prevent the reviewer from being overwhelmed when validating your code during a merge request.","title":"Commit Code"},{"location":"git/#merge-request","text":"Also known as Pull Request, this is an option in e.g., Github GUI, to merge the code from one branch to another, e.g. dev to master. The branch owner will validate the code changes, give comments, and accept/reject the request.","title":"Merge Request"},{"location":"git/#git-cmd-workflow","text":"The various git commands and how they interact at different stages are as illustrated below.","title":"Git CMD Workflow"},{"location":"security/","text":"Security Scans If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline. Source Code Scan Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Installation: pip install bandit Single File: bandit your_file.py Directory: bandit -r ~/your_repos/project Display only High Severities: bandit -r ~/your_repos/project -lll Json Output: bandit --format json --recursive project/ -l --output bandit.json Skip Certain Vulnerabilities, by placing .bandit file at directory to check [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' ) Dependency Scan This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels. Installation: pip install safety Check installed packages in VM: safety check Check requirements.txt, does not include dependencies: safety check -r requirements.txt Full Report: safety check --full-report Json Output: safety check --json --output insecure_report.json Secrets Scan detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. Installation: pip install detect-secrets Directory: detect-secrets scan directory/*","title":"Security"},{"location":"security/#security-scans","text":"If your work moves to production, you will definitely want to ensure that you have done your part to identify & resolve any security vulnerabilities present in the source code. We can use various open-sourced libraries to help out. They should, ideally be placed in a precommit, or added to the repository's CI automated pipeline.","title":"Security Scans"},{"location":"security/#source-code-scan","text":"Also known as Static Application Security Test (SAST), the most popular python src scanning is bandit . Each vulnerability detected is classified by their severity & confidence level. Installation: pip install bandit Single File: bandit your_file.py Directory: bandit -r ~/your_repos/project Display only High Severities: bandit -r ~/your_repos/project -lll Json Output: bandit --format json --recursive project/ -l --output bandit.json Skip Certain Vulnerabilities, by placing .bandit file at directory to check [bandit] skips: B104,B101 Here is a bash script to detect the presence of high vulnerabilities. bandit -r ../ $module -lll bandit --format json --recursive ../ $module --output bandit.json high_count = $( cat bandit.json | jq '[.results [] | select(.issue_severity==\"HIGH\")]' | jq '. | length' )","title":"Source Code Scan"},{"location":"security/#dependency-scan","text":"This scans for vulnerabilities in python libraries. safety is decent open-sourced library with the free database being updated once a month. However, it does not classify vulnerabilities by severity levels. Installation: pip install safety Check installed packages in VM: safety check Check requirements.txt, does not include dependencies: safety check -r requirements.txt Full Report: safety check --full-report Json Output: safety check --json --output insecure_report.json","title":"Dependency Scan"},{"location":"security/#secrets-scan","text":"detect-secrets scans for the presence of a variety of secrets & passwords. It creates a json output, with the key \"results\" filled if any secrets are detected. Installation: pip install detect-secrets Directory: detect-secrets scan directory/*","title":"Secrets Scan"},{"location":"serverless/","text":"Serverless Architecture Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice. Lambda & API Gateway In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins . Zappa Zappa is a popular python library used to automatically launch python lambda functions & api-gateways. VENV Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works. How to Use After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"python-serverless-deployment-s3\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user policy is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only. Commands Cmd Desc zappa init create a zappa_settings.json file zappa deploy deploy stage as specified in json zappa update update stage zappa undeploy delete lambda & API Gateway zappa package zip all files together zappa tail print tail logs from CloudWatch zappa status check status","title":"Serverless"},{"location":"serverless/#serverless-architecture","text":"Serverless does not mean that are no servers, but rather, refers to a server that will spin up only when required, and shut down when the job is done. Such an architecture has potential to save a lot costs. Of course, the requirement for serverless is that the job should be something that is light, and not required to run 24/7. This can be an AI microservice, or intermediate functions, e.g. taking data from S3 > process the data > send to AI microservice.","title":"Serverless Architecture"},{"location":"serverless/#lambda-api-gateway","text":"In AWS, the serverless architecture is done using a lambda function, with an accompanying API gateway (if required), which generates an API URL for the function. The gateway can also control access through an API token, and have rate limits. It is also important to note that the lambda function can only have a size of 50 Mb , If required, certain blobs can be stored in S3 and lambda can access from there. It also have a default runtime of 30sec, and can be increase to a maximum of 15 mins .","title":"Lambda &amp; API Gateway"},{"location":"serverless/#zappa","text":"Zappa is a popular python library used to automatically launch python lambda functions & api-gateways.","title":"Zappa"},{"location":"serverless/#venv","text":"Your application/function needs a virtual environment with all the relevant python libraries installed in the root. See the Virtual Environment example on how to use venv for this. Before zappa deployment, ensure that the venv is activated and the app works.","title":"VENV"},{"location":"serverless/#how-to-use","text":"After installing zappa, we init it at the root of your function, which will create a default zappa_settings.json. pip install zappa zappa init All the instructions to launch the Lambda & API Gateway are set in zappa_settings.json . Refer to the official readme for the entire list. We can have another stage, e.g. \"prod\" with its own configuration. { \"dev\" : { \"app_function\" : \"app.app\" , \"aws_region\" : \"ap-southeast-1\" , \"profile_name\" : \"default\" , \"project_name\" : \"maskdetection-lambda\" , \"runtime\" : \"python3.6\" , \"s3_bucket\" : \"python-serverless-deployment-s3\" , \"slim_handler\" : true , \"apigateway_description\" : \"lambda for AI microservice\" , \"lambda_description\" : \"lambda for AI microservice\" , \"timeout_seconds\" : 60 } } Then, we will deploy it to the cloud, by stating the stage. Behind the scenes, it does all these: New user policy is created for lambda execution Zappa will do some wrapping of your code & libraries, then package it as a zip The zip will be uploaded to an S3 bucket stated in zappa_settings.json Zip file is deleted Lambda will recieve the zip from S3 and launch the application API gateway to this lambda is setup We might encounter errors along the way; to view the logs, we can use zappa tail , which extract the tail logs from AWS CloudWatch. If we are using other frameworks to upload our lambda, e.g. Serverless , we can also use zappa to zip the code only.","title":"How to Use"},{"location":"serverless/#commands","text":"Cmd Desc zappa init create a zappa_settings.json file zappa deploy deploy stage as specified in json zappa update update stage zappa undeploy delete lambda & API Gateway zappa package zip all files together zappa tail print tail logs from CloudWatch zappa status check status","title":"Commands"},{"location":"virtual_env/","text":"Virtual Environment Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment. Anaconda Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname> VENV venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"Virtual Environment"},{"location":"virtual_env/#virtual-environment","text":"Every project has a different set of requirements & different sets of python packages might be required to support it. The versions of each package can differ or break with each python or dependent packages versions update, so it is important to isolate every project within an enclosed virtual environment.","title":"Virtual Environment"},{"location":"virtual_env/#anaconda","text":"Anaconda is a python installation that bundles all essential packages for a data science project, hence by default most people will have this installed. It comes with its own virtual environment. Here are the basic commands to create & remove. conda create -n <yourenvname> python = 3 .7 pip install -r requirements.txt conda activate <yourenvname> conda deactivate conda env list conda env remove -n <yourenvname>","title":"Anaconda"},{"location":"virtual_env/#venv","text":"venv is my next favourite, as it is a default library shipped with python, and very easy to setup. The libraries will be installed in the directory you are in, so remember to add the folder in .gitignore . To remove the libraries, just delete the folder that was created. cd <projectfolder> python -m venv <name> source <name>/bin/activate pip install -r requirements.txt deactivate","title":"VENV"}]}